% Extracted from thesis.tex (around line 1)
\begin{table}[h]
  \centering
  \caption{Comparison of representative automated grading systems.}
  \label{tab:literature_comparison}
  \begin{tabularx}{\textwidth}{l l X X}
    \toprule
    Study & Modality & Key Strength & Notable Limitation \\
    \midrule
    Poličar et~al.~\cite{policar2025} & Text essays & Demonstrates feasibility of open-source LLM deployment within a Streamlit HITL workflow. & Lacks quantitative alignment metrics and reproducible prompts. \\
    Chu et~al.~\cite{chu2025} & Short answers & RAG improves rubric adherence and explanation coherence. & Depends on proprietary GPT models; no editable feedback path. \\
    Törnqvist et~al.~\cite{tornqvist2023} & Short answers & Generates criterion-level rationales via attention mechanisms. & Requires handcrafted features and supervised pairs for each course. \\
    DeepEval~\cite{deepeval2024} & Feedback scoring & Supplies holistic rubric-aligned evaluation metrics. & Operates as an assessment tool rather than an end-to-end grader. \\
    \bottomrule
  \end{tabularx}
\end{table}


% Extracted from thesis.tex (around line 2)
\begin{table}[h]
  \centering
  \caption{Score accuracy against human ground truth.}
  \label{tab:mae_pearson}
  \begin{tabular}{lcc}
    \toprule
    Model & MAE $\downarrow$ & Pearson's $r$ $\uparrow$ \\
    \midrule
    Single-agent baseline & 4.6 & 0.61 \\
    Multi-agent consensus & \textbf{2.8} & \textbf{0.82} \\
    Human instructors      & 0.0 & 1.00 \\
    \bottomrule
  \end{tabular}
\end{table}


% Extracted from thesis.tex (around line 3)
\begin{table}[h]
  \centering
  \caption{Core technology stack for the automated grading framework.}
  \label{tab:tech_stack}
  \begin{tabularx}{\textwidth}{l l Y}
    \toprule
    Component & Technology & Rationale \\
    \midrule
    Frontend/Backend & Streamlit & Rapid development of data-centric workflows in Python within a single codebase. \\
    Database & PostgreSQL & ACID-compliant relational store suited to structured course, submission, and grading data. \\
    AI Orchestration & LangChain & Abstractions for agentic workflows and Retrieval-Augmented Generation with pluggable models. \\
    Local AI/ML Runtime & MLX (Apple Silicon) & Efficient on-device inference for open-source LLMs. \\
    PDF Processing & PyMuPDF (\texttt{fitz}) & High-accuracy text extraction and metadata parsing from heterogeneous PDFs. \\
    Vector Store & FAISS & Low-latency similarity search underpinning the RAG memory. \\
    Code Sandboxing & Docker & Ephemeral, isolated execution environment for untrusted student code and unit tests. \\
    \bottomrule
  \end{tabularx}
\end{table}


% Extracted from thesis.tex (around line 4)
\begin{table}[h]
  \centering
  \caption{Selected fields from the \texttt{grading\_results} table.}
  \label{tab:grading_results_schema}
  \begin{tabularx}{\textwidth}{l l Y}
    \toprule
    Column & Data Type & Description \\
    \midrule
    \texttt{id} & \texttt{SERIAL} & Primary key identifying each grading outcome. \\
    \texttt{course} & \texttt{VARCHAR(255)} & Course identifier associated with the submission. \\
    \texttt{assignment\_no} & \texttt{VARCHAR(255)} & Assignment number or label. \\
    \texttt{question} & \texttt{TEXT} & Prompt being evaluated. \\
    \texttt{student\_answer} & \texttt{TEXT} & Raw student submission text extracted from PDF. \\
    \texttt{old\_score} & \texttt{INTEGER} & Initial AI-generated score prior to human review. \\
    \texttt{old\_feedback} & \texttt{TEXT} & Initial AI-generated formative feedback. \\
    \texttt{new\_score} & \texttt{INTEGER} & Optional human-corrected score. \\
    \texttt{new\_feedback} & \texttt{TEXT} & Optional human-authored feedback revision. \\
    \texttt{graded\_at} & \texttt{TIMESTAMP WITH TIME ZONE} & Timestamp capturing when the automated grading occurred. \\
    \texttt{corrected\_by} & \texttt{INTEGER} & Foreign key referencing the educator who supplied corrections. \\
    \bottomrule
  \end{tabularx}
\end{table}


% Extracted from thesis.tex (around line 5)
\begin{table}[h]
  \centering
  \caption{Test environment and tooling.}
  \label{tab:test_environment}
  \begin{tabularx}{\textwidth}{l Y}
    \toprule
    Component & Configuration \\
    \midrule
    Hardware & MacBook Pro (M1, 16\,GB RAM) with simulated multi-user load \\
    Operating system & macOS 13 (Ventura) \\
    Runtime & Python 3.10 (Streamlit) + Ollama 0.5.0 \\
    Database & PostgreSQL 14 with tables \codeword{grading_results}, \codeword{grading_corrections}, \codeword{result_shares} \\
    Base LLM & \codeword{mistral} 7B (local), with comparative runs on LLaMA 3 8B and Falcon 7B \\
    Embeddings & \codeword{all-MiniLM-L6-v2} (sentence-transformers, CPU) \\
    Datasets & Anonymised lecturer rubrics and student PDFs, plus synthetic benchmarks for gap analysis \\
    \bottomrule
  \end{tabularx}
\end{table}


% Extracted from thesis.tex (around line 6)
\begin{table}[h]
  \centering
  \caption{LLM comparison summary (synthetic benchmark).}
  \label{tab:model_comparison}
  \begin{tabularx}{\textwidth}{lccc}
    \toprule
    Model & Text agreement & Code agreement & Avg latency / cost note \\
    \midrule
    Mistral (7B, Ollama) & 91\% & 88\% & 35\,s per response; $\sim\$0.60$/1k tokens equivalent \\
    LLaMA 3 8B (GGUF) & \textbf{94\%} & \textbf{89\%} & 42\,s per response; higher VRAM footprint \\
    Falcon 7B Instruct & 88\% & 82\% & 51\,s per response; struggled with rubric alignment \\
    \bottomrule
  \end{tabularx}
\end{table}
