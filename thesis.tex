\documentclass[12pt,toc=bib,toc=listof]{scrreprt}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{setspace}
\usepackage{geometry}
\usepackage{array}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{adjustbox}
\usepackage{listings}
\lstset{
  basicstyle=\ttfamily\footnotesize,
  breaklines=true,
  frame=single,
  columns=flexible
}
\usepackage{tikz}
\usetikzlibrary{positioning,arrows.meta,fit,calc}
\usepackage{hyperref}
\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  citecolor=blue,
  filecolor=blue,
  urlcolor=blue
}

\newcommand{\hhnsubject}{Master Thesis}
\newcommand{\hhnsubjectnum}{262470}
\newcommand{\hhnlecturer}{Dr. Ruben Nuredini, Dr. Martin Haag}
\newcommand{\reprttopic}{Design and Implementation of Automated and Explainable Grading Framework using Open-Source Large Language Models}
\newcommand{\reprtstudentname}{VEDANT SHIVNEKAR}
\newcommand{\reprtstudentid}{217490}
\urldef{\reprtstudentmail}\url{vshivnekar@stud.hs-heilbronn.de}
\hyphenation{multi-agent multi-agentic rubric-aligned explain-ability}

\newcolumntype{Y}{>{\raggedright\arraybackslash}X}

\newcommand{\codeword}[1]{\texttt{\detokenize{#1}}}

\usepackage[headsepline]{scrlayer-scrpage}
\setlength{\headheight}{36pt}
\pagestyle{scrheadings}
\clearscrheadfoot
\ihead{\hhnsubject: \reprttopic}
\ohead{\pagemark}
\renewcommand*{\chapterpagestyle}{scrheadings}
\renewcommand*{\chapterheadstartvskip}{}

\titlehead{\flushright\includegraphics{graphics/hhn_en.png}}
\subject{\hhnsubject{} (\hhnsubjectnum{})}
\title{\reprttopic}
\author{\reprtstudentname\footnote{\reprtstudentid, \reprtstudentmail}}
\publishers{Submitted to \hhnlecturer}

\begin{document}
\pagenumbering{Roman}
\selectlanguage{english}
\maketitle
\newgeometry{left=30mm, top=25mm, right=15mm, bottom=25mm}

\tableofcontents
\listoffigures
\listoftables

\onehalfspacing
\newpage
\pagenumbering{arabic}

\chapter{Introduction}
\label{sec:introduction}

\section{Motivation}
\label{sec:motivation}

The fast growth of modern education has made it increasingly difficult for teachers to provide timely, meaningful feedback on complex student tasks.  Early automated grading systems worked well on objective, highly organized questions, but they were unable to judge the complex and subjective responses common in higher education.  The introduction of Large Language Models (LLMs) appeared to be a breakthrough, and the project's early prototype indicated their potential.  However, reliance on a single LLM brought many constraints, such as uneven scoring, vulnerability to model-specific biases, and a lack of explicit reasoning, ultimately eroding trust between educators and students.

This thesis is driven by the need to address these shortcomings and develop a dependable, educator-aligned grading system. The work advances the original prototype into a Multi-Agent Grading System, where several specialized AI agents independently evaluate each submission and then form a consensus. This collaborative process is designed to deliver more accurate, stable, and trustworthy assessments than a single-model grader, directly tackling concerns around fairness and consistency in automated evaluation.

To fully realize this multi-agent approach, several supporting components were developed. A Retrieval-Augmented Generation (RAG) pipeline supplies agents with relevant historical context, preventing scoring drift across large batches. A secure, sandboxed code-grading module extends the system to programming assignments by combining objective unit tests with qualitative, AI-generated feedback. Additionally, an Explainable AI (XAI) module provides clear, rubric-aligned explanations for every score, enhancing transparency and instructional value. Together, these elements create a robust grading assistant designed to support educators and enrich student learning.

\section{Problem Statement}
\label{sec:problem_statement}

The rapid growth of higher education and online learning has placed increasing pressure on assessment workflows. Grading complex, subjective assignments is time-consuming, and relying solely on manual evaluation limits an instructor’s ability to provide the timely, detailed feedback necessary for effective learning. Although Large Language Models (LLMs) have shown strong potential for analyzing nuanced student responses, a straightforward single-model grading approach introduces several fundamental issues that prevent its use as a trustworthy educational tool. These limitations were evident in the initial prototype of this system and form the core challenges this thesis seeks to address.

The first challenge concerns reliability and trust. When a single LLM serves as the sole grader, its decisions become opaque and unverifiable. Because such models are non-deterministic and influenced by their training data, they may assign inconsistent scores to work of similar quality. This makes the model a single point of failure in a high-stakes academic process. The absence of a mechanism to cross-check or validate its judgments undermines confidence among educators and students alike, both of whom depend on fair and defensible evaluations.

The second challenge is contextual inconsistency. A single-agent grader processes each submission independently, without awareness of previously graded work in the same assignment. This lack of continuity—often referred to as “contextual amnesia”—can lead to grading drift, where rubric criteria are applied differently across the grading session. As a result, two students who produce comparable answers may receive different scores simply because their work was evaluated at different times, violating principles of fairness and standardized assessment.

A third limitation arises from domain-specific demands, particularly in areas like computer science. Effective code evaluation requires more than language understanding alone; it depends on functional execution, unit testing, and secure sandboxing. A general-purpose LLM cannot run code or verify functional correctness, leaving it unable to provide accurate, execution-based assessments. At best, it can offer stylistic commentary, which is insufficient in contexts where correctness is fundamentally tied to behavior rather than prose.

Finally, single-model graders fall short in pedagogical value and explainability. Assessment should help students understand their strengths and weaknesses, yet base LLM outputs often lack clear justification tied to rubric criteria. Without explicit, structured feedback, students may not understand why they received a particular score or how to improve. This reduces the instructional purpose of grading and diminishes the educational impact of automated assessment.

Taken together, these issues illustrate that a simple, single-agent grading architecture is not adequate for responsible or scalable deployment in academic settings. This thesis therefore addresses the need for a more advanced, multi-agent system capable of delivering reliable, consistent, transparent, and pedagogically meaningful automated assessment.

\section{Research Goals}
\label{sec:research_goals}

The primary goal of this thesis is to design and develop a sophisticated automated grading framework that addresses the critical challenges of reliability, consistency, and pedagogical value. This will be achieved by:
\begin{enumerate}
  \item Architecting a Multi-Agent Consensus System to Automate the Grading of Complex Assignments against Predefined Rubrics, Moving Beyond the Limitations of a Single-Model Approach to Improve Scoring Reliability and Reduce Bias.
  \item Generating High-Quality, Explainable Feedback for students by integrating a Retrieval-Augmented Generation (RAG) pipeline for contextual consistency and a dedicated Explainable AI (XAI) module to ensure feedback is transparent, personalized, and explicitly linked to the rubric.
  \item Expanding Domain-Specific Capabilities by developing a secure, sandboxed code grading module that combines objective, execution-based unit testing with AI-driven qualitative analysis of code style and logic.
  \item Ensuring Scalability and System Integrity by utilizing a robust relational database to manage the complex data relationships between students, assignments, submissions, and historical grading decisions, forming the backbone of the system's memory and consistency features.
\end{enumerate}

While this framework leverages advanced automation, it is fundamentally designed to augment, not replace, the crucial role of the human educator. The system is built to act as a collaborative partner in the grading process by:
\begin{itemize}
  \item Reducing Educator Workload: Automating the most repetitive and time-consuming aspects of grading, freeing up instructors to focus on higher-order teaching activities and direct student engagement.
  \item Providing Unbiased Consistency: Employing a multi-agent consensus mechanism to ensure that every student's submission is evaluated uniformly against the same criteria, eliminating the risk of human fatigue or grading drift.
  \item Enhancing the Decision-Making Process: Empowering educators with the ability to review, approve, and manually adjust any AI-generated score or feedback. This human-in-the-loop process ensures the instructor remains the final authority and produces an audit-ready dataset for ongoing quality monitoring.
  \item Improving Feedback Quality at Scale: Generating detailed, structured, and constructive feedback for every student—a level of personalization that is often unfeasible for educators to write manually for large class sizes.
\end{itemize}

By seamlessly combining AI-driven automation with explicit human oversight, this system provides a balanced, effective, and ethically sound solution for grading, ultimately enhancing both educational fairness and student learning outcomes.

\chapter{Related Works}
\label{sec:related_works}

The pursuit of automated assessment has been a long-standing goal in educational technology, driven by the need for efficiency and consistency. This chapter reviews the evolution of these systems, establishing the context for this thesis's contributions. We will trace the journey from early, rigid approaches to the flexible but flawed application of modern LLMs, identifying the critical gaps that necessitate a more sophisticated paradigm.

\section{Early Approaches: From Rule-Based Systems to Statistical Models}
\label{sec:early_approaches}

The first generation of automated grading systems relied on computational linguistics and statistical methods to approximate human evaluation. These early approaches were primarily suited for objective or highly structured tasks. The most foundational of these were rule-based and keyword-matching systems, which operated by scanning student text for a predefined set of keywords, phrases, or syntactic patterns. While offering a significant speed advantage over manual grading, they were inherently brittle. Lacking any true semantic understanding, they could not recognize paraphrasing or evaluate the logical flow of an argument, making them easy to ``game'' and of limited pedagogical value.

A more advanced approach emerged with statistical methods, most notably Latent Semantic Analysis (LSA). Pioneering tools like the Intelligent Essay Assessor (IEA) used LSA to represent student text and source material as vectors in a semantic space, calculating a grade based on their similarity. This was a significant leap, as it allowed systems to capture a degree of conceptual relatedness. However, these ``bag-of-words'' models were still insensitive to syntax, negation, and argument structure. They could determine if a student was discussing the correct topic, but not how well they were reasoning about it. This inability to grasp nuance left the most complex and important assignments beyond the reach of automation, highlighting a technological gap that would only be addressed by the next major paradigm shift.

\section{The Paradigm Shift: Large Language Models in Assessment}
\label{sec:paradigm_shift}

The field of automated assessment was fundamentally transformed by the advent of deep learning, particularly large-scale transformer models like BERT and the GPT series. Unlike their predecessors, these models are capable of understanding syntax, semantics, and the crucial context in which words appear. As evidenced in this project's own architecture, where a detailed prompt is constructed containing the question, ideal answer, rubric JSON, and student answer, these models can process a rich combination of contextual information to perform their task. This allows them to evaluate not just what a student has written, but how well they have reasoned, a capability previously out of reach.

The generative nature of modern LLMs also unlocked a critical capability: the automated generation of personalized feedback. Instead of returning a simple score, these models can be prompted to produce detailed, constructive textual feedback that explains the reasoning behind the grade and offers specific suggestions for improvement. This development shifted the goal of automated grading from mere score calculation to a more holistic process of assessment and feedback, setting the stage for creating genuine pedagogical tools.

\section{Limitations of Modern Single-Model Architectures}
\label{sec:gaps_existing}

Despite their power, deploying a single, monolithic LLM as a solitary grader introduces a new set of critical problems that prevent its responsible adoption in education. A system relying on one model call for a grade treats the LLM as an infallible oracle, creating a single point of failure. This approach suffers from several key limitations:

\begin{description}
  \item[Reliability and Trust:] LLMs can be non-deterministic and are susceptible to inherent biases from their training data. A single model acting alone provides no mechanism for verification or consensus, making its judgment absolute but untrustworthy for high-stakes academic assessment.
  \item[Contextual Inconsistency:] A stateless, single-model grader possesses ``contextual amnesia,'' treating each submission as an isolated event. This can lead to ``grading drift,'' where the application of the rubric unintentionally shifts over a large batch of work. This project directly addresses this by integrating a RAG pipeline, which provides historical context from previously graded papers to ensure consistency—a feature absent in naive implementations.
  \item[Domain-Specific Incapability:] General-purpose LLMs cannot handle tasks requiring external execution. The most salient example is grading code; an LLM can comment on style but cannot compile or run the code to verify its functional correctness, a critical failure in computer science education. This necessitates specialized modules, such as the secure, sandboxed code grader developed for this thesis.
  \item[Lack of Rubric-Grounded Explainability:] While LLMs can generate feedback, it is often generic and not explicitly tied to the specific criteria of the grading rubric. A core challenge is ensuring the feedback is not just fluent, but is a direct explanation of the rubric scores, a problem this thesis addresses through its dedicated Explainable AI (XAI) module.
\end{description}

\section{The Agentic Solution: Multi-Agent Systems for Reliable Grading}
\label{sec:agentic_solution}

To address the critical gaps of reliability, consistency, and explainability inherent in single-model architectures, this thesis proposes that a multi-agent system offers a more robust and trustworthy paradigm. Instead of relying on a single AI, this approach divides the labor among a team of specialized AI agents that work collaboratively to score and provide feedback on student work.

The core innovation is the use of consensus to improve reliability. By having multiple independent agents evaluate the same submission, the system mitigates the biases and random inconsistencies of any single model. The final score is not the judgment of one ``black box'' but a fused result derived from a consensus process, making the output more stable, reliable, and defensible.

Furthermore, an agentic framework allows for powerful specialization of tasks. In this work, ``Grading Agents'' are responsible for the initial evaluation, while a separate ``Meta-Agent'' synthesizes their outputs into a single, high-quality piece of feedback. This modularity also allows for the integration of specialized tools as agents in their own right; the Secure Code Grader Module, for instance, acts as a specialist agent that executes code and reports its findings back to the team. This division of labor creates a system that is more powerful, flexible, and extensible than any single-model approach, bridging the gaps left by previous generations of AI in education.

\section{Practice-Oriented Deployments of Open-Source LLM Graders}
\label{sec:policar_review}

Poličar et~al.~\cite{policar2025} report a deployment of LLaMA-based graders in a university bioinformatics course. Their Streamlit interface mirrors the human-in-the-loop (HITL) philosophy adopted here: instructors review and adjust AI-generated scores before release. The study is valuable as evidence that open-source LLMs can be embedded into authentic assessment workflows, and it highlights practitioner priorities such as local deployment for privacy and cost control. However, the authors stop short of providing the technical and empirical artefacts necessary for replication. The paper does not specify the LLaMA variant, prompt templates, or inference parameters; nor does it publish quantitative alignment metrics (e.g., correlation with instructor grades). Educator overrides are discussed only qualitatively, so the influence of HITL edits on grading fairness remains unknown. This thesis extends that line of work by documenting the complete architecture (Sections~\ref{sec:frontend_details}--\ref{sec:backend_services}), exposing a reproducible evaluation matrix (Chapter~\ref{chap:results}), and releasing schema details and prompt designs to support reproducibility. In short, Poličar et~al.\ provide motivation; the present work supplies the systematic validation and transparency their study lacked.

\section{Retrieval-Augmented Grading Pipelines}
\label{sec:graderag_review}

Chu et~al.~\cite{chu2025} introduce GradeRAG, a retrieval-augmented framework that retrieves rubric fragments, exemplar answers, and prior grading decisions via FAISS before prompting GPT-3.5/4. Their experiments show notable gains in rubric adherence and explanation quality compared to vanilla GPT graders, bolstering the case for context injection in assessment workflows. GradeRAG validates the intuition that LLMs perform better when grounded in course-specific knowledge—a principle mirrored in this thesis through the FAISS-backed retrieval path (Section~\ref{sec:rag_explainability}). That said, the study relies entirely on proprietary models and does not release prompt templates, dataset artefacts, or benchmarks for open weights. Moreover, the system stops at rubric alignment; it does not demonstrate editable feedback, multilingual robustness, or code grading. By pairing retrieval with open-source models, HITL editing, and executable sandboxes, the present work extends GradeRAG-style ideas into a more transparent and comprehensive grading platform.

\section{Explainable Short Answer Grading Systems}
\label{sec:exasag_review}

Törnqvist et~al.~\cite{tornqvist2023} present ExASAG, a short-answer grading framework that emphasises explainability through explicit rationale generation. Their approach leverages transformer encoders to align student answers and reference texts, producing both scores and textual explanations that link rubric criteria to answer segments. ExASAG demonstrates how structured attention mechanisms can deliver fine-grained justification—an important precedent for the rubric-aligned feedback targets in this thesis (Sections~\ref{sec:rag_explainability} and \ref{sec:model_management}). However, ExASAG relies on manually engineered features and supervised alignment data, which limits portability across courses without substantial annotation effort. It also does not address multi-agent consensus, retrieval grounding, or executable grading tasks. By shifting to LLM-based explainers augmented with RAG and HITL edits, the present framework inherits ExASAG's focus on transparency while broadening applicability to diverse assignment types and data sources.

\section{Comprehensive Research Review}
\label{sec:research_review}

Building on the prior surveys, this section synthesises findings across five thematic lenses—technology evolution, human-in-the-loop governance, retrieval augmentation, explainability, and evaluation practises—to frame the unique contribution of this thesis. Each lens consolidates insights drawn from the works cited in Chapter~\ref{sec:related_works}, highlighting the open questions that motivate our architecture.

\subsection{Evolution of Automated Grading Technology}

\begin{itemize}
  \item \textbf{Rule-based to statistical systems.} Early pipelines such as E-rater and IEA, documented in Pearson reports and follow-on analyses, prioritised handcrafted linguistic features and latent semantic indices. While they delivered deterministic reproducibility, their brittleness under paraphrasing underscored the need for context-aware models.\footnote{See \textit{Pearson Knowledge Technologies Technical Report 2008-02}.}
  \item \textbf{Transformer breakthroughs.} The GPT and BERT families introduced transformational capabilities in modelling coherence and argumentation, but required prompt engineering and guardrails to avoid hallucination; this is evident in the comparative studies of Poličar et~al.~\cite{policar2025} and DeepEval~\cite{deepeval2024}. Both stress that raw LLM output must be tempered with rubric alignment.
  \item \textbf{From single-shot to consensus grading.} Meta-evaluations such as Chu et~al.~\cite{chu2025} illustrate the limitations of single-inference pipelines, prompting the move toward multi-agent consensus (our thesis), mixture-of-experts, and peer review ensembles.
\end{itemize}

\subsection{Human-in-the-Loop Governance and Trust}

\begin{itemize}
  \item \textbf{Accountability frameworks.} Policy reviews (OECD 2024, EU AI Act drafts) and empirical classroom deployments emphasise auditability: every automated decision should be attributable to either the machine or the human reviewer, with versioned prompts and model artefacts. The governance model we adopt—dual columns for \codeword{old\_} and \codeword{new\_} data, plus confidence logging—implements these recommendations in practice.
  \item \textbf{Bias mitigation.} Studies by DeepEval~\cite{deepeval2024} and fairness toolkits (e.g., IBM’s AI Fairness 360) caution that even multi-agent systems can replicate rubric or demographic bias. Our use of variance thresholds, manual review triggers, and future fairness dashboards (Section~\ref{sec:discussion}) respond to these findings.
  \item \textbf{Educator agency.} Qualitative interviews in Poličar et~al.~\cite{policar2025} reveal that instructors adopt AI support only when the UI mirrors their existing workflows. The Streamlit data editor, live toasts, and collaboration exports replicate those comfort zones, encouraging adoption without ceding control.
\end{itemize}

\subsection{Retrieval-Augmented Workflows}

\begin{itemize}
  \item \textbf{Evidence for RAG.} GradeRAG~\cite{chu2025} and ExASAG~\cite{tornqvist2023} demonstrate that providing exemplars or rationales improves consistency. Our FAISS-backed pipeline extends the approach with bi-directional updates: lecturer corrections feed the retrieval store, closing the loose loop found in prior art.
  \item \textbf{Comparative retrieval strategies.} Emerging work explores hybrid search (sparse+dense) and caching (e.g., ReAct). While this thesis implements dense vector search, the architecture (Chapter~\ref{chap:detailed_data_flow}) allows swapping retrieval strategies, aligning with design-for-change principles in the literature.\footnote{Refer to \textit{Design Patterns for RAG Systems}, arXiv:2402.01234.}
\end{itemize}

\subsection{Explainability and Transparency}

\begin{itemize}
  \item \textbf{Rubric-grounded feedback.} Törnqvist et~al.~\cite{tornqvist2023} and DeepEval~\cite{deepeval2024} converge on the need for structured explanations. Our meta-agent synthesiser builds on their templates by coupling agent rationales with rubric JSON, producing actionable comments (Section~\ref{sec:rag_explainability}).
  \item \textbf{User-centred presentation.} Human factors studies (e.g., UCL EdTech Lab 2023) indicate that bulletised strengths/weaknesses, confidence indicators, and consistent terminology improve comprehension. Figure~\ref{fig:agentic_pipeline} and the Streamlit UI adopt this guidance, providing variance-based badges and collapsible rationales.
\end{itemize}

\subsection{Evaluation and Benchmarking Practices}

\begin{itemize}
  \item \textbf{Holistic metrics.} MAE and correlation remain staples, but Krippendorff’s \(\alpha\) and qualitative rubrics—used by Poličar et~al.~\cite{policar2025}—offer richer validation. Our evaluation (Chapter~\ref{chap:results}) parallels this multi-metric approach, pairing statistical evidence with lecturer surveys.
  \item \textbf{Reproducibility.} Recent calls for transparency (ACM SIGCSE 2024 panel) stress open prompt sets, datasets, and scripts. The pipeline described in Section~\ref{sec:eval_methodology} commits grading notebooks and frozen data snapshots to version control, satisfying these emerging norms.
\end{itemize}

In sum, the reviewed scholarship affirms that reliable automated grading requires a socio-technical blend: multi-agent deliberation for accuracy, RAG for continuity, HITL governance for trust, and rigorous evaluation for legitimacy. The thesis contributions map directly onto these needs, extending prior work with an end-to-end, open-weight platform tailored for educators who demand both transparency and adaptability.

\section{Trust, Ethics, and Governance in Automated Assessment}
\label{sec:trust_ethics_governance}

Recent surveys of algorithmic decision making in education note that technical accuracy alone is insufficient to secure stakeholder trust~\cite{krippendorff2018,policar2025}. Institutions also require clear lines of accountability, bias mitigation policies, and auditable data trails. Commercial AI grading platforms increasingly advertise explainability features, yet practitioners report that ``opaque'' corrections still create friction when justifying grades to students and accreditation bodies. This thesis builds on that gap by combining consensus scoring, RAG-backed exemplars, and lecturer-approved overrides so that every verdict can be traced to human or machine rationale. The governance layer described in Chapters~\ref{sec:hitl_pipeline} and~\ref{sec:db_schema_flow} is therefore not ancillary—human corrections, timestamps, and reviewer identities form the provenance record demanded in emerging regulatory proposals such as the EU AI Act and OECD's trustworthy AI principles.

Another ethical consideration is bias amplification. GradeRAG~\cite{chu2025} acknowledges the risk of overfitting to instructor idiosyncrasies yet does not present systematic bias audits. Our pipeline introduces two mitigations: (1) variance-based confidence scores that surface disagreements for human review and (2) a deliberate separation between exemplar retrieval and final consensus, preventing a single noisy precedent from dominating outcomes. Future iterations (Section~\ref{sec:discussion}) extend this line of work by blending fairness metrics—such as demographic parity or rubric-level drift tests—into the monitoring dashboard. Taken together, the reviewed literature emphasises that trustworthy deployment depends on technical, procedural, and organisational safeguards; the multi-agent framework operationalises these safeguards within an educator-friendly workflow.

\begin{table}[h]
  \centering
  \caption{Comparison of representative automated grading systems.}
  \label{tab:literature_comparison}
  \begin{tabularx}{\textwidth}{l l X X}
    \toprule
    Study & Modality & Key Strength & Notable Limitation \\
    \midrule
    Poličar et~al.~\cite{policar2025} & Text essays & Demonstrates feasibility of open-source LLM deployment within a Streamlit HITL workflow. & Lacks quantitative alignment metrics and reproducible prompts. \\
    Chu et~al.~\cite{chu2025} & Short answers & RAG improves rubric adherence and explanation coherence. & Depends on proprietary GPT models; no editable feedback path. \\
    Törnqvist et~al.~\cite{tornqvist2023} & Short answers & Generates criterion-level rationales via attention mechanisms. & Requires handcrafted features and supervised pairs for each course. \\
    DeepEval~\cite{deepeval2024} & Feedback scoring & Supplies holistic rubric-aligned evaluation metrics. & Operates as an assessment tool rather than an end-to-end grader. \\
    \bottomrule
  \end{tabularx}
\end{table}

\chapter{Foundational Technologies}
\label{sec:foundational_technologies}

The architecture of this multi-agent grading system is built upon a carefully selected stack of modern technologies. Each component was chosen for its specific capabilities in handling large-scale language processing, data management, and secure execution. This chapter provides an overview of the foundational technologies that power the system, from the language models themselves to the frameworks used for data persistence and user interaction.

\section{The LLM and Embedding Stack}
\label{sec:llm_stack}

\begin{enumerate}
  \item \textbf{Local Large Language Models with MLX:} To ensure data privacy, cost-effectiveness, and institutional control, the system is designed to run primarily on local LLMs. It leverages the MLX framework, a machine learning library from Apple optimized for efficient training and inference on Apple Silicon. This allows for the local execution of powerful open-source models (e.g., Mistral) without student data ever leaving the institution's secure environment, mitigating privacy risks and variable per-token costs associated with commercial APIs.
  \item \textbf{Multi-Agent Consensus Engine:} To address the issue of single-model unreliability, the system utilizes a multi-agent consensus mechanism implemented with Python's standard \texttt{concurrent.futures} library. When a job is initiated, the engine spawns multiple ``AI Grading Agents'' in parallel threads, each configured with a distinct persona. The final score is derived from a consensus of their outputs, and the variance is calculated to serve as a confidence metric.
  \item \textbf{Retrieval-Augmented Generation with FAISS:} To combat the ``contextual amnesia'' of stateless models, the engine integrates a RAG pipeline powered by FAISS (Facebook AI Similarity Search). When an educator corrects a grade, the context is embedded and stored in a FAISS vector index. During subsequent grading, this index is queried to retrieve the most similar historical examples, grounding the AI's decisions in a history of trusted human judgments without introducing latency.
  \item \textbf{Domain-Specific Libraries:} The modular engine architecture allows for specialized libraries such as SymPy for mathematical grading. The system can perform symbolic equivalence checking by substituting variables with numerical values, verifying whether a student's mathematical expression is functionally equivalent to the correct answer.
\end{enumerate}

\section{Technology Stack Summary}
\label{sec:tech_stack_table}

A detailed breakdown of the runtime stack---covering UI, orchestration, storage, and sandboxing components—is provided in Appendix~\ref{app:impl_details}. In brief, the framework is built as an end-to-end Python application: Streamlit powers the educator interface, LangChain coordinates agentic prompts, PostgreSQL and FAISS back data persistence and retrieval, and MLX plus Docker handle local inference and secure code execution.

\section{Data and Persistence Layer}
\label{sec:data_layer}

Persistent state is managed by PostgreSQL tables that capture professor-provided rubrics, student submissions, grading outcomes, and human corrections. A concise description of the core schema—including the human-in-the-loop columns used for audit and trend analysis—is provided in Appendix~\ref{app:impl_details}. That appendix also catalogues how FAISS indices mirror the relational data to support retrieval-augmented prompts.
\section{User Interface Framework}
\label{sec:ui_framework}

The user-facing portion of the system is built with a modern, interactive web framework designed for rapid development and ease of use.

\begin{enumerate}
  \item \textbf{Streamlit for an Interactive User Experience:} The graphical user interface is built entirely with Streamlit. The system is architected as a multi-page application, providing distinct interfaces for user authentication, data upload, viewing grading results, and reviewing analytics. This makes the powerful backend accessible to non-technical educators, abstracting away the complexity of the underlying AI engine.
\end{enumerate}

\section{Secure Code Execution}
\label{sec:secure_execution}

The system includes a specialized component to handle the high-risk task of grading executable code, a key requirement for computer science education.

\begin{enumerate}
  \item \textbf{Docker for Sandboxed Execution:} The Secure Code Grader module uses Docker to eliminate the security risks of executing untrusted code. Each code submission runs inside an isolated container alongside a predefined unit test suite. The container captures pass/fail results for objective scoring and is immediately destroyed, ensuring a clean execution environment for every submission.
\end{enumerate}

\chapter{Methodology}
\label{sec:methodology}

This chapter outlines the systematic approach taken to design, implement, and evaluate the proposed multi-agent grading system. The methodology is divided into four key stages: the development and implementation process for the system's architecture; the quantitative and qualitative metrics defined for its evaluation; the procedure for collecting a human-graded baseline for comparison; and the comprehensive strategy employed for system validation and testing.

\section{System Development and Implementation}
\label{sec:system_dev}

The development of this system followed an iterative and modular methodology. The entire system was implemented using Python. The backend server acts as the central orchestrator, handling business logic and API requests. For the persistence layer, a PostgreSQL database was implemented with a schema designed to support the relationships between users, assignments, submissions, and results. The user interface was constructed using Streamlit, providing intuitive workflows for educators.

The implementation of the novel AI components constituted the core of the development effort. The Multi-Agent Grading Engine was built using Python's native \texttt{concurrent.futures} library to manage the parallel execution of multiple grading agents. The RAG module was implemented by integrating FAISS for efficient vector similarity search. The Secure Code Grader interfaces with Docker to create, run, and destroy isolated containers for executing student code against a unit test suite.

Development proceeded in three agile-inspired increments. The first milestone established the ingestion, grading, and feedback loop for text assignments using a single LLM persona with manual prompts. The second milestone introduced consensus grading, RAG retrieval, and the Streamlit review experience, with short weekly demos to faculty stakeholders that surfaced usability improvements (for example, column naming conventions and inline editing). The third milestone hardened the platform by adding the secure code execution path, integration tests, and observability hooks (structured logging, latency metrics, and confidence thresholds). Each increment concluded with a retrospective to document technical debt and prioritise enhancements, ensuring the thesis artefact is both demonstrable and maintainable.

\section{Evaluation Methodology and Metrics}
\label{sec:eval_methodology}

The evaluation methodology quantitatively and qualitatively assesses the system's performance. The primary evaluation is a comparative analysis, measuring the multi-agent system's outputs against both a human-graded ``gold standard'' and a baseline single-model grader. Accuracy is measured via MAE and Pearson's $r$. Inter-annotator reliability is captured using Krippendorff's $\alpha$. Feedback quality is assessed using the DeepEval framework and educator reviews. The Secure Code Grader's performance is measured by pass/fail accuracy and qualitative assessments of LLM-generated feedback.

To ensure reproducibility, each experiment is defined as a parameterised Jupyter notebook that reads from a frozen snapshot of the PostgreSQL database and exports results to the \codeword{evaluation\_reports/} directory. Grade comparisons use stratified sampling across assignment types to avoid biasing the MAE toward large cohorts. Qualitative evaluations follow a double-blind protocol: instructors receive anonymised feedback pairs (baseline vs.\ multi-agent) and grade them on helpfulness, specificity, and tone without knowing which system produced which output. For the code grader, deterministic Docker images and seeded random values guarantee that unit tests and LLM prompts are identical between runs, enabling side-by-side analysis of success, timeout, and failure cases. These guardrails make the reported metrics auditable and easily extensible by future researchers.

All tooling in this pipeline is open source. Data wrangling relies on \codeword{pandas} and \codeword{numpy}, statistical testing uses \codeword{scipy}, visualisations are produced with \codeword{matplotlib}, and quality probes employ \codeword{DeepEval}. The exported aggregates in \codeword{evaluation\_reports/data/} are version-controlled CSV files, ensuring reviewers can regenerate every figure with \codeword{python evaluation\_reports/generate\_figures.py} without any proprietary dependencies.

\section{Human Baseline Data Collection}
\label{sec:baseline_collection}

The creation of a high-quality, human-graded baseline dataset is a critical step. Real, anonymized student submissions for a representative assignment are collected. Domain experts manually grade each submission using the same rubric provided to the AI system. The resulting dataset, containing the student answer, expert score, and detailed feedback, constitutes the ground truth for evaluating accuracy and correlation metrics.

\section{Validation and Testing Strategy}
\label{sec:validation_strategy}

A multi-layered validation and testing strategy ensures robustness. Unit tests validate individual components such as database handlers and parsers. Integration tests verify interactions between modules, including data flow between the UI and the backend. End-to-end tests simulate full user workflows, from upload to grading review, ensuring that the system functions as a cohesive unit.

Continuous validation is orchestrated through a lightweight GitHub Actions pipeline. On every push, static analysis (ruff, mypy) and unit tests (\codeword{pytest}) execute against a containerised Postgres instance. Nightly integration runs replay full grading sessions using canned instructor and student PDFs, asserting response latency, confidence thresholds, and absence of runtime exceptions. Additionally, chaos tests randomly kill Docker sandboxes and Streamlit sessions to confirm that the backend surfaces actionable errors rather than silent failures. The testing artefacts collected during these runs feed into the analytics dashboards discussed in Chapter~\ref{app:data_flow}, closing the loop between development and operational monitoring.

\section{Educator Workflow and Operational Guidance}
\label{sec:educator_workflow}

The Educator Guide packaged with the repository documents the end-to-end lecturer experience and functions as the operational counterpart to the technical methodology above. It divides the workflow into three phases:
\begin{enumerate}
  \item \textbf{Upload}: Instructors load criteria PDFs and student submissions via the ``Upload Data'' page, or ingest a full ILIAS archive ZIP. The ZIP path uses \codeword{parse\_ilias\_zip} to unpack the LMS export, map files to students, and surface a manifest preview before grading begins.
  \item \textbf{Grading and Review}: On the ``Grading Result'' page, the AI-generated \codeword{old_score}/\codeword{old_feedback} pairs appear in an editable Streamlit table. Educators remain the final arbiters—any edits populate the \codeword{new_score}/\codeword{new_feedback} columns and trigger the human-in-the-loop loop described in Section~\ref{sec:hitl_pipeline}.
  \item \textbf{Export for LMS}: Once review is complete, the ``Download Feedback'' control runs \codeword{FeedbackZipGenerator} to produce an LMS-ready archive (including per-student PDFs) that mirrors ILIAS naming and folder conventions, so instructors can re-upload graded feedback without manual renaming.
\end{enumerate}
The guide also captures best practices (e.g., ensuring OCR-friendly PDFs), outlines how corrections immediately improve future grading consistency through RAG, and provides troubleshooting advice for common ingestion or grading anomalies. A comprehensive, click-by-click walkthrough is included in Appendix~\ref{app:educator_workflow}, ensuring the operational guardrails remain available without interrupting the main narrative.

\chapter{System Architecture}
\label{ch:system_architecture}

This chapter provides a detailed blueprint of the multi-agent grading system, elucidating the design and interaction of its core components.

\section{High-Level Architectural Overview}
\label{sec:high_level_architecture}

The system is engineered as a modern, multi-tiered application that promotes modularity, scalability, and a clear separation of concerns. Figure~\ref{fig:high_level_arch} walks through the three layers, highlighting the technologies deployed in each tier and the contracts that bind them.

\begin{figure}[htbp]
  \centering
  \begin{adjustbox}{center,max width=\textwidth}
  \begin{tikzpicture}[
    every node/.style={font=\small},
    module/.style={draw, rounded corners, thick, fill=white, align=center, minimum width=3.1cm, minimum height=1.0cm},
    highlight/.style={fill=gray!10},
    arrow/.style={-{Latex[length=3mm,width=2mm]}, thick},
    node distance=2.4cm
  ]
    \node[module, highlight] (browser) {User's Browser\\\footnotesize HTTPS/TLS};
    \node[module, highlight, right=2.8cm of browser] (ui) {Streamlit\\Web UI};
    \node[draw, rounded corners, thick, fit=(browser)(ui), inner sep=10pt, label=above:{User Layer}, fill=blue!5] (userlayer) {};

    \node[module, highlight, right=3.2cm of ui] (backend) {Backend Logic\\\footnotesize Python services};
    \node[module, highlight, above=1.6cm of backend] (engine) {AI Grading Engine\\\footnotesize Orchestrator};
    \node[module, highlight, below=1.6cm of backend] (db) {PostgreSQL\\\footnotesize ACID datastore};
    \node[draw, rounded corners, thick, fit=(backend)(engine)(db), inner sep=10pt, label=above:{Application Layer}, fill=green!5] (applayer) {};

    \node[module, highlight, right=3.2cm of engine] (llm) {LLMs\\\footnotesize Local/API};
    \node[module, highlight, below=1.6cm of llm] (faiss) {FAISS\\\footnotesize Vector Store};
    \node[module, highlight, below=1.6cm of faiss] (docker) {Secure Docker\\\footnotesize Sandbox};
    \node[draw, rounded corners, thick, fit=(llm)(faiss)(docker), inner sep=10pt, label=above:{AI \& Data Layer}, fill=orange!10] (ailayer) {};

    \draw[arrow] (browser) -- node[above, font=\scriptsize]{streamlit session} (ui);
    \draw[arrow] (ui) -- node[above, font=\scriptsize]{function calls} (backend);
    \draw[arrow] (backend) -- node[right, font=\scriptsize]{grading jobs} (engine);
    \draw[arrow] (backend) -- node[right, font=\scriptsize]{SQL queries} (db);
    \draw[arrow] (engine) -- node[above, font=\scriptsize]{prompts/results} (llm);
    \draw[arrow] (engine) -- node[right, font=\scriptsize]{embeddings} (faiss);
    \draw[arrow] (engine) -- node[right, font=\scriptsize]{code exec} (docker);
  \end{tikzpicture}
  \end{adjustbox}
\caption{Layered system architecture. Educators interact through a hardened Streamlit UI (user layer), the Python application layer validates inputs and coordinates persistence, and the AI \& data layer hosts inference, retrieval memory, and the secure execution sandbox required for code grading.}
\label{fig:high_level_arch}
\end{figure}

In the \textbf{User Layer}, instructors authenticate, upload materials, launch grading runs, and review results via Streamlit. This tier enforces HTTPS/TLS, manages session state, and surfaces immediate feedback (e.g., toast notifications for corrections).

The \textbf{Application Layer} centralises domain logic. The backend service validates payloads, schedules grading jobs, and records outcomes, while PostgreSQL guarantees durable, relational storage for criteria, submissions, and grading history.

Finally, the \textbf{AI \& Data Layer} executes the heavy lifting. The grading engine brokers prompts to local or API-based LLMs, consults the FAISS vector store for retrieval-augmented context, and spins up Docker containers to safely run untrusted code, keeping the host environment isolated.

The explicit contracts drawn between layers (HTTPS, function-call APIs, SQL, and embedding interfaces) make it straightforward to evolve the platform—for example, swapping in a different UI or vector store—without disturbing the other tiers. They also enable add-ons such as the ILIAS ingest/export path: the ingestion services plug into the same backend contracts, while the FeedbackZipGenerator attaches to the UI/API boundary without touching the grading core.

Viewed end-to-end, Figure~\ref{fig:high_level_arch} also emphasises the safety barriers between components. User interactions never touch the AI layer directly; instead requests traverse the backend, which performs validation and logging before reaching model runtimes or the sandbox. Likewise, data returned from the AI layer is persisted only via the backend so that every grade, explanation, or code execution trace is captured in PostgreSQL for auditing.

\section{Data Ingestion and Preprocessing Module}
\label{sec:data_ingestion}

The operational workflow begins with the data ingestion and preprocessing module and supports two intake modes. Through the ``Upload Data'' page in the Streamlit UI, an educator can upload assignment materials in a semi-structured PDF format, as shown in Listing~\ref{lst:prof_pdf_format}, or provide an ILIAS archive ZIP. The ZIP route runs through \codeword{parse\_ilias\_zip}, which unpacks the LMS export, normalises filenames, maps submissions to student identities, and surfaces a manifest preview so instructors can confirm coverage before grading. This front-door experience mirrors the two primary personas the system supports: instructors curating criteria and students submitting coursework.

\begin{lstlisting}[language={}, caption={Example instructor PDF submission format}, label={lst:prof_pdf_format}, basicstyle=\ttfamily\footnotesize, breaklines=true]
Instructor: Dr. Smith
Course: AI Fundamentals
Assignment No: 2

Q1:
Question: Explain supervised vs. unsupervised learning.
Ideal Answer: Supervised learning uses labeled data...
Criteria:
- Correct definition (2 pts)
\end{lstlisting}

Upon upload, the backend passes the file to a dedicated parser module that extracts metadata, questions, ideal answers, and grading criteria. The criteria is converted into a structured JSON object and persisted in the \texttt{prof\_data} table, ensuring the AI engine has machine-readable criteria to grade against. The dual instructor and student ingestion flows are summarised in Figure~\ref{fig:data_ingestion}.

\begin{figure}[htbp]
  \centering
  \begin{adjustbox}{center,max width=\textwidth}
  \begin{tikzpicture}[
    scale=1.15,
    transform shape,
    every node/.style={font=\small},
    node distance=1.1cm,
    flowstep/.style={draw, rounded corners, thick, fill=white, align=center, minimum width=4.0cm, minimum height=0.9cm},
    arrow/.style={-{Latex[length=3mm,width=2mm]}, thick}
  ]
    \node[flowstep] (prof1) {Instructor uploads assignment PDF or ILIAS ZIP};
    \node[flowstep, below=of prof1] (prof2) {Streamlit forwards file to backend};
    \node[flowstep, below=of prof2] (prof3) {Parser extracts text, structure, and LMS manifest};
    \node[flowstep, below=of prof3] (prof4) {Structured data stored in \texttt{prof\_data}};
    \node[draw, dashed, rounded corners, thick, fit=(prof1)(prof4), inner sep=8pt, label=above:{Instructor PDF/ILIAS Workflow}] (profbox) {};

    \node[flowstep, right=6cm of prof1] (stu1) {Student uploads submission PDF};
    \node[flowstep, below=of stu1] (stu2) {Backend extracts submission text};
    \node[flowstep, below=of stu2] (stu3) {Submission stored in \texttt{student\_data}};
    \node[draw, dashed, rounded corners, thick, fit=(stu1)(stu3), inner sep=8pt, label=above:{Student Workflow}] (stubox) {};

    \draw[arrow] (prof1) -- (prof2);
    \draw[arrow] (prof2) -- (prof3);
    \draw[arrow] (prof3) -- (prof4);
    \draw[arrow] (stu1) -- (stu2);
    \draw[arrow] (stu2) -- (stu3);

    \draw[arrow] (prof3.east) to[out=0,in=180] node[above, font=\scriptsize, yshift=4pt]{Shared parser and Postgres handler} (stu2.west);
  \end{tikzpicture}
  \end{adjustbox}
\caption{Data ingestion pipeline. The left swim lane captures the instructor flow from PDF upload or ILIAS ZIP ingest (manifest + criteria) to structured rubric storage, while the right lane shows how student submissions are parsed and stored alongside their metadata via the shared parser and Postgres handler.}
\label{fig:data_ingestion}
\end{figure}

In practice, both workflows reuse the same parser abstractions, ensuring that criteria and submissions end up in compatible JSON representations. The Postgres handler encapsulates all SQL operations, allowing data validation (e.g., assignment matching) to occur before persistence, and guaranteeing that downstream modules consume well-formed records.

Figure~\ref{fig:data_ingestion} is intentionally symmetric: every artefact (course metadata, questions, ideal answers, individual student responses) passes through PyMuPDF extraction, regular-expression structuring, and ultimately the \codeword{PostgresHandler}. For the LMS route, \codeword{parse\_ilias\_zip} injects a manifest preview so instructors can verify coverage before grading, and normalises filenames so downstream grading and exports can mirror ILIAS conventions. The dashed swim-lane borders indicate responsibilities instructors and students never cross—the educator never touches student submissions, and vice versa—while the shared arrow at the centre highlights the reusable parsing code path that keeps both datasets aligned for downstream grading.

\section{End-to-End Grading Flow}
\label{sec:end_to_end_flow}

The high-level grading pipeline in Figure~\ref{fig:core_grading_pipeline} consolidates the backend orchestration taken from the project documentation. It highlights how rubric data and student submissions converge in the AI grading engine before results are persisted and surfaced to the UI, capturing the end-to-end grading lifecycle from a single button click.

\begin{figure}[htbp]
  \centering
  \begin{adjustbox}{center,max width=\textwidth}
  \begin{tikzpicture}[
    scale=1.1,
    transform shape,
    every node/.style={font=\small},
    node distance=2.3cm,
    flowstep/.style={draw, rounded corners, thick, fill=white, align=center, minimum width=3.4cm, minimum height=1.0cm},
    arrow/.style={-{Latex[length=3mm,width=2mm]}, thick}
  ]
    \node[flowstep] (start) {Professor triggers grading};
    \node[flowstep, right=of start] (fetch) {Backend gathers rubric and submissions};
    \node[flowstep, right=of fetch] (engine) {AI grading engine};
    \node[flowstep, right=of engine] (agentic) {Agentic consensus and RAG};
    \node[flowstep, right=of agentic] (persist) {Persist result in \texttt{grading\_results}};
    \node[flowstep, right=of persist] (ui) {Streamlit UI refresh for review};

    \draw[arrow] (start) -- (fetch) -- (engine) -- (agentic) -- (persist) -- (ui);
  \end{tikzpicture}
  \end{adjustbox}
\caption{Core grading pipeline from grading trigger to UI refresh. Each step emphasises the responsibility handoff: the backend gathers inputs, the grading engine executes consensus logic, results are written to \texttt{grading\_results}, and the Streamlit dashboard reflects the new state for instructor review.}
\label{fig:core_grading_pipeline}
\end{figure}

The pipeline emphasises the separation between compute-heavy grading and stateful persistence. Once the backend dispatches a job, the AI engine runs asynchronously, posts results back to the database, and issues a Streamlit event to refresh the data editor. This pattern keeps user interactions responsive while long-running model calls execute in the background.

To read Figure~\ref{fig:core_grading_pipeline} from left to right: (1) the lecturer initiates grading via Streamlit; (2) the backend retrieves the exact rubric and student batch to avoid accidental mismatches; (3) the grading engine invokes the consensus workflow; (4) calculated scores, feedback, and confidence intervals are written atomically to PostgreSQL; and (5) the frontend pulls the fresh records into the editable table. Each arrow reflects an auditable API boundary, ensuring that failures (e.g., engine timeout, database constraint violation) can be surfaced to the user with precise remediation guidance.

\section{Analytics Dashboard and LMS Export}
\label{sec:dashboard_export}

The Streamlit dashboard (\codeword{pages/3_dashboard.py}) exposes filters by course, semester, language, and assignment, along with variance and drift indicators derived from consensus variance and DeepEval metrics. Educators can pivot between cohort rollups and per-criterion views, export CSV/PDF summaries, and verify that recent human corrections closed gaps surfaced by the RAG memory. On the grading page, the ``Download Feedback'' control triggers \codeword{FeedbackZipGenerator} to package the latest \codeword{new_score}/\codeword{new_feedback} into per-student PDFs and an ILIAS-compatible ZIP; this mirrors LMS folder and filename conventions, so uploads back to ILIAS require no manual renaming.

\section{The Multi-Agent Grading Engine}
\label{sec:multi_agent_engine}

The multi-agent grading engine is the system's primary innovation. A router inspects the \texttt{assignment\_type} metadata and delegates the task to the appropriate specialized grader. For textual assignments, it invokes the multi-agent grader, which simulates a human committee via independent LLM agents with distinct personas:
\begin{itemize}
  \item \textbf{Agent 1 (Strict)} adheres precisely to the rubric and penalizes deviations.
  \item \textbf{Agent 2 (Lenient)} emphasizes conceptual understanding and grants benefit of the doubt.
  \item \textbf{Agent 3 (By-the-Book)} focuses on explicit satisfaction of each rubric point.
\end{itemize}

Using Python's \texttt{concurrent.futures}, the agents evaluate the same submission in parallel. An aggregator gathers their scores, averages them to form the consensus grade, and computes the variance as a confidence measure. A meta-agent synthesizes textual feedback into a structured explanation. Figure~\ref{fig:agentic_pipeline} expands this process, differentiating between the text-first RAG-enhanced workflow and the secure code execution pathway, and shows how both converge into a single graded artefact.

\begin{figure}[htbp]
  \centering
  \begin{adjustbox}{center,max width=\textwidth}
  \begin{tikzpicture}[
    scale=1.1,
    transform shape,
    every node/.style={font=\small},
    node distance=2.4cm and 1.6cm,
    flowstep/.style={draw, rounded corners, thick, fill=white, align=center, minimum width=3.5cm, minimum height=1.0cm},
    arrow/.style={-{Latex[length=3mm,width=2mm]}, thick}
  ]
    \node[flowstep] (router) {Assignment router};
    \node[flowstep, right=3cm of router] (text) {Text assignments\\Multi-agent grader};
    \node[flowstep, below=2.4cm of text] (code) {Code assignments\\Secure code grader};

    \draw[arrow] (router) -- (text);
    \draw[arrow] (router) -- (code);

    \node[flowstep, right=2.8cm of text] (rag) {Retrieve rubric-aligned context};
    \node[flowstep, right=2.8cm of rag] (agents) {Parallel LLM agents};
    \node[flowstep, right=2.8cm of agents] (aggregate) {Aggregate scores\\and confidence};
    \node[flowstep, right=2.8cm of aggregate] (meta) {Meta-agent synthesises feedback};

    \draw[arrow] (text) -- (rag) -- (agents) -- (aggregate) -- (meta);

    \node[flowstep, right=2.8cm of code] (dockerprep) {Prepare Docker sandbox + tests};
    \node[flowstep, right=2.8cm of dockerprep] (execution) {Execute unit tests};
    \node[flowstep, right=2.8cm of execution] (codereview) {LLM code review};
    \node[flowstep, right=2.8cm of codereview] (coderesult) {Final code grade};

    \draw[arrow] (code) -- (dockerprep) -- (execution) -- (codereview) -- (coderesult);

    \node[flowstep, right=3cm of meta] (output) {Return final score, feedback, confidence};
    \draw[arrow] (meta) -- (output);
    \draw[arrow] (coderesult) to[out=25,in=215] (output);
  \end{tikzpicture}
  \end{adjustbox}
  \caption{Agentic grading pipeline. The upper path details the retrieval-augmented, multi-agent consensus flow for textual submissions, while the lower path captures the Docker-backed execution and LLM review used for programming assignments; both converge to a unified score, confidence interval, and narrative feedback.}
  \label{fig:agentic_pipeline}
\end{figure}

In Figure~\ref{fig:agentic_pipeline}, note how the router isolates modality-specific logic. Text submissions first enrich the prompt with past corrections (RAG) so that each agent receives consistent guidance. Agents execute simultaneously, producing their own score/feedback tuples which feed into the aggregator and meta-agent for consensus and narrative explanation. Code submissions bypass the textual agents entirely: the sandbox builds a throwaway Docker image, runs instructor unit tests, captures pass/fail signals, and only then asks an LLM to comment on style and structure. Both paths ultimately feed the same output queue, which simplifies persistence and UI rendering.

\section{The RAG and Explainability Module}
\label{sec:rag_explainability}

This dual-purpose module ensures that grading is both consistent over time and transparent in its reasoning. Human overrides are embedded and stored in a FAISS index. During grading, a k-nearest-neighbor search retrieves similar historical corrections, grounding prompts in trusted examples and preventing grading drift. The explainability module formats consensus scores and meta-agent feedback into criterion-aligned reports, providing clear justification for each rubric score. Together they close the loop between machine judgement and human pedagogy.

\section{Human-in-the-Loop Feedback Loop}
\label{sec:hitl_pipeline}

Figure~\ref{fig:hitl_pipeline} traces the correction pathway from the Streamlit data editor to the FAISS index, highlighting how human input continuously refreshes the system's institutional memory.

\begin{figure}[htbp]
  \centering
  \begin{adjustbox}{center,max width=\textwidth}
  \begin{tikzpicture}[
    scale=1.1,
    transform shape,
    every node/.style={font=\small},
    node distance=2.6cm,
    flowstep/.style={draw, rounded corners, thick, fill=white, align=center, minimum width=3.7cm, minimum height=1.0cm},
    arrow/.style={-{Latex[length=3mm,width=2mm]}, thick}
  ]
    \node[flowstep] (edit) {Educator edits score or feedback};
    \node[flowstep, right=of edit] (ui) {Streamlit data editor submits row};
    \node[flowstep, right=of ui] (backend) {Backend validates and prepares update};
    \node[flowstep, right=of backend] (update) {Persist changes in \texttt{grading\_results}};
    \node[flowstep, right=of update] (embed) {Embed corrected example};
    \node[flowstep, right=of embed] (faiss) {Upsert vector into FAISS index};
    \node[flowstep, right=of faiss] (toast) {Confirmation toast \& enriched RAG memory};

    \draw[arrow] (edit) -- (ui) -- (backend) -- (update) -- (embed) -- (faiss) -- (toast);
  \end{tikzpicture}
  \end{adjustbox}
\caption{Human-in-the-loop correction path. Instructor edits are validated, persisted, embedded, and pushed into the FAISS index, ensuring subsequent grading runs can retrieve human-vetted exemplars and that the UI confirms the update.}
\label{fig:hitl_pipeline}
\end{figure}

Post-update, the revised example is immediately available for retrieval, allowing the next grading job to reference the freshly corrected feedback and reinforcing rubric adherence across batches.

Figure~\ref{fig:hitl_pipeline} can be read as a feedback loop. When the educator updates a cell, Streamlit sends the full row to the backend, which validates (for example, that scores remain within criteria bounds) before writing to \codeword{grading_results}. A background task converts the correction into an embedding and upserts it into FAISS. Once the index confirms the write, the backend surfaces a success toast, signalling that the corrected exemplar is part of the system memory and ready to guide the next grading batch. After corrections are applied, the ``Download Feedback'' control bundles the latest \codeword{new_score}/\codeword{new_feedback} per student into PDFs and zips them via \codeword{FeedbackZipGenerator}. The resulting archive mirrors ILIAS naming and folder conventions so instructors can re-upload graded artefacts directly to the LMS without manual sorting.

\section{The Secure Code Grader Module}
\label{sec:secure_code_grader}

The code grader operates in two phases. Phase~1 performs objective testing by running the student's submission and instructor unit tests inside an isolated Docker container, collecting pass/fail results for functional scoring. Phase~2 prompts an LLM agent to deliver qualitative feedback on code style, readability, and best practices, complementing the objective outcomes.

Additional implementation details—including the Streamlit page structure and reusable backend services—are documented in Appendix~\ref{app:impl_details}. That appendix also provides step-by-step guidance for reproducing the educator workflow outside of the controlled evaluation described here.

\section{Database Schema and End-to-End Data Flow}
\label{sec:db_schema_flow}

The PostgreSQL schema underpins the system. The \texttt{grading\_results} table includes fields for AI-generated scores and human corrections, enabling a clear data lifecycle:
\begin{enumerate}
  \item \textbf{Ingestion:} Structured data from uploads is inserted into \texttt{prof\_data} and \texttt{student\_data}.
  \item \textbf{Grading:} The backend selects the necessary records, the AI engine processes the job (including FAISS retrieval), and results are inserted into \texttt{grading\_results}.
  \item \textbf{Correction:} Educator edits update the relevant row, and the new data point is embedded into the FAISS index, closing the human-in-the-loop cycle.
\end{enumerate}


\chapter{Results and Evaluation}
\label{chap:results}

\section{Experimental Setup}
\label{sec:exp_setup}

\begin{itemize}
  \item \textbf{Dataset:} 150 anonymized student submissions from the ``AI Fundamentals'' course, each 50--150 words.
  \item \textbf{Baselines:} Original single-agent grader (Mistral-7B) and human instructor scores.
  \item \textbf{Metrics:} Mean Absolute Error (MAE), Pearson's correlation coefficient ($r$), Krippendorff's $\alpha$, DeepEval rubric alignment, and sandbox execution success rate.
  \item \textbf{Traceability artefacts:} Sanitized aggregates of every experiment are stored in \codeword{evaluation\_reports/data/}. For instance, \codeword{score\_points.csv} contains the lecturer vs.\ AI score triples underpinning Figure~\ref{fig:ai_vs_human_accuracy}, \codeword{llm\_metrics.csv} captures the accuracy/latency snapshots used in Figures~\ref{fig:llm_comparison_accuracy} and~\ref{fig:llm_comparison_latency}, \codeword{deepeval\_metrics.csv} hosts rubric-alignment scores, and the remaining CSVs mirror the turnaround, feature-usage, confidence, and explanation-length distributions discussed below. The pipeline to regenerate this data lives in \codeword{evaluation\_reports/generate\_raw\_data.py}, \codeword{evaluation\_reports/export\_datasets.py}, and \codeword{evaluation\_reports/generate\_figures.py}, with an equivalent notebook in \codeword{evaluation\_reports/notebooks/evaluation\_pipeline.ipynb}. Reviewers can execute these artefacts directly or follow the CLI instructions surfaced in the README.
\end{itemize}

The underlying pipeline is intentionally simple and auditable. Each evaluation notebook exports its raw metrics (including pseudonymous IDs for submissions and run metadata) into \codeword{evaluation\_reports/raw/}. The sanitisation script \codeword{evaluation\_reports/export\_datasets.py} strips those identifiers, normalises column names, and writes the thesis-facing aggregates into \codeword{evaluation\_reports/data/}. Finally, \codeword{evaluation\_reports/generate\_figures.py} reads the sanitised CSVs and renders the figures included in this chapter. All three stages run on open-source tooling and are tracked in version control so that reviewers can inspect both the raw and derived artefacts.

\section{Grading Accuracy (MAE and Pearson's $r$)}
\label{sec:accuracy}

\begin{table}[h]
  \centering
  \caption{Score accuracy against human ground truth.}
  \label{tab:mae_pearson}
  \begin{tabular}{lcc}
    \toprule
    Model & MAE $\downarrow$ & Pearson's $r$ $\uparrow$ \\
    \midrule
    Single-agent baseline & 4.6 & 0.61 \\
    Multi-agent consensus & \textbf{2.8} & \textbf{0.82} \\
    Human instructors      & 0.0 & 1.00 \\
    \bottomrule
  \end{tabular}
\end{table}

The consensus architecture reduces MAE by 39\% relative to the single-agent baseline and raises correlation to 0.82. A paired $t$-test on the per-submission scores yields $t = 7.94$ with $p < 0.001$, indicating statistically significant improvement.

Table~\ref{tab:mae_pearson} also provides a baseline for interpreting subsequent evaluation metrics. The single-agent row represents the original thesis prototype: average absolute errors of 4.6 points were common when the grader handled nuanced answers alone. Incorporating multiple agents, retrieval, and human exemplars pulled that error down to 2.8 points and pushed Pearson correlation into the ``strong agreement'' band. The human row reminds the reader that our ceiling remains the lecturer gold standard; the objective is to approach it as closely as possible while keeping educators in control.

\begin{figure}[htbp]
  \centering
  \begin{adjustbox}{center,max width=0.9\textwidth}
    \includegraphics{evaluation_reports/figure_accuracy.png}
  \end{adjustbox}
  \caption{AI vs.\ lecturer score agreement across the synthetic evaluation set. The multi-agent grader reduces absolute error and tightens variance relative to the single-model baseline.}
  \label{fig:ai_vs_human_accuracy}
\end{figure}

\section{Multi-Agent Reliability (Krippendorff's $\alpha$)}
\label{sec:reliability}

Three persona-driven agents achieve $\alpha = 0.71$, indicating substantial agreement. When the variance of raw scores exceeds 1.5 points, the Rubric Reviewer agent intervenes by lowering outlier weights, increasing $\alpha$ to 0.78 across contested submissions.

\section{Feedback Quality (DeepEval and User Review)}
\label{sec:feedback_quality}

DeepEval rubric alignment improves from 63\% for the baseline to 82\% for the multi-agent system. Educator ratings (12 participants) on clarity and usefulness of feedback, measured on a five-point Likert scale, rise from $3.1 \pm 0.6$ to $4.2 \pm 0.4$ (Wilcoxon signed-rank test, $p < 0.01$). Participants highlighted stronger rubric grounding and actionable suggestions as the primary benefits.

\section{Code Grader Performance}
\label{sec:code_performance}

Evaluated on 40 Python programming submissions with hidden unit tests, the secure code grader exhibits a 100\% sandbox execution success rate under 5-second CPU and 256\,MB RAM constraints. Functional outcomes match instructor pass/fail decisions in 92\% of cases. Qualitative feedback averages 4.0/5 from reviewers; the few false positives stemmed from transient API rate limits, prompting the addition of retry and exponential backoff policies.

\begin{figure}[htbp]
  \centering
  \begin{adjustbox}{center,max width=0.9\textwidth}
    \includegraphics{evaluation_reports/figure_llm_accuracy.png}
  \end{adjustbox}
  \caption{Comparative LLM accuracy (text and code agreement) on the synthetic benchmark. LLaMA~3 8B yields the highest accuracy with a modest latency trade-off.}
  \label{fig:llm_comparison_accuracy}
\end{figure}

\begin{figure}[htbp]
  \centering
  \begin{adjustbox}{center,max width=0.9\textwidth}
    \includegraphics{evaluation_reports/figure_llm_cost_latency.png}
  \end{adjustbox}
  \caption{Latency and estimated token cost per response for the evaluated LLMs. Mistral offers the best cost/latency balance for local runs; LLaMA~3 is slower but more accurate.}
  \label{fig:llm_comparison_latency}
\end{figure}

\section{Extended Evaluation Artefacts}
\label{sec:extended_eval_summary}

Full reproducibility materials for the evaluation—covering the hardware/software environment, module-by-module test suite, comparative model benchmarks, and analytics dashboards—are compiled in Appendix~\ref{app:eval_details}. The main text therefore focuses on headline accuracy, reliability, and feedback outcomes, while the appendix preserves the granular evidence required for replication or future audits.

\section{Automation Assets and Test Coverage}
\label{sec:eval_assets}

Every quantitative statement in this chapter is backed by executable code in the repository. The Python unit tests under \codeword{tests/} cover the complete workflow: \codeword{test_pdf_ingestion.py} verifies extraction accuracy for lecturer and student PDFs, \codeword{test_grading_validation.py} checks rubric boundary handling and consensus aggregation, \codeword{test_code_grader_module.py} replays Docker-based execution using deterministic fixtures, \codeword{test_math_grader_module.py} asserts symbolic equivalence via SymPy, and \codeword{test_multimodal_rag_store.py} exercises the FAISS-backed retrieval layer. All tests run with \codeword{pytest -q tests} on GitHub Actions and locally on macOS (Section~\ref{sec:validation_strategy}), producing machine-readable junit logs inside \codeword{logs/}.

The figures in this chapter are regenerated on demand by the pipeline that runs \codeword{python evaluation_reports/generate_raw_data.py} (pseudonymised raw exports), \codeword{python evaluation_reports/export_datasets.py} (sanitised aggregates), and \codeword{python evaluation_reports/generate_figures.py} (PNG rendering). This process recreates all charts (e.g., Figures~\ref{fig:ai_vs_human_accuracy}--\ref{fig:llm_comparison_latency}) from open-source tooling alone. The full source for the test suite and the sanitisation/plotting scripts is reprinted verbatim in Appendix~\ref{app:evaluation_assets}.

\section{Discussion and Limitations}
\label{sec:discussion}

Multi-agent consensus improves accuracy and feedback quality but increases inference cost by a factor of 2.6, with peak GPU memory reaching 18\,GB when agents run locally. The current evaluation still leans on anonymised or synthetic lecturer datasets until the golden benchmark corpus is finalised, and the RAG pipeline does not yet persist all corrections back into the embedding store by default. Explainability depends on well-structured rubrics; poorly formatted rubrics still degrade outcomes. Future work should extend experiments to STEM assignments, explore model distillation for lighter deployments, automate analytics regression checks, close the loop between human corrections and retrieval memory, and introduce a managed MLOps layer to govern any eventual retraining workflows.

\chapter{Conclusion}
\label{chap:conclusion}

This thesis designed, implemented, and evaluated an automated grading framework that combines multi-agent reasoning, retrieval-augmented grounding, and secure code execution into a cohesive educator workflow. The architecture deliberately separates the Streamlit presentation layer from the Python orchestration services and the AI \& data plane, which allowed the solution to satisfy security constraints such as on-premise deployment and transparent audit logging while still delivering near-real-time feedback to lecturers. Through this modularisation, each capability—text grading, code grading, retrieval, explainability, and analytics—can evolve independently without destabilising the end-to-end experience.

Empirical studies on 150 mixed-modality submissions demonstrated that consensus-based grading outperformed a single-model baseline, reducing the mean absolute error from 4.6 to 2.8 points and shrinking variance across repeated runs. The RAG memory, continuously refreshed by human-in-the-loop corrections, improved rubric adherence and explanation quality, enabling the system to surface contextually grounded feedback that lecturers could trust. For programming assignments, the Docker sandbox paired objective unit-test scoring with qualitative LLM review, ensuring that functional correctness and pedagogical feedback were captured in one pass. The accompanying dashboards confirmed that educators benefited from shorter turnaround times and richer analytics on grading reliability.

The work also translated these technical advances into operational assets: ingestion tooling for PDFs and LMS exports and DeepEval-based quality monitoring. These deliverables position the framework as a practical assistant rather than a research prototype and highlight how open-weight models can be responsibly adopted in academic settings that require data locality, transparency, and budget predictability.

Several limitations remain. Evaluation relied on partially synthetic lecturer annotations, so future work should focus on curating gold-standard datasets, expanding coverage to STEM-heavy tasks (e.g., mathematics with symbolic manipulation), and stress-testing the system under multilingual and accessibility constraints. Further, the RAG store currently ingests corrections asynchronously; closing that loop in near real time, introducing automated rubric validation, and exploring lightweight model distillation would reduce operating costs while preserving accuracy. Addressing these gaps will push the framework closer to institutional deployment and broaden its applicability across disciplines.

\clearpage

\appendix

\chapter{Extended Architecture Reference}
\label{chap:extended_architecture}
\label{app:extended_architecture}

% --- Begin inline copy of architecture_appendix.tex ---
\section{Application Architecture}

This document provides a detailed overview of the technical architecture of the automated grading application. It is intended for developers and system administrators. It reflects dual ILIAS/PDF ingestion and the LMS-ready feedback export path.

\medskip\hrule\medskip

\subsection{1. High-Level Overview}

The application is a multi-tiered system composed of a web-based frontend, a robust backend with a relational database, and a sophisticated AI-powered grading engine. 

\begin{verbatim}
graph TD
    A[Browser] --> B{Streamlit Frontend};
    B --> C{Backend Server};
    C --> D[PostgreSQL Database];
    C --> E{AI Grading Engine};
    E --> F[LLM APIs / Local Models];
    E --> G[Vector Store];

    subgraph User Interface
        B
    end

    subgraph Core Logic & Data
        C
        D
    end

    subgraph AI Processing
        E
        F
        G
    end
\end{verbatim}

\begin{itemize}
\item \textbf{Frontend:} A multi-page Streamlit application provides the user interface for professors and teaching assistants, including dual PDF/ILIAS upload and a feedback download control.
\item \textbf{Backend:} A Python backend orchestrates the application logic, handling user requests, database interactions, calls to the grading engine, and the feedback ZIP builder.
  \item \textbf{Database:} A PostgreSQL database stores all persistent data, including user information, assignment details, student submissions, and grading results.
  \item \textbf{AI Grading Engine:} A modular engine that leverages Large Language Models (LLMs) to perform the grading. It includes specialized modules for different types of assignments.
  \item \textbf{Vector Store:} A FAISS-based vector database stores embeddings of past grading decisions to provide historical context (RAG).
\end{itemize}

\medskip\hrule\medskip

\subsection{2. Frontend (Streamlit Application)}

The frontend is built using Streamlit and is organized into a multi-page application structure. 

\subsubsection{\textbf{2.1. Directory Structure}}

\begin{itemize}
  \item \codeword{app.py}: The main entry point of the Streamlit application. It handles routing and global configuration.
\item \codeword{pages/}: This directory contains the individual pages of the application, such as:
\item \codeword{0\_auth.py}: User authentication.
\item \codeword{1\_upload\_data.py}: Interface for professors to upload assignment PDFs, ingest ILIAS ZIPs, and for students to submit their work.
\item \codeword{2\_grading\_result.py}: Displays the results of the grading process and exposes the Download Feedback (ILIAS ZIP) control.
\item \codeword{3\_dashboard.py}: Cohort analytics, KPI exports (CSV/PDF), and drift/variance filters.
\end{itemize}

\subsubsection{\textbf{2.2. Authentication and Session Management}}

Authentication is managed through the \codeword{auth} module. User session data, including login status and user roles, is stored in \codeword{st.session\_state}. This ensures that sensitive pages are protected and that the application context is maintained as the user navigates between pages. 

\medskip\hrule\medskip

\subsection{3. Backend and Database}

The backend logic is tightly integrated with the PostgreSQL database, which serves as the single source of truth. 

\subsubsection{\textbf{3.1. Database Schema}}

The database consists of several key tables: 

\begin{itemize}
  \item \codeword{users}: Stores user credentials and roles (e.g., professor, assistant).
  \item \codeword{prof\_data}: Contains the data uploaded by professors, including the course, assignment number, questions, ideal answers, and grading rubrics.
  \item \codeword{student\_data}: Stores student submissions, linked to a specific assignment.
  \item \codeword{grading\_results}: This is the central table for storing the output of the grading engine. It includes the original student answer, the AI-generated score and feedback (\codeword{old\_feedback}), and any human-in-the-loop corrections (\codeword{new\_feedback}, \codeword{new\_score}).
\end{itemize}

\subsubsection{\textbf{3.2. Data Handler (\codeword{database/postgres\_handler.py})}}

All database interactions are abstracted away by the \codeword{PostgresHandler} class. This class provides a standardized interface for executing queries (SELECT, INSERT, UPDATE) and managing connections. This centralized approach ensures consistency and simplifies database management. 

\medskip\hrule\medskip

\subsection{4. The AI Grading Engine (\codeword{grader\_engine/})}

This is the core of the application where the AI-powered grading takes place. 

\begin{verbatim}
graph TD
    A[Request from Backend] --> B{Router};
    B --> |Text| C[Text Grader];
    B --> |Code| D[Code Grader];
    B --> |Multi-Agent| E[Multi-Agent Grader];
    B --> |Multimodal| F[Multimodal Grader];

    C --> G{RAG Integration};
    E --> G;
    
    G --> H[Vector Store];
    C --> I{Explainability Module};
    E --> I;

    I --> J[Final Grade & Feedback];
\end{verbatim}

\subsubsection{\textbf{4.1. Router (\codeword{router.py})}}

The \codeword{Router} is the main entry point for the grading engine. It inspects the assignment type (e.g., text, code, multimodal) and directs the request to the appropriate specialized grader. 

\subsubsection{\textbf{4.2. Multi-Agent Grader (\codeword{multi\_agent.py})}}

To improve reliability and reduce bias, the system employs a multi-agent consensus mechanism: 

\begin{enumerate}
  \item \textbf{Agent Roles:} Multiple LLM agents are instantiated with slightly different personas (e.g., a "strict" grader, a "lenient" grader, a "by-the-book" grader).
  \item \textbf{Concurrent Grading:} These agents grade the same submission in parallel using \codeword{concurrent.futures}.
  \item \textbf{Consensus:} The scores and feedback from each agent are collected. The final score is typically the mean or median of the agents' scores, and the variance is used as a confidence metric.
  \item \textbf{Final Review:} A final "meta-agent" reviews the collected feedback and synthesizes it into a single, high-quality explanation.
\end{enumerate}

\subsubsection{\textbf{4.3. Code Grader (\codeword{code\_grader.py})}}

The code grader provides a secure and comprehensive way to evaluate programming assignments: 

\begin{enumerate}
  \item \textbf{Sandboxed Execution:} Student code is executed within a secure, isolated Docker container to prevent any potential security risks.
  \item \textbf{Unit Testing:} The code is run against a set of predefined \codeword{unittest} cases. The results (pass/fail) form the basis of the objective score.
  \item \textbf{Qualitative Feedback:} An LLM analyzes the student's code, along with the unit test results, to provide qualitative feedback on code style, efficiency, best practices, and potential areas for improvement.
\end{enumerate}

\subsubsection{\textbf{4.4. RAG Integration (\codeword{rag\_integration.py})}}

To ensure consistency over time, the grading engine uses Retrieval Augmented Generation (RAG): 

\begin{enumerate}
  \item \textbf{Vector Store:} When a human-in-the-loop correction is made, the grading context (question, student answer, corrected feedback) is embedded and stored in a FAISS vector store.
  \item \textbf{Contextual Retrieval:} When grading a new submission, the engine queries the vector store to find the most similar previously graded examples.
  \item \textbf{Prompt Injection:} These historical examples are injected into the LLM prompt, providing valuable context that helps the model "remember" how similar cases were graded in the past.
\end{enumerate}

\subsubsection{\textbf{4.5. Explainability Module (\codeword{explainer.py})}}

This module is responsible for generating the detailed, rubric-aligned justifications for the final score. It takes the grading results and structures them into a clear, easy-to-understand format that explains which criteria were met and why. Concretely, it:
\begin{itemize}
  \item Normalises rubric criterion IDs/names and binds each score to a short rationale, reducing ambiguity across courses or terminology variants.
  \item Builds a structured payload (JSON + markdown) that lists strengths and gaps per criterion, highlights missing evidence, and mirrors the order of the uploaded rubric so reviewers can trace each decision.
  \item Injects RAG citations (IDs of retrieved exemplars) and the agent confidence value, giving instructors an at-a-glance signal of certainty and provenance.
  \item Enforces safety checks to avoid hallucinated criteria by cross-validating every referenced criterion against the stored rubric JSON before rendering.
  \item Emits a human-readable block for the UI and a machine-readable block that can be exported in LMS-ready PDFs/ZIPs without further transformation.
\end{itemize}

\medskip\hrule\medskip

% --- End inline copy of architecture_appendix.tex ---

\chapter{Detailed Data Flow Pipelines}
\label{chap:detailed_data_flow}
\label{app:data_flow}

% --- Begin inline copy of data_flow_appendix.tex ---
\section{Detailed Data Flow Diagrams}

This document provides a series of detailed diagrams illustrating the data flow through every major pipeline in the Automated Grading Framework. 

\medskip\hrule\medskip

\subsection{1. Data Ingestion Pipeline (ETL)}

This pipeline describes how course materials (from professors) and student submissions are processed from raw PDFs \emph{or ILIAS LMS archives} into structured, queryable data in the PostgreSQL database. 

\begin{verbatim}
sequenceDiagram
    participant Prof as Professor
    participant Stu as Student
    participant UI as Streamlit UI (st.file_uploader)
    participant Parser as PDF/ILIAS Parser (PyMuPDF / regex / parse_ilias_zip)
    participant Backend as Backend Logic (1_upload_data.py)
    participant DB as PostgresHandler
    participant PG as PostgreSQL

    rect rgb(224,255,255)
        Prof->>UI: Upload professor PDF (questions, rubric) OR ILIAS ZIP export
        UI->>Backend: Pass file bytes
        Backend->>Parser: Extract text / unpack ZIP
        Parser-->>Backend: Return raw text + manifest
        Backend->>Parser: Parse into structured data
        Parser-->>Backend: Return JSON structure
        Backend->>DB: execute_query INSERT into prof_data
        DB->>PG: SQL insert
    end

    rect rgb(255,250,205)
        Stu->>UI: Upload submission PDF
        UI->>Backend: Pass file bytes
        Backend->>Parser: Extract text
        Parser-->>Backend: Return raw text
        Backend->>DB: execute_query INSERT into student_data
        DB->>PG: SQL insert
    end
\end{verbatim}

\textbf{Description:} 

The ingestion process happens in two distinct (but similar) workflows: 

\begin{itemize}
  \item \textbf{Professor Workflow:}
\begin{enumerate}
  \item The professor uploads a PDF containing the assignment details.
  \item The backend uses \codeword{PyMuPDF} to extract text and regular expressions (\codeword{re}) to parse it into a structured format (questions, rubric, etc.).
  \item The structured data is saved to the \codeword{prof\_data} table in the PostgreSQL database.
\end{enumerate}
\end{itemize}

\begin{itemize}
  \item \textbf{Student Workflow:}
\begin{enumerate}
  \item The student uploads their submission as a PDF.
  \item The backend uses \codeword{PyMuPDF} to extract the raw text of their answer.
  \item This text is saved to the \codeword{student\_data} table, linked to the appropriate assignment.
\end{enumerate}
\end{itemize}

\medskip\hrule\medskip

\subsection{2. Core Grading Pipeline}

This diagram shows the end-to-end process when a professor initiates a grading job, culminating in the results being displayed on the screen and allowing LMS-ready download afterwards. 

\begin{verbatim}
graph TD
    A[Start: User clicks Grade] --> B(Backend Logic 2_grading_result.py)
    B --> C[Fetch submissions from student_data]
    B --> D[Fetch rubric from prof_data or ILIAS manifest]
    C --> E[AI Grading Engine]
    D --> E
    E --> F[Agentic Pipeline]
    F --> G(Grading Result score, feedback, confidence)
    G --> H[Insert into grading_results]
    H --> I[Streamlit UI updates]
    I --> J[Professor reviews grades]
    J --> K[Download Feedback ZIP (ILIAS-ready)]
\end{verbatim}

\textbf{Description:} 
\begin{enumerate}
  \item The process begins when the user starts a grading job from the Streamlit UI.
  \item The backend fetches the relevant student submissions and the corresponding professor-defined rubric from the PostgreSQL database.
  \item This data is dispatched as a job to the \textbf{AI Grading Engine}.
  \item The engine performs the complex grading task (detailed in the next section).
  \item The final, aggregated result is returned to the backend.
  \item The backend saves this result to the \codeword{grading\_results} table for persistence.
  \item The UI is updated to display the new grades in an editable table, completing the flow.
\end{enumerate}

\medskip\hrule\medskip

\subsection{3. Agentic Grading Engine Pipeline (Deep Dive)}

This diagram provides a detailed look inside the AI Grading Engine itself, showing how a single submission is processed by the multi-agent system. 

\begin{verbatim}
graph TD
    A[Grading Job Received] --> B[Router]
    B --> |text| C[Multi-Agent Grader]
    B --> |code| D[Code Grader]

    subgraph Code Grading Sandbox
        D --> D1[Create Dockerfile + unittests]
        D1 --> D2[docker build & docker run]
        D2 --> D3[Capture unittest stdout]
        D3 --> D4[Parse score]
        D4 --> D5[LLM feedback]
        D5 --> D6[Final code grade]
    end

    subgraph Multi-Agent Text Grading
        C --> C1[RAG retrieval]
        C1 --> C2[Similar past corrections]
        C2 --> C3[Context bundle]

        C --> C4[Concurrent agents]
        C3 --> C4
        C4 --> Agent1[Agent α]
        C4 --> Agent2[Agent β]
        C4 --> AgentN[Agent γ]

        subgraph Single Agent Execution
            Agent1 --> P1[Prompt Builder]
            P1 --> L1[LLM call]
            L1 --> R1[Score + feedback]
        end

        R1 --> C5[Aggregator]
        Agent2 --> C5
        AgentN --> C5

        C5 --> C6[Mean/median + confidence]
        C5 --> C7[Meta-agent synthesis]
        C6 --> C8[Final text grade]
        C7 --> C8
    end

    D6 --> Z[Return Result]
    C8 --> Z
\end{verbatim}

\textbf{Description:} 
\begin{itemize}
  \item \textbf{Routing:} The engine first routes the job based on the assignment type.
  \item \textbf{Code Grading:} For code, it enters a secure Docker sandbox to run unit tests for an objective score, then uses an LLM to generate qualitative feedback on the code itself.
  \item \textbf{Text Grading:} For text, the process is more complex:
\begin{enumerate}
  \item \textbf{RAG:} The RAG module first retrieves relevant historical grading examples from the FAISS vector store.
  \item \textbf{Concurrent Grading:} Multiple AI agents, each with a different persona, are spawned in parallel. They each receive the submission, the rubric, and the context from the RAG module.
  \item \textbf{Aggregation:} Once all agents complete, their individual scores are statistically aggregated (e.g., taking the median). The variance in their scores is used as a confidence metric.
  \item \textbf{Synthesis:} A final "meta-agent" reviews the feedback from all other agents and synthesizes it into a single, high-quality, comprehensive piece of feedback for the student.
\end{enumerate}
\end{itemize}

\medskip\hrule\medskip

\subsection{4. Human-in-the-Loop (HITL) \& RAG Update Pipeline}

This pipeline shows what happens when a professor makes a correction to an AI-generated grade. This is a critical feedback loop for the system. 

\begin{verbatim}
sequenceDiagram
    participant User as Professor
    participant UI as Streamlit UI (`st.data_editor`)
    participant Backend as Backend Logic
    participant DB as PostgresHandler
    participant PG as PostgreSQL Database
    participant RAG as RAG Update Module
    participant Embed as Embedding Model
    participant FAISS as FAISS Vector Store

    User->>UI: Edits a score or feedback field in the table
    UI->>Backend: On change, submits the full row of data
    Backend->>DB: Calls `UPDATE grading_results` to set `new_score` and `new_feedback`
    DB->>PG: Executes SQL UPDATE
    PG-->>DB: Confirms update
    DB-->>Backend: Success

    Backend->>RAG: Triggers RAG update with the corrected data
    RAG->>Embed: Creates a document from the question and corrected feedback
    Embed->>RAG: Generates a vector embedding for the document
    RAG->>FAISS: Adds the new vector to the FAISS index
    FAISS-->>RAG: Confirms save
    RAG-->>Backend: Success
    Backend-->>UI: Displays "Correction Saved" toast
\end{verbatim}

\textbf{Description:} 
\begin{enumerate}
  \item The professor edits a grade directly in the Streamlit UI.
  \item The backend receives the corrected data.
  \item It first updates the \codeword{grading\_results} table in the PostgreSQL database, preserving both the original AI grade (\codeword{old\_feedback}) and the new human-verified grade (\codeword{new\_feedback}).
  \item Next, this correction is used to improve the RAG system. The corrected feedback is converted into a vector embedding.
  \item This new vector is added to the FAISS vector store, making this human-verified example available for all future grading tasks to improve their context and accuracy.
\end{enumerate}

\medskip\hrule\medskip
% --- End inline copy of data_flow_appendix.tex ---

\chapter{Software Design Specification}
\label{chap:software_design_spec}

% --- Begin inline copy of design_appendix.tex ---
\section{Software Design Document}

This document provides a detailed technical design for the Automated Grading Framework. It is divided into two main sections: a High-Level Design (HLD) that describes the overall system architecture and a Low-Level Design (LLD) that provides detailed specifications for each component. 

\medskip\hrule\medskip

\subsection{1. High-Level Design (HLD)}

The High-Level Design outlines the major components of the system, their interactions, and the technology stack. 

\subsubsection{1.1. System Architecture Overview}

The application is a monolithic web application built on a multi-tiered architecture. It is designed for modularity to allow for future expansion and potential migration to a microservices-based architecture. 

\begin{verbatim}
graph TD
    subgraph User Layer
        A[User's Browser] -->|HTTPS| B{Streamlit Web UI}
    end

    subgraph Application Layer
        B -->|Function Calls| C{Backend Logic}
        C -->|SQL Queries| D[PostgreSQL Database]
        C -->|Grading Jobs| E{AI Grading Engine}
    end

    subgraph AI & Data Layer
        E -->|API Calls / Local Inference| F{LLMs}
        E -->|Embeddings & Retrieval| G[FAISS Vector Store]
        E -->|Secure Execution| H{Docker Sandbox}
    end

    style B fill:#f9f9f9,stroke:#333,stroke-width:2px
    style D fill:#cde4f8,stroke:#333,stroke-width:2px
    style G fill:#cde4f8,stroke:#333,stroke-width:2px
\end{verbatim}

\textbf{Component Responsibilities:} 

\begin{enumerate}
  \item \textbf{Streamlit Web UI (Frontend):} A multi-page application that serves as the primary user interface. It handles user input, authentication, data visualization, and presents the grading results. It is stateful within a user session.
  \item \textbf{Backend Logic (Integrated with Frontend):} In the current architecture, the backend logic is tightly coupled with the Streamlit frontend. It processes user requests, orchestrates database interactions via the \codeword{PostgresHandler}, and initiates grading jobs.
  \item \textbf{PostgreSQL Database:} The central repository for all persistent data. It stores user accounts, course materials, student submissions, and all grading results, including human-in-the-loop corrections.
  \item \textbf{AI Grading Engine:} The core of the system. It contains the logic for different grading modalities (text, code, etc.) and orchestrates the complex interactions between LLMs, RAG, and other AI components.
  \item \textbf{LLMs (Large Language Models):} The underlying intelligence. The system is designed to be model-agnostic, capable of interfacing with various local models (e.g., via MLX) or external APIs (like OpenAI or Google's Gemini).
  \item \textbf{FAISS Vector Store:} An on-disk vector database used to implement Retrieval Augmented Generation (RAG). It stores embeddings of past human-corrected gradings to improve future consistency.
  \item \textbf{Docker Sandbox:} A secure, containerized environment for executing untrusted student code against predefined unit tests, preventing any potential harm to the host system.
\end{enumerate}

\subsubsection{1.2. Technology Stack}

| Component             | Technology/Framework  | Justification                                                                                             | 
| --------------------- | --------------------- | --------------------------------------------------------------------------------------------------------- | 
| \textbf{Frontend/Backend}  | Streamlit             | Enables rapid development of data-centric web applications with Python. Ideal for prototyping and internal tools. | 
| \textbf{Database}          | PostgreSQL            | A robust, open-source relational database that handles structured data and complex queries effectively.     | 
| \textbf{AI Orchestration}  | LangChain             | Simplifies the development of LLM-powered applications, particularly for multi-agent and RAG workflows.  | 
| \textbf{Local AI/ML}       | MLX (for Apple Silicon) | A highly efficient framework for running machine learning models on Apple Silicon, used for local inference. | 
| \textbf{PDF Processing}    | PyMuPDF (\codeword{fitz})      | Chosen for its high speed and accuracy in extracting text and metadata from PDF files.                     | 
| \textbf{Vector Store}      | FAISS (from Facebook AI) | An efficient library for similarity search and clustering of dense vectors, forming the core of the RAG module. | 
| \textbf{Code Sandboxing}   | Docker                | The industry standard for creating isolated, reproducible environments, ensuring secure code execution.       | 

\subsubsection{1.3. User Roles and Permissions}

Two primary user roles are defined within the system: 

\begin{itemize}
  \item \textbf{Professor:} Has full access to the system. Can upload course materials, view all student submissions, initiate grading, review and modify AI-generated grades, and view analytics.
  \item \textbf{Student (Future Scope):} While the current system focuses on the professor's workflow, a student role would have restricted access, limited to uploading their own submissions and viewing their final, published grades.
\end{itemize}

\subsubsection{1.4. Data Flow Diagram}

This diagram illustrates the end-to-end flow for a typical text-based assignment grading task. 

\begin{verbatim}
sequenceDiagram
    participant User as Professor
    participant UI as Streamlit UI
    participant Backend as Backend Logic
    participant DB as PostgreSQL DB
    participant Engine as AI Grading Engine
    participant ILIAS as ILIAS Parser / Exporter

    User->>UI: Uploads Assignment PDF or ILIAS ZIP
    UI->>Backend: Parses PDF/ZIP, extracts data & manifest
    Backend->>ILIAS: Normalise filenames, map to students
    ILIAS-->>Backend: JSON/manifest
    Backend->>DB: INSERT into `prof_data` table
    DB-->>Backend: Confirms save

    User->>UI: Uploads Student Submissions (PDFs or ZIP)
    UI->>Backend: Parses PDFs / ZIP contents
    Backend->>DB: INSERT into `student_data` table
    DB-->>Backend: Confirms save

    User->>UI: Clicks "Start Grading"
    UI->>Backend: Initiates grading job
    Backend->>Engine: Dispatches job with submissions and rubric
    
    Engine->>Engine: Spawns multiple AI agents
    loop For Each Agent
        Engine->>Engine: Retrieves similar past examples (RAG) from Vector Store
        Engine->>Engine: Constructs detailed prompt
        Engine->>Engine: Grades submission using LLM
    end
    Engine->>Engine: Aggregates results to form consensus grade
    
    Engine-->>Backend: Returns final grade, feedback, and explanation
    Backend->>DB: INSERT into `grading_results`

    Backend-->>UI: Displays grading results
    User->>UI: Reviews and (optionally) corrects a grade
    UI->>Backend: Submits correction
    Backend->>DB: UPDATE `grading_results` (sets `new_score`, `new_feedback`)
    Backend->>Engine: Stores correction in Vector Store for future RAG
    User->>UI: Clicks "Download Feedback"
    UI->>ILIAS: Request LMS-ready ZIP
    ILIAS-->>User: Returns ILIAS-compatible archive
\end{verbatim}

\subsubsection{1.5. Future-State Cloud Architecture (Google Cloud)}

For a production-grade system, migrating from a local, monolithic architecture to a scalable cloud-based one is recommended. Google Cloud offers a suite of services that align perfectly with this application's needs. 

\begin{verbatim}
graph TD
    subgraph User & CDN
        A[User Browser] --> B[Cloud CDN & Load Balancer]
    end

    subgraph Application Services
        B --> C[Cloud Run Frontend]
        B --> D[Cloud Functions Backend API]
    end

    subgraph Data & Storage
        C --> E[Cloud SQL for PostgreSQL]
        D --> E
        D --> F[Cloud Storage Assets]
    end

    subgraph AI Platform
        D --> G[Vertex AI Pipelines]
        G --> H[Vertex AI Model Garden]
        G --> I[Vertex AI Vector Search]
        G --> J[Cloud Run Jobs for Code Execution]
    end

    C --> F
\end{verbatim}

\textbf{Migration Benefits:} 

\begin{itemize}
  \item \textbf{Scalability \\& Reliability:} Cloud Run and Cloud Functions automatically scale with traffic, including scaling to zero, which is highly cost-effective. Cloud SQL provides managed, high-availability PostgreSQL.
  \item \textbf{Decoupled Frontend/Backend:} Separating the Streamlit frontend (served via Cloud Run) from the backend logic (refactored into API endpoints in Cloud Functions) creates a more robust and scalable microservices architecture.
  \item \textbf{Centralized MLOps with Vertex AI:}
    \item \textbf{Vertex AI Pipelines:} The entire grading and evaluation workflow can be defined as a reproducible pipeline, making it easier to manage, version, and trigger.
  \item \textbf{Vertex AI Vector Search:} A fully managed, highly scalable, and low-latency vector database to replace the local FAISS store.
  \item \textbf{Model Management:} Use the Vertex AI Model Garden for pre-trained models or host custom-trained models, streamlining model deployment and versioning.
  \item \textbf{Secure, Serverless Execution:} Replace the local Docker sandbox with ephemeral Cloud Run Jobs for even more secure and scalable code execution.
\end{itemize}

\medskip\hrule\medskip

\subsection{2. Low-Level Design (LLD)}

The Low-Level Design provides detailed specifications for individual modules, data structures, and logic. 

\subsubsection{2.1. Database Schema (PostgreSQL)}

Below is a detailed breakdown of the core tables. All text fields are \codeword{TEXT} to accommodate variable lengths. Timestamps are used for auditing. 

\textbf{Table: \codeword{users}} 

| Column          | Data Type             | Constraints                                   | Description                                   | 
| --------------- | --------------------- | --------------------------------------------- | --------------------------------------------- | 
| \codeword{id}            | \codeword{SERIAL}              | \codeword{PRIMARY KEY}                                 | Unique identifier for the user.               | 
| \codeword{username}      | \codeword{VARCHAR(255)}        | \codeword{UNIQUE}, \codeword{NOT NULL}                          | User's chosen login name.                     | 
| \codeword{password\_hash} | \codeword{VARCHAR(255)}        | \codeword{NOT NULL}                                    | Hashed password for secure storage.           | 
| \codeword{role}          | \codeword{VARCHAR(50)}         | \codeword{DEFAULT 'professor'}                         | User's role (e.g., 'professor').              | 
| \codeword{created\_at}    | \codeword{TIMESTAMP WITH TIME ZONE} | \codeword{DEFAULT CURRENT\_TIMESTAMP}                   | Timestamp of account creation.                | 

\textbf{Table: \codeword{prof\_data}} 

| Column          | Data Type             | Constraints                                   | Description                                   | 
| --------------- | --------------------- | --------------------------------------------- | --------------------------------------------- | 
| \codeword{id}            | \codeword{SERIAL}              | \codeword{PRIMARY KEY}                                 | Unique identifier for the dataset entry.      | 
| \codeword{course}        | \codeword{VARCHAR(255)}        | \codeword{NOT NULL}                                    | The course name (e.g., 'CS101').              | 
| \codeword{assignment\_no} | \codeword{VARCHAR(255)}        | \codeword{NOT NULL}                                    | The assignment number (e.g., 'HW1').          | 
| \codeword{question}      | \codeword{TEXT}                | \codeword{NOT NULL}                                    | The text of the assignment question.          | 
| \codeword{ideal\_answer}  | \codeword{TEXT}                |                                               | The professor-provided ideal answer.          | 
| \codeword{rubric}        | \codeword{JSONB}               | \codeword{NOT NULL}                                    | The grading rubric stored in JSON format.     | 
| \codeword{uploaded\_by}   | \codeword{INTEGER}             | \codeword{REFERENCES users(id)}                        | Foreign key linking to the \codeword{users} table.     | 
| \codeword{uploaded\_at}   | \codeword{TIMESTAMP WITH TIME ZONE} | \codeword{DEFAULT CURRENT\_TIMESTAMP}                   | Timestamp of the upload.                      | 

\textbf{Table: \codeword{grading\_results}} 

| Column          | Data Type             | Constraints                                   | Description                                   | 
| --------------- | --------------------- | --------------------------------------------- | --------------------------------------------- | 
| \codeword{id}            | \codeword{SERIAL}              | \codeword{PRIMARY KEY}                                 | Unique identifier for the grading result.     | 
| \codeword{course}        | \codeword{VARCHAR(255)}        | \codeword{NOT NULL}                                    | The course name.                              | 
| \codeword{assignment\_no} | \codeword{VARCHAR(255)}        | \codeword{NOT NULL}                                    | The assignment number.                        | 
| \codeword{question}      | \codeword{TEXT}                | \codeword{NOT NULL}                                    | The question being graded.                    | 
| \codeword{student\_answer}| \codeword{TEXT}                | \codeword{NOT NULL}                                    | The full text of the student's answer.        | 
| \codeword{old\_score}     | \codeword{INTEGER}             |                                               | The original score given by the AI.           | 
| \codeword{old\_feedback}  | \codeword{TEXT}                |                                               | The original feedback generated by the AI.    | 
| \codeword{new\_score}     | \codeword{INTEGER}             |                                               | The human-corrected score (if any).           | 
| \codeword{new\_feedback}  | \codeword{TEXT}                |                                               | The human-corrected feedback (if any).        | 
| \codeword{graded\_at}     | \codeword{TIMESTAMP WITH TIME ZONE} | \codeword{DEFAULT CURRENT\_TIMESTAMP}                   | Timestamp when the grading was performed.     | 
| \codeword{corrected\_by}  | \codeword{INTEGER}             | \codeword{REFERENCES users(id)}                        | Foreign key to the user who made the correction. | 

\subsubsection{2.2. AI Grading Engine (\codeword{grader\_engine/}) - Module Breakdown}

\paragraph{\textbf{\codeword{multi\_agent.py}}}

\begin{itemize}
  \item \textbf{Class: \codeword{MultiAgentGrader}}
  \item \codeword{\_\_init\_\_(self, agents: List[Callable], aggregator: Callable)}: Initializes with a list of agent functions and an aggregator function.
  \item \codeword{grade\_submission(self, submission: str, rubric: dict, context: str) -> dict}:
\begin{enumerate}
  \item Uses \codeword{concurrent.futures.ThreadPoolExecutor} to run each agent function in parallel.
  \item Each agent function receives the submission, rubric, and RAG context.
  \item Collects the results (score, feedback) from all agents.
  \item Passes the list of results to the \codeword{aggregator} function.
  \item Returns the aggregated result.
\end{enumerate}
\end{itemize}

\begin{itemize}
  \item \textbf{Function: \codeword{create\_grading\_agent(persona: str, llm: BaseLLM) -> Callable}}
  \item A factory function that takes a persona (e.g., "You are a strict but fair grader...") and an LLM instance.
  \item It constructs a LangChain prompt template that includes placeholders for the persona, submission, rubric, and RAG context.
  \item Returns a callable function (a LangChain chain) that executes the grading task for that specific agent.
\end{itemize}

\begin{itemize}
  \item \textbf{Function: \codeword{aggregate\_results(results: List[dict]) -> dict}}
  \item Calculates the mean, median, and standard deviation of the scores.
  \item Uses a separate LLM call to a "meta-agent" to synthesize the feedback from all agents into a single, high-quality, and comprehensive explanation.
  \item Returns a dictionary containing the final score, synthesized feedback, and a confidence metric based on score variance.
\end{itemize}

\paragraph{\textbf{\codeword{code\_grader.py}}}

\begin{itemize}
  \item \textbf{Class: \codeword{CodeGrader}}
  \item \codeword{grade\_submission(self, student\_code: str, test\_cases: str) -> dict}:
\begin{enumerate}
  \item \textbf{Prepare Docker Environment:} Creates a temporary directory containing the \codeword{student\_code}, the \codeword{test\_cases} (as a \codeword{unittest} file), and a \codeword{Dockerfile}.
  \item \textbf{Build Docker Image:} Runs \codeword{docker build} to create a self-contained image.
  \item \textbf{Run Docker Container:} Executes \codeword{docker run} on the image. The container runs the \codeword{unittest} suite and captures the \codeword{stdout} and \codeword{stderr} to a results file.
  \item \textbf{Parse Test Results:} Reads the output from the container to determine which tests passed and failed, calculating an objective score.
  \item \textbf{Generate Qualitative Feedback:} Makes an LLM call with a specialized prompt containing the student code, the test cases, and the pass/fail results. The prompt asks the LLM to provide feedback on style, efficiency, and correctness, explaining why the tests failed.
  \item \textbf{Cleanup:} Removes the Docker container, image, and temporary directory.
  \item Returns a dictionary with the objective score, a list of passed/failed tests, and the LLM-generated qualitative feedback.
\end{enumerate}
\end{itemize}

\subsubsection{2.3. Finetuning Workflow (\codeword{pages/3\_fine\_tuning.py})}

This page acts as a user-friendly orchestrator for a complex MLOps task. 

\begin{itemize}
  \item \textbf{Function: \codeword{generate\_training\_data()}}
\begin{enumerate}
  \item Instantiates a \codeword{PostgresHandler}.
  \item Executes a specific SQL query (as detailed in the file) to fetch all rows from \codeword{grading\_results} where \codeword{new\_feedback} is not null and differs from \codeword{old\_feedback}.
  \item Iterates through the results, formatting each one into a JSON object with a single key, \codeword{"text"}.
  \item The value for \codeword{"text"} is a long string formatted according to the \codeword{PROMPT\_TEMPLATE}, injecting the question, student answer, ideal answer/rubric, and critically, the human-written \codeword{corrected\_feedback} as the model's target response.
  \item Concatenates these JSON objects, separated by newlines, to create a valid JSONL file string.
  \item Stores this string in \codeword{st.session\_state} to make it available for download.
\end{enumerate}
\end{itemize}

\begin{itemize}
  \item \textbf{UI Logic:}
  \item A button, \codeword{"Generate Training Data"}, triggers the \codeword{generate\_training\_data} function.
  \item Upon successful generation (if \codeword{st.session\_state['generated\_training\_data']} exists), a \codeword{st.download\_button} is rendered, allowing the user to save the \codeword{.jsonl} file.
  \item A \codeword{st.code} block displays the full, static content of the \codeword{colab\_finetune.py} script, providing a clear, read-only view with a copy button for user convenience.
\end{itemize}
% --- End inline copy of design_appendix.tex ---

\chapter{Evaluation Playbook}
\label{chap:evaluation_playbook}

% --- Begin inline copy of evaluation_appendix.tex ---
\section{Comprehensive Evaluation Strategy}

This document outlines the detailed, module-by-module evaluation plan for the Automated Grading Framework. For each component, it analyzes the available evaluation options and provides a clear justification for the chosen, open-source method. 

\medskip\hrule\medskip

\subsubsection{1. Data Ingestion \& ETL Module (\codeword{1\_upload\_data.py})}

\begin{itemize}
  \item \textbf{What We Need to Evaluate:} The accuracy of the \codeword{PyMuPDF} and regular expression-based parser in correctly extracting and structuring the \textbf{questions, rubric, and student answers} from various PDF files.
\end{itemize}

\begin{itemize}
  \item \textbf{Available Options:}
\begin{enumerate}
  \item \textbf{Manual Spot-Checking:}
\begin{itemize}
  \item \emph{Advantages:} Simple to perform, requires no setup, fast for a very small number of documents.
  \item \emph{Disadvantages:} Not scalable, highly subjective, not statistically valid, and likely to miss subtle but critical edge-case errors (e.g., incorrect parsing of a specific rubric format).
\begin{enumerate}
  \item \textbf{Error Rate Monitoring:}
\begin{itemize}
  \item \emph{Advantages:} Easy to implement by wrapping the parsing logic in a try-except block; effectively catches 100\% of catastrophic failures where the program would otherwise crash.
  \item \emph{Disadvantages:} Provides a false sense of security. It cannot detect logical errors, such as misattributing an answer to the wrong question or failing to extract the last item in a rubric. The parser can "succeed" but still produce garbage data.
\begin{enumerate}
  \item \textbf{Golden Dataset Testing:}
\begin{itemize}
  \item \emph{Advantages:} Provides objective, quantifiable, and reproducible metrics (e.g., F1-score, Precision). An automated script compares the parser's output against a "perfect" ground truth, catching both catastrophic and subtle logical errors. It enables regression testing to ensure future changes don't break existing functionality.
  \item \emph{Disadvantages:} Requires a significant upfront investment of time to create a diverse and representative "golden dataset" of PDFs and their corresponding ideal JSON outputs.
\end{itemize}
\end{enumerate}
\end{itemize}
\end{enumerate}
\end{itemize}
\end{enumerate}
\end{itemize}

\begin{itemize}
  \item \textbf{Chosen Method: Golden Dataset Testing}
  \item \textbf{Open-Source Confirmation:} This is a methodology, and its implementation will exclusively use open-source tools, primarily \textbf{\codeword{pytest}} for the testing framework and Python's standard libraries for file handling and comparison.
  \item \textbf{Justification:} The quality of the data entering the system is non-negotiable. While it requires upfront effort, the Golden Dataset method is the only one that provides true, quantifiable confidence in the parser's accuracy. It moves from "it seems to work" to "it is 99.2\% accurate on these 15 document types." This rigor is essential for a reliable application and justifies the initial time investment.
\end{itemize}

\medskip\hrule\medskip

\subsection{Core Grader Evaluations}

\subsubsection{2.1. Evaluation of the Multi-Agent Text Grader}

\begin{itemize}
  \item \textbf{What We Need to Evaluate:}
\begin{enumerate}
  \item \textbf{Numeric Score Accuracy:} Comparison of the AI's score to a human expert's score.
  \item \textbf{Feedback Quality:} The relevance, helpfulness, and factual correctness of the generated text.
  \item \textbf{Consensus Reliability (RQ1):} The statistical consistency among the different AI agents.
\end{enumerate}
\end{itemize}

\begin{itemize}
  \item \textbf{Available Options for Feedback Quality:}
\begin{enumerate}
  \item \textbf{Lexical Similarity Metrics (e.g., BLEU, ROUGE):}
\begin{itemize}
  \item \emph{Advantages:} Simple to calculate, fast, and completely objective.
  \item \emph{Disadvantages:} Fundamentally unsuited for this task. They only measure n-gram overlap with a reference answer, not semantic meaning, factual accuracy, or logical reasoning. They cannot determine if feedback is helpful or even correct.
\begin{enumerate}
  \item \textbf{LLM-as-a-Judge (Proprietary):}
\begin{itemize}
  \item \emph{Advantages:} Highly scalable and capable of evaluating complex, abstract qualities of the text.
  \item \emph{Disadvantages:} Often relies on closed-source, proprietary models (e.g., GPT-4), which violates our open-source constraint. It can be expensive, and the "judge" model may have its own biases.
\begin{enumerate}
  \item \textbf{DeepEval (Open-Source Framework):}
\begin{itemize}
  \item \emph{Advantages:} It is \textbf{open-source}, specifically designed for evaluating LLM outputs, and provides metrics for the exact problems we face, such as \codeword{Faithfulness} (fact-checking against a context) and \codeword{AnswerRelevancy}. It integrates cleanly with \codeword{pytest} for automated testing.
  \item \emph{Disadvantages:} Requires some configuration and a clear understanding of what each metric is measuring to be used effectively.
\end{itemize}
\end{enumerate}
\end{itemize}
\end{enumerate}
\end{itemize}
\end{enumerate}
\end{itemize}

\begin{itemize}
  \item \textbf{Chosen Method: A Hybrid of DeepEval and Open-Source Statistical Libraries}
  \item \textbf{Open-Source Confirmation:} This approach exclusively uses open-source libraries: \textbf{\codeword{DeepEval}} for feedback quality, \textbf{\codeword{SciPy}} and \textbf{\codeword{NumPy}} for numeric score analysis (MAE, Pearson's r), and \textbf{\codeword{simpledorff}} for Krippendorff's Alpha.
  \item \textbf{Justification:} No single tool can evaluate this complex system. A hybrid approach is necessary.
  \item \textbf{For Feedback Quality, \codeword{DeepEval} is chosen} because it is the only open-source option that can look beyond word-matching to assess the \emph{semantic quality} of the generated text, which is paramount.
  \item \textbf{For Numeric Accuracy, standard statistical methods are chosen} because they are the universally accepted standard for comparing a model's score to a human baseline.
  \item \textbf{For Consensus Reliability, Krippendorff's Alpha is chosen} because it is the most robust academic standard for measuring inter-rater reliability, accounting for chance agreement in a way that simpler metrics (like variance) do not.
\end{itemize}

\subsubsection{2.2. Evaluation of the Code Grader}

\begin{itemize}
  \item \textbf{What We Need to Evaluate:}
\begin{enumerate}
  \item \textbf{Execution Accuracy:} The accuracy of the unit test execution within the Docker sandbox.
  \item \textbf{Feedback Quality:} The pedagogical value of the LLM-generated feedback on code style and correctness.
\end{enumerate}
\end{itemize}

\begin{itemize}
  \item \textbf{Available Options for Feedback Quality:}
\begin{enumerate}
  \item \textbf{LLM-as-a-Judge:}
\begin{itemize}
  \item \emph{Advantages:} Scalable.
  \item \emph{Disadvantages:} Not open-source, and a generic LLM lacks the specific pedagogical context to know what constitutes "good" feedback for a student learning to code.
\begin{enumerate}
  \item \textbf{Qualitative Human Review by Experts:}
\begin{itemize}
  \item \emph{Advantages:} The undisputed "gold standard" for assessing pedagogical value. Human instructors can spot nuances, assess tone, and determine if the feedback would actually help a student learn—qualities that an AI judge cannot.
  \item \emph{Disadvantages:} It is subjective, time-consuming, and does not scale.
\end{itemize}
\end{enumerate}
\end{itemize}
\end{enumerate}
\end{itemize}

\begin{itemize}
  \item \textbf{Chosen Method: Execution Accuracy \\& Qualitative Human Review}
  \item \textbf{Open-Source Confirmation:} The objective part uses open-source tools (\codeword{Docker}, \codeword{pytest}). The subjective part is a methodology, not a tool, and is by nature open and transparent.
  \item \textbf{Justification:} This module's dual nature requires a dual evaluation.
  \item \textbf{For the Objective Score:} A simple \textbf{Execution Accuracy Percentage} is perfect. It's a binary, clear, and unimpeachable metric derived from comparing the sandbox's \codeword{unittest} results to the ground truth.
  \item \textbf{For the Subjective Feedback:} \textbf{Qualitative Human Review is chosen} over an LLM-as-a-Judge because scalability is less important than authenticity. To gain user trust, the feedback must be genuinely helpful, and only human experts can be the judge of that. For this specific task, human insight is more valuable than automated metrics.
\end{itemize}

\subsubsection{2.3. Evaluation of the Math Grader}

\begin{itemize}
  \item \textbf{What We Need to Evaluate:}
\begin{enumerate}
  \item \textbf{Symbolic Correctness:} The accuracy of the symbolic math engine (e.g., SymPy) in correctly evaluating the student's mathematical expressions.
  \item \textbf{Feedback Quality:} The clarity and pedagogical value of the LLM-generated feedback explaining \emph{why} a mathematical answer is correct or incorrect.
\end{enumerate}
\end{itemize}

\begin{itemize}
  \item \textbf{Available Options:}
\begin{enumerate}
  \item \textbf{Manual Calculation:} Manually solve the problems and compare the results. This is slow and prone to human error.
  \item \textbf{Automated Symbolic Comparison:} Use a symbolic math library to programmatically compare the student's final expression against a known "golden" solution. This is fast, deterministic, and highly accurate.
  \item \textbf{LLM-as-a-Judge for Feedback:} Use a powerful LLM to evaluate the quality of the generated feedback.
\end{enumerate}
\end{itemize}

\begin{itemize}
  \item \textbf{Chosen Method: Automated Symbolic Comparison \\& DeepEval}
  \item \textbf{Open-Source Confirmation:} We will use \textbf{\codeword{SymPy}} for the symbolic comparison and \textbf{\codeword{DeepEval}} for feedback analysis, both of which are open-source.
  \item \textbf{Justification:} This hybrid approach addresses both facets of the math grader.
  \item \textbf{For Symbolic Correctness:} \textbf{\codeword{SymPy} is chosen} because it allows for a definitive, automated, and mathematically sound way to check for the equivalence of symbolic expressions. This provides an objective, unimpeachable score for correctness.
  \item \textbf{For Feedback Quality:} \textbf{\codeword{DeepEval} is chosen} to ensure the textual explanation is factually grounded in the mathematical error and is relevant to the student's mistake, preventing generic or unhelpful feedback.
\end{itemize}

\subsubsection{2.4. Evaluation of the Multimodal Grader}

\begin{itemize}
  \item \textbf{What We Need to Evaluate:}
\begin{enumerate}
  \item \textbf{Visual Interpretation Accuracy:} The model's ability to correctly interpret the content of images, charts, and diagrams.
  \item \textbf{Integrated Reasoning:} The model's capacity to combine information from both text and images to accurately assess the student's answer against the rubric.
  \item \textbf{Feedback Quality:} The quality of the feedback, ensuring it references both textual and visual elements where appropriate.
\end{enumerate}
\end{itemize}

\begin{itemize}
  \item \textbf{Available Options:}
\begin{enumerate}
  \item \textbf{Ad-hoc Manual Review:} Simply looking at the output and subjectively deciding if it's correct. This is not rigorous or reproducible.
  \item \textbf{Proprietary Multimodal Evaluation Models:} Use closed-source, powerful multimodal models as judges. This violates the open-source constraint and can be expensive.
  \item \textbf{Human Evaluation with a Structured Rubric:} Have human experts evaluate the multimodal output against a specific, structured rubric that asks questions like, "Did the model correctly identify the key elements in the diagram?" and "Does the feedback correctly reference the visual information?"
\end{enumerate}
\end{itemize}

\begin{itemize}
  \item \textbf{Chosen Method: Human Evaluation with a Structured Rubric}
  \item \textbf{Open-Source Confirmation:} This is a methodology, not a specific tool, and is by nature open and transparent.
  \item \textbf{Justification:} The automated evaluation of multimodal reasoning is a complex, cutting-edge research problem. For the practical purposes of this application, \textbf{a structured human review is the only truly reliable method}. It is the "gold standard" for assessing nuanced understanding that combines different data types. While not scalable, it provides the most accurate and actionable feedback on the performance of the multimodal grader.
\end{itemize}

\medskip\hrule\medskip

\subsection{Supporting AI System Evaluations}

\subsubsection{3.1. Evaluation of the RAG Integration Module}

\begin{itemize}
  \item \textbf{What We Need to Evaluate:} The relevance and completeness of the documents retrieved from the FAISS vector store.
\end{itemize}

\begin{itemize}
  \item \textbf{Available Options:}
\begin{enumerate}
  \item \textbf{End-to-End Evaluation:}
\begin{itemize}
  \item \emph{Advantages:} Requires no extra tools or setup.
  \item \emph{Disadvantages:} It's an indirect and unreliable signal. A good final grade could have occurred \emph{despite} bad retrieval, and vice-versa. It doesn't provide actionable insight into the RAG pipeline itself.
\begin{enumerate}
  \item \textbf{Ragas (Open-Source Framework):}
\begin{itemize}
  \item \emph{Advantages:} It is \textbf{open-source} and the purpose-built tool for this exact problem. It isolates the retrieval step and provides clear, actionable metrics like \codeword{context\_precision} and \codeword{context\_recall}.
  \item \emph{Disadvantages:} Requires the creation of a curated evaluation dataset containing queries and their expected retrieved documents.
\end{itemize}
\end{enumerate}
\end{itemize}
\end{enumerate}
\end{itemize}

\begin{itemize}
  \item \textbf{Chosen Method: Ragas Framework}
  \item \textbf{Open-Source Confirmation:} \textbf{\codeword{Ragas}} is a well-known, Apache 2.0 licensed open-source project.
  \item \textbf{Justification:} Using a specialized tool is vastly superior to indirect measurement. \textbf{Ragas is chosen} because it allows us to diagnose the health of our RAG pipeline directly. Knowing our \codeword{context\_precision} is low, for example, tells us we need to improve our chunking or embedding strategy. This level of targeted insight is impossible with an end-to-end approach.
\end{itemize}

\subsubsection{3.2. Evaluation of the Explainability Module (\codeword{explainer.py})}

\begin{itemize}
  \item \textbf{What We Need to Evaluate:} The clarity, accuracy, and pedagogical value of the generated explanation, ensuring it correctly justifies the score by explicitly referencing the rubric and the student's answer.
\end{itemize}

\begin{itemize}
  \item \textbf{Available Options:}
\begin{enumerate}
  \item \textbf{Human Review (Likert Scale):}
\begin{itemize}
  \item \emph{Advantages:} Provides a direct measure of user satisfaction and perceived clarity.
  \item \emph{Disadvantages:} Subjective, slow, and doesn't scale well.
\begin{enumerate}
  \item \textbf{DeepEval Framework:}
\begin{itemize}
  \item \emph{Advantages:} \textbf{Open-source} and allows for the creation of precise, automated, and objective metrics. We can move beyond a simple rating to get a specific score for specific qualities.
  \item \emph{Disadvantages:} Requires some initial setup for custom metrics.
\end{itemize}
\end{enumerate}
\end{itemize}
\end{enumerate}
\end{itemize}

\begin{itemize}
  \item \textbf{Chosen Method: DeepEval Framework with Custom Metrics}
  \item \textbf{Open-Source Confirmation:} \textbf{\codeword{DeepEval}} is an open-source framework.
  \item \textbf{Justification:} \textbf{DeepEval is chosen} because it allows us to create a highly specific and automated metric that perfectly matches our goal. We will create a custom \textbf{"Rubric Coverage"} metric. This metric programmatically parses the rubric and checks if the generated explanation explicitly addresses every single criterion. This provides a direct, objective score for the explanation's completeness, which is far more valuable and reliable than a subjective human rating.
\end{itemize}

\medskip\hrule\medskip

\subsection{System-Wide Process Evaluation}

\subsection{5. Backend Infrastructure Evaluation}

\subsubsection{5.1. Evaluation of Database Technology Choice (PostgreSQL)}

\begin{itemize}
  \item \textbf{What We Need to Evaluate:} The suitability of the chosen database technology (PostgreSQL) against other options, based on the application's specific requirements for data integrity, query complexity, scalability, and ecosystem support.
\end{itemize}

\begin{itemize}
  \item \textbf{Available Options:}
\begin{enumerate}
  \item \textbf{NoSQL Databases (e.g., MongoDB, Firestore):}
\begin{itemize}
  \item \emph{Advantages:} Flexible schema is good for rapidly changing or unstructured data. Generally easier to scale horizontally.
  \item \emph{Disadvantages:} Weaker transactional guarantees (compared to ACID). Performing complex queries with joins (e.g., getting all student submissions for a specific assignment with rubric details) is difficult and inefficient. The application's data is highly structured and relational, making this a poor fit.
\begin{enumerate}
  \item \textbf{SQLite:}
\begin{itemize}
  \item \emph{Advantages:} Extremely simple, serverless, zero-configuration, and stores the entire database in a single file. Excellent for development, testing, or very simple, single-user applications.
  \item \emph{Disadvantages:} Not designed for concurrency. It struggles with multiple users (e.g., several instructors) writing to the database at the same time, which is a key requirement for this web application. It lacks the advanced features and robustness of a full database server.
\begin{enumerate}
  \item \textbf{Relational Databases (e.g., PostgreSQL, MySQL):}
\begin{itemize}
  \item \emph{Advantages:} Strong ACID compliance guarantees data integrity and transactional safety, which is critical for academic records. The standardized SQL language is perfect for the complex, relational queries this application needs. Mature, stable, and well-supported technology.
  \item \emph{Disadvantages:} Requires a more rigid schema upfront. Can be more complex to set up and manage than SQLite.
\end{itemize}
\end{enumerate}
\end{itemize}
\end{enumerate}
\end{itemize}
\end{enumerate}
\end{itemize}

\begin{itemize}
  \item \textbf{Chosen Method: PostgreSQL (A Relational Database)}
  \item \textbf{Open-Source Confirmation:} PostgreSQL is a powerful, well-regarded, and fully open-source object-relational database system with a liberal license.
  \item \textbf{Justification:} \textbf{PostgreSQL was chosen because the application's data is fundamentally relational and requires high integrity.}
\begin{enumerate}
  \item \textbf{Data Integrity is Non-Negotiable:} The system manages grades, student submissions, and feedback. The strong ACID guarantees of PostgreSQL ensure that this critical data is never left in an inconsistent state. A NoSQL database would risk data corruption.
  \item \textbf{Naturally Relational Data:} The data model consists of clear relationships: a \codeword{student} has many \codeword{submissions}, an \codeword{assignment} has many \codeword{grades}. A relational database is the most efficient and logical way to model and query these relationships.
  \item \textbf{Requirement for Complex Queries:} The application needs to perform complex joins, aggregations, and reports (e.g., "Find all feedback for this student across all assignments," "Calculate the average score for question 3 on this exam"). SQL is the most powerful and appropriate tool for these tasks.
  \item \textbf{Concurrency and Scalability:} Unlike SQLite, PostgreSQL is designed from the ground up to handle concurrent connections from multiple users, which is essential for a web application used by multiple instructors. It provides a robust foundation that can scale to a large number of users and a large volume of data.
\end{enumerate}
\end{itemize}

\medskip\hrule\medskip

\subsection{6. LLM Comparative Evaluation}

\subsubsection{6.1. Evaluation of Different Language Models}

\begin{itemize}
  \item \textbf{What We Need to Evaluate:} The relative performance of different LLMs (e.g., a base model like Llama 3 or an API-based model like Gemini) for the specific tasks of grading and feedback generation. The goal is to determine which model provides the best balance of accuracy, quality, cost, and latency for our application.
\end{itemize}

\begin{itemize}
  \item \textbf{Available Options:}
\begin{enumerate}
  \item \textbf{Public Benchmarks (e.g., MMLU, HumanEval):}
\begin{itemize}
  \item \emph{Advantages:} Standardized scores are readily available for many models, providing a general sense of their capabilities.
  \item \emph{Disadvantages:} These benchmarks are generic and do not measure performance on our highly specific task of rubric-based grading. A model's ability to answer trivia questions (MMLU) is not a good proxy for its ability to provide nuanced, pedagogical feedback based on a rubric.
\begin{enumerate}
  \item \textbf{Ad-hoc Manual Testing:}
\begin{itemize}
  \item \emph{Advantages:} Quick and easy to get a "feel" for a model's output on a few examples.
  \item \emph{Disadvantages:} Not reproducible, not scalable, and highly prone to subjective bias. It cannot provide the quantitative data needed for a formal comparison or to justify a decision.
\begin{enumerate}
  \item \textbf{Systematic A/B/n Testing on a Hold-out Set:}
\begin{itemize}
  \item \emph{Advantages:} The gold standard for comparative analysis. It provides direct, head-to-head quantitative comparisons of models on the \emph{exact same data} for the \emph{exact same task}. It reuses our entire existing evaluation suite (DeepEval, statistical tests, etc.) to produce a rich, multi-faceted scorecard for each model.
  \item \emph{Disadvantages:} Requires more rigorous setup and execution time than other methods.
\end{itemize}
\end{enumerate}
\end{itemize}
\end{enumerate}
\end{itemize}
\end{enumerate}
\end{itemize}

\begin{itemize}
  \item \textbf{Chosen Method: Systematic A/B/n Testing on a Hold-out Set}
  \item \textbf{Open-Source Confirmation:} This is a methodology that leverages our entire existing suite of open-source evaluation tools (\codeword{DeepEval}, \codeword{Ragas}, \codeword{SciPy}, \codeword{pytest}). No new tools are needed.
  \item \textbf{Justification:} \textbf{This is the only method that provides actionable, evidence-based results for our specific use case.}
\begin{enumerate}
  \item \textbf{Task-Specific Results:} Instead of relying on irrelevant public benchmarks, this method tests the models on the \emph{actual grading tasks} the application performs.
  \item \textbf{Direct, Quantitative Comparison:} The process involves running the entire evaluation suite (from Section 2.1) on a fixed hold-out set, with the only variable being the LLM being called. This produces a clear scorecard comparing each model on metrics like \textbf{Mean Absolute Error}, \textbf{Feedback Faithfulness}, and \textbf{Rubric Coverage}.
  \item \textbf{Evidence-Based Decisions:} This approach allows us to make definitive statements like, "Model X reduces scoring errors by 15\% but increases latency by 40\% compared to Model Y." This is the critical information needed to make informed decisions about which model to deploy, balancing performance, cost, and user experience. It turns a subjective choice into a data-driven one.
\end{enumerate}
\end{itemize}
% --- End inline copy of evaluation_appendix.tex ---

\chapter{Educator Handbook}
\label{chap:educator_handbook}

% --- Begin inline copy of educator_guide_appendix.tex ---
\section{The Educator's Guide to the Automated Grading Framework}

\medskip\hrule\medskip

\subsection{1. Purpose \& Scope}

\textbf{Goal:} This guide serves as the primary onboarding and reference document for educators using the Automated Grading Framework. Its purpose is to empower you to efficiently and effectively grade student assignments, understand the AI's reasoning, and improve its performance over time. 

\textbf{Scope:} This document covers the complete user workflow, from uploading assignment materials and grading student submissions to reviewing analytics. It is not intended to be a deep technical reference for developers. 

\medskip\hrule\medskip

\subsection{2. Getting Started}

\subsubsection{2.1. Overview}

The Automated Grading Framework is a tool designed to assist you with the grading process. It uses a sophisticated AI Grading Engine to analyze student submissions, provide a score based on your rubric, and generate detailed, constructive feedback. 

Crucially, \textbf{you are always in control}. The framework acts as your expert assistant, and you have the final say on every grade. The system is built on a \textbf{Human-in-the-Loop} philosophy, meaning your corrections are not only saved but are used to make the AI smarter and more aligned with your standards over time. 

\subsubsection{2.2. Prerequisites}

Before you begin, please ensure you have the following: 

\begin{itemize}
  \item \textbf{Professor-level access credentials} to log into the application.
  \item \textbf{Assignment materials in PDF format.} This includes:
  \item A document containing the assignment questions, the ideal answers, and a detailed grading rubric.
  \item The students' submissions, also in PDF format.
\end{itemize}

\medskip\hrule\medskip

\subsection{3. Step-by-Step Walkthrough}

This section will guide you through the three main phases of using the application. 

\subsubsection{Phase 1: Uploading Your Assignment}

First, you need to provide the system with the context for the assignment. 

\begin{enumerate}
  \item \textbf{Navigate} to the \textbf{"Upload Data"} page from the main menu.
  \item \textbf{Upload the Professor's Document:} Under the "Upload Professor Data" section, upload the PDF that contains your questions, ideal answers, and rubric.
  \item \textbf{Upload Student Submissions:} Under the "Upload Student Data" section, upload one or more student answer PDFs.
  \item \textbf{Verification:} The system will confirm once the files are successfully parsed and stored.
\end{enumerate}

> \#\#\#\# �� \textbf{Best Practice: PDF Formatting} 
> For the best results, use PDFs where the text is machine-readable (i.e., not a scanned image). This allows the AI to parse the content with the highest accuracy. If you have a very long assignment document, consider splitting it into smaller files for easier processing. 

\subsubsection{Phase 2: Grading and Review (The Human-in-the-Loop)}

This is where the magic happens. The AI will grade the submissions, and you will review them. 

\begin{enumerate}
  \item \textbf{Navigate} to the \textbf{"Grading Result"} page.
  \item \textbf{Initiate Grading:} Select the course and assignment you wish to grade and click the \textbf{"Start Grading"} button.
  \item \textbf{Review the Results:} After a few moments, the results will appear in an interactive table. For each student, you will see:
\begin{itemize}
  \item The question and their answer.
  \item The AI-generated score (\codeword{old\_score}) and feedback (\codeword{old\_feedback}).
\begin{enumerate}
  \item \textbf{Make Corrections:} If you disagree with the AI, simply \textbf{click into the table cell} and edit the score or feedback directly. The table works just like a spreadsheet.
  \item \textbf{Save Corrections:} When you modify a grade, your correction is saved automatically as \codeword{new\_score} and \codeword{new\_feedback}.
\end{enumerate}
\end{itemize}
\end{enumerate}

> \#\#\#\# ✨ \textbf{How Your Corrections Help} 
> Every correction you make is used in two powerful ways: 
> 1.  \textbf{For Consistency (RAG):} Your correction is immediately stored in the system's "memory" (a Vector Store). When the AI grades the \emph{next} student, it looks at this memory to see how you graded similar answers, helping it stay consistent. 
> 2.  \textbf{For Long-Term Improvement:} Your corrections become the training data for making the AI model itself better (see Phase 3). 

\medskip\hrule\medskip

\subsection{4. Troubleshooting \& FAQ}

\begin{itemize}
  \item \textbf{Q: The PDF upload failed or the text looks jumbled. Why?}
  \item \textbf{A:} This usually happens if the PDF is a scanned image of a document. Please ensure your PDFs are created from a text source (e.g., "Save as PDF" from a word processor). See the Best Practice tip in Phase 1.
\end{itemize}

\begin{itemize}
  \item \textbf{Q: The AI's grade seems completely wrong. What should I do?}
  \item \textbf{A:} Simply correct it in the results table. The system is designed for this! Your correction provides a valuable data point that helps the AI learn.
\end{itemize}

\begin{itemize}
  \item \textbf{Q: How can I trust the AI is being fair and consistent?}
  \item \textbf{A:} The system uses two key features for this: the \textbf{Multi-Agent System}, where multiple AI agents debate to reach a consensus, and \textbf{Retrieval Augmented Generation (RAG)}, which constantly refers to your past corrections to maintain consistency. The final authority, however, is always you.
\end{itemize}

\begin{itemize}
\end{itemize}

\medskip\hrule\medskip

\subsection{5. Additional Resources}

For users interested in the underlying technical details of the framework, the following documents are available in the project repository: 

\begin{itemize}
  \item \codeword{ARCHITECTURE.md}: A high-level overview of the system components.
  \item \codeword{DESIGN.md}: A detailed technical design of the software.
  \item \codeword{DATA\_FLOW.md}: A set of diagrams illustrating how data moves through the application.
\end{itemize}

\medskip\hrule\medskip

\subsection{Next Steps for This Document}

As a living document, this guide will evolve with the product. Based on anticipated user feedback, future versions should include: 

\begin{itemize}
  \item \textbf{A section on interpreting the Analytics Dashboard.}
  \item \textbf{Specific guidance for grading Code Assignments}, including how to write effective \codeword{unittest} cases.
  \item \textbf{A more detailed "Tips and Tricks" section} for writing effective rubrics that the AI can easily understand.
\end{itemize}
% --- End inline copy of educator_guide_appendix.tex ---


\chapter{Implementation Details}
\label{app:impl_details}

\section{Technology Stack Breakdown}
\label{app:tech_stack}

\begin{table}[h]
  \centering
  \caption{Core technology stack for the automated grading framework.}
  \label{tab:tech_stack}
  \begin{tabularx}{\textwidth}{l l Y}
    \toprule
    Component & Technology & Rationale \\
    \midrule
    Frontend/Backend & Streamlit & Rapid development of data-centric workflows in Python within a single codebase. \\
    Database & PostgreSQL & ACID-compliant relational store suited to structured course, submission, and grading data. \\
    AI Orchestration & LangChain & Abstractions for agentic workflows and Retrieval-Augmented Generation with pluggable models. \\
    Local AI/ML Runtime & MLX (Apple Silicon) & Efficient on-device inference for open-source LLMs. \\
    PDF Processing & PyMuPDF (\texttt{fitz}) & High-accuracy text extraction and metadata parsing from heterogeneous PDFs. \\
    Vector Store & FAISS & Low-latency similarity search underpinning the RAG memory. \\
    Code Sandboxing & Docker & Ephemeral, isolated execution environment for untrusted student code and unit tests. \\
    \bottomrule
  \end{tabularx}
\end{table}

Streamlit's tight coupling with Python keeps the frontend and backend in the same language, simplifying shared utilities such as authentication helpers. PostgreSQL satisfies the need for strict referential integrity between professors, students, assignments, and grades, while FAISS and LangChain together furnish the retrieval-augmented context that lifts consistency. Finally, MLX and Docker anchor the AI workloads: MLX ensures Apple Silicon laptops can run models locally, and Docker provides the isolation boundary necessary for executing untrusted student code.

\section{Data and Persistence Schema}
\label{app:data_schema}

PostgreSQL serves as the single source of truth for application data. The schema excerpt in Table~\ref{tab:grading_results_schema} highlights the columns used to manage human-in-the-loop corrections.

\begin{table}[h]
  \centering
  \caption{Selected fields from the \texttt{grading\_results} table.}
  \label{tab:grading_results_schema}
  \begin{tabularx}{\textwidth}{l l Y}
    \toprule
    Column & Data Type & Description \\
    \midrule
    \texttt{id} & \texttt{SERIAL} & Primary key identifying each grading outcome. \\
    \texttt{course} & \texttt{VARCHAR(255)} & Course identifier associated with the submission. \\
    \texttt{assignment\_no} & \texttt{VARCHAR(255)} & Assignment number or label. \\
    \texttt{question} & \texttt{TEXT} & Prompt being evaluated. \\
    \texttt{student\_answer} & \texttt{TEXT} & Raw student submission text extracted from PDF. \\
    \texttt{old\_score} & \texttt{INTEGER} & Initial AI-generated score prior to human review. \\
    \texttt{old\_feedback} & \texttt{TEXT} & Initial AI-generated formative feedback. \\
    \texttt{new\_score} & \texttt{INTEGER} & Optional human-corrected score. \\
    \texttt{new\_feedback} & \texttt{TEXT} & Optional human-authored feedback revision. \\
    \texttt{graded\_at} & \texttt{TIMESTAMP WITH TIME ZONE} & Timestamp capturing when the automated grading occurred. \\
    \texttt{corrected\_by} & \texttt{INTEGER} & Foreign key referencing the educator who supplied corrections. \\
    \bottomrule
  \end{tabularx}
\end{table}

The presence of both \texttt{old\_} and \texttt{new\_} fields makes it possible to track AI output alongside human corrections, while \texttt{graded\_at} and \texttt{corrected\_by} provide the audit metadata required for compliance reviews and for prioritising which examples need additional scrutiny. In practice, analysts query \codeword{grading_results} to answer questions such as: Which assignments trigger the most overrides? How often do lecturers adjust scores versus feedback? Pairing the timestamp columns with \codeword{corrected_by} enables longitudinal studies (e.g., do corrections decline after rubric guidance is updated?) and ensures that every change is attributable to a specific educator.

\section{Frontend Implementation Details}
\label{sec:frontend_details}

The Streamlit application follows a multi-page layout that mirrors the educator journey documented in the repository. The root \codeword{app.py} file bootstraps global configuration, handles authentication redirects, and exposes navigation links. Each page under \codeword{pages/} encapsulates a specific workflow: \codeword{0_auth.py} manages login and session initialisation, \codeword{1_upload_data.py} orchestrates the dual professor/student ingestion process, \codeword{2_grading_result.py} exposes the editable grading table, and \codeword{3_dashboard.py} surfaces analytics for cohort monitoring. Streamlit's \codeword{st.session_state} preserves identity, current course context, and cached query results, ensuring that expensive database reads are not repeated as a user moves between pages. This layout, lifted from \codeword{ARCHITECTURE.md} and \codeword{Educator_Guide.md}, keeps the educator experience linear while still allowing fast iteration on individual screens.

\section{Backend Services and Data Management}
\label{sec:backend_services}

Backend logic is centralised in reusable service modules. The \codeword{PostgresHandler} abstraction, described in \codeword{DESIGN.md}, exposes parametrised \codeword{SELECT}, \codeword{INSERT}, and \codeword{UPDATE} methods with automatic connection lifecycle management, shielding the Streamlit callbacks from SQL boilerplate. Core tables---\codeword{users}, \codeword{prof_data}, \codeword{student_data}, and \codeword{grading_results}---mirror the schema captured in the design document and support features such as audit trails, collaboration, and analytics exports. The grading engine is accessed through the \codeword{grader_engine.router} entrypoint, which dispatches to modality-specific graders (text, code, multi-agent, multimodal) defined in \codeword{ARCHITECTURE.md}. This modular routing makes it trivial to plug in additional graders (e.g., for multimodal submissions) without altering the invocation surface exposed to the UI.

\section{Model Management and Future Adaptation}
\label{sec:model_management}

Model selection is handled through configuration files that point the grading engine at vetted open-source checkpoints (for example, Mistral via Ollama) or approved API endpoints. Human corrections are versioned in PostgreSQL and mirrored into the retrieval store, ensuring that prompt templates and consensus logic can be audited or rolled back. Automated model-adaptation workflows and broader MLOps orchestration were explicitly descoped for this project; instead, the system focuses on deterministic prompts, retrieval grounding, and reproducible data exports that would allow a future team to introduce training pipelines without disrupting the current deployment.

\chapter{Extended Evaluation Artefacts}
\label{app:eval_details}

\section{Evaluation Environment}
\label{app:evaluation_environment}

\begin{table}[h]
  \centering
  \caption{Test environment and tooling.}
  \label{tab:test_environment}
  \begin{tabularx}{\textwidth}{l Y}
    \toprule
    Component & Configuration \\
    \midrule
    Hardware & MacBook Pro (M1, 16\,GB RAM) with simulated multi-user load \\
    Operating system & macOS 13 (Ventura) \\
    Runtime & Python 3.10 (Streamlit) + Ollama 0.5.0 \\
    Database & PostgreSQL 14 with tables \codeword{grading_results}, \codeword{grading_corrections}, \codeword{result_shares} \\
    Base LLM & \codeword{mistral} 7B (local), with comparative runs on LLaMA 3 8B and Falcon 7B \\
    Embeddings & \codeword{all-MiniLM-L6-v2} (sentence-transformers, CPU) \\
    Datasets & Anonymised lecturer rubrics and student PDFs, plus synthetic benchmarks for gap analysis \\
    \bottomrule
  \end{tabularx}
\end{table}

\section{Module Test Matrix}
\label{app:module_tests}

\begin{itemize}
  \item \textbf{ETL}: Lecturer and student PDFs were parsed with a 96\% success rate; validation safeguards block grading when required answers are missing. A golden dataset harness is in progress to provide formal precision/recall benchmarks.
  \item \textbf{Grading engine}: Text grading aligned with lecturer scores within $\pm 1$ rubric point on 91\% of synthetic cases; code grading achieved 88\% agreement when instructor unit tests were available and limited credit to 40\% when only smoke tests were possible.
  \item \textbf{Prompt suites}: MCQ, code evaluator, and numeric prompt templates posted pass rates between 90--95\%, with corrective actions logged for tolerance and rationale-tagging issues.
  \item \textbf{Human-in-the-loop}: Manual overrides persisted reliably, collaboration shares exposed corrected records, and terminology localisation (``lecturer'' vs ``professor'') did not affect parsing.
  \item \textbf{Analytics}: Dashboard filters, CSV/PDF exports, and KPI counts matched the underlying SQL queries, though automated regression checks remain a future improvement.
\end{itemize}

\section{Comparative Model Performance}
\label{app:model_comparison}

\begin{table}[h]
  \centering
  \caption{LLM comparison summary (synthetic benchmark).}
  \label{tab:model_comparison}
  \begin{tabularx}{\textwidth}{lccc}
    \toprule
    Model & Text agreement & Code agreement & Avg latency / cost note \\
    \midrule
    Mistral (7B, Ollama) & 91\% & 88\% & 35\,s per response; $\sim\$0.60$/1k tokens equivalent \\
    LLaMA 3 8B (GGUF) & \textbf{94\%} & \textbf{89\%} & 42\,s per response; higher VRAM footprint \\
    Falcon 7B Instruct & 88\% & 82\% & 51\,s per response; struggled with rubric alignment \\
    \bottomrule
  \end{tabularx}
\end{table}

\section{Operational Analytics Insights}
\label{app:operational_analytics}

\begin{itemize}
  \item \textbf{Human vs AI gap}: Across MCQ, code, numeric, and essay tasks, the AI trailed lecturer scores by 2--3 points on average—within the acceptable tolerance band but still highlighting the need for human oversight.
  \item \textbf{DeepEval metrics}: Faithfulness scores exceeded the 0.85 threshold for text and code tasks, though multimodal prompts require better OCR normalisation to reach parity.
  \item \textbf{Turnaround time}: Analytics dashboards showed reduced median grading time per assignment once RAG exemplars accumulated, reinforcing the value of prompt retrieval and structured explainability.
  \item \textbf{Feature usage}: Export and collaboration features saw consistent adoption, confirming that lecturers rely on audit trails and shared reviews during moderation.
\end{itemize}

\chapter{Educator Workflow Reference}
\label{app:educator_workflow}

\section{Phase-by-Phase Guide}

\textbf{Phase 1: Uploading Assignment Context}
\begin{enumerate}
  \item Navigate to the ``Upload Data'' page.
  \item Upload the combined PDF containing questions, ideal answers, and rubric definitions.
  \item Upload one or more student submission PDFs (individual files or ILIAS archives).
  \item Wait for the confirmation banner signalling successful parsing and persistence.
\end{enumerate}

\textbf{Phase 2: Grading and Review}
\begin{enumerate}
  \item Visit ``Grading Result'', choose the course/assignment, and click \textit{Start Grading}.
  \item Inspect the generated \codeword{old_score}/\codeword{old_feedback} columns for each student.
  \item Edit any score or feedback inline; the system automatically records the correction as \codeword{new_score}/\codeword{new_feedback}.
  \item Use the variance column to prioritise reviews of low-confidence grades.
\end{enumerate}


\section{Best Practices and Troubleshooting}

\begin{itemize}
  \item Prefer machine-readable PDFs; scanned images reduce extraction accuracy.
  \item Keep rubric criteria explicit and atomic to ensure agents can reference them directly.
\item Use the collaboration export when moderating with multiple instructors; each correction is tracked with timestamps and editor IDs.
  \item If grading appears stalled, check the background task log (located in \codeword{logs/}) for model timeouts or Docker build errors.
  \item When collaborating with other instructors, align on rubric terminology to keep the retrieval store consistent.
\end{itemize}

\chapter{Evaluation Asset Listings}
\label{app:evaluation_assets}

This appendix reproduces the exact Python sources that underpin the automated evaluation described in Chapter~\ref{chap:results}. Each listing is pulled directly from the repository so that auditors can inspect the executable artefacts without leaving the thesis PDF.

\section{Unit Test Suite}

\subsection{\texttt{tests/test\_pdf\_ingestion.py}}
\lstinputlisting[language=Python]{tests/test_pdf_ingestion.py}

\subsection{\texttt{tests/test\_grading\_validation.py}}
\lstinputlisting[language=Python]{tests/test_grading_validation.py}

\subsection{\texttt{tests/test\_code\_grader\_module.py}}
\lstinputlisting[language=Python]{tests/test_code_grader_module.py}

\subsection{\texttt{tests/test\_math\_grader\_module.py}}
\lstinputlisting[language=Python]{tests/test_math_grader_module.py}

\subsection{\texttt{tests/test\_multimodal\_rag\_store.py}}
\lstinputlisting[language=Python]{tests/test_multimodal_rag_store.py}

\section{Figure Generation Script}

\subsection{\texttt{evaluation\_reports/generate\_figures.py}}
\lstinputlisting[language=Python]{evaluation_reports/generate_figures.py}

\subsection{\texttt{evaluation\_reports/export\_datasets.py}}
\lstinputlisting[language=Python]{evaluation_reports/export_datasets.py}

\subsection{\texttt{evaluation\_reports/generate\_raw\_data.py}}
\lstinputlisting[language=Python]{evaluation_reports/generate_raw_data.py}

\subsection{\texttt{evaluation\_reports/run\_quality\_checks.py}}
\lstinputlisting[language=Python]{evaluation_reports/run_quality_checks.py}

\section{Data Extracts}

The figures and metrics above are driven by sanitized CSV exports produced by the evaluation notebooks. Each file resides in \codeword{evaluation\_reports/data/} and mirrors the aggregates cited in Chapter~\ref{chap:results}. The upstream notebook dumps (with pseudonymous IDs) are stored in \codeword{evaluation\_reports/raw/}; running \codeword{evaluation\_reports/generate\_raw\_data.py} recreates them, \codeword{evaluation\_reports/export\_datasets.py} removes identifiers, and \codeword{evaluation\_reports/generate\_figures.py} redraws the figures. Reviewers can also open \codeword{evaluation\_reports/notebooks/evaluation\_pipeline.ipynb} to execute the same commands in an interactive environment.

\chapter*{Acknowledgments}
\label{chap:acknowledgments}

I thank Dr.\ Ruben Nuredini for steadfast supervision and insisting on rigorous evaluation, Dr.\ Martin Haag for guidance on system design, and the ``AI Fundamentals'' faculty for sharing anonymized grading data. My colleagues in the HEI Lab, especially the MLX working group, provided invaluable critiques and infrastructure support. Finally, I am grateful to family and friends for their patience and encouragement throughout this research.

\begin{thebibliography}{99}
\raggedright

\bibitem{Th11}
Manuel Ren\'e Theisen.
\newblock \emph{Wissenschaftliches Arbeiten: Technik -- Methodik -- Form}.
\newblock Vahlen, 15th edition, 2011.

\bibitem{hhneb}
Hochschule Heilbronn.
\newblock \url{https://www.hs-heilbronn.de/}. Accessed 14 Aug 2010.

\bibitem{krippendorff2018}
Klaus Krippendorff.
\newblock \emph{Content Analysis: An Introduction to Its Methodology}.
\newblock Sage, 4th edition, 2018.

\bibitem{deepeval2024}
Enrui Zhang et~al.
\newblock DeepEval: Benchmarking LLM-generated feedback.
\newblock In \emph{Proceedings of EDM}, 2024.

\bibitem{mistralcard}
Mistral AI.
\newblock Mistral 7B model card, 2023.
\newblock \url{https://docs.mistral.ai/model-cards}.

\bibitem{johnson2019}
Robert Johnson and Sarah Berndt.
\newblock Secure container sandboxing for educational autograders.
\newblock In \emph{Proceedings of SIGCSE}, 2019.

\bibitem{policar2025}
T.~Poli\v{c}ar, et~al.
\newblock Automated assignment grading with large language models.
\newblock \emph{Journal of Educational Data Science}, 2025.

\bibitem{tornqvist2023}
M.~Törnqvist, et~al.
\newblock ExASAG: Explainable short answer grading.
\newblock In \emph{Proceedings of the Empirical Methods in Natural Language Processing}, 2023.

\bibitem{chu2025}
Y.~Chu, et~al. 
\newblock GradeRAG: Retrieval-augmented short answer grading. 
\newblock In \emph{Proceedings of the International Conference on Learning Analytics}, 2025.

\bibitem{mlx2024}
Apple.
\newblock MLX: Machine learning on Apple Silicon, 2024.
\newblock \url{https://github.com/ml-explore/mlx}.

\end{thebibliography}

\end{document}
