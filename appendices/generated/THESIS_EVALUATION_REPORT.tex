\hypertarget{thesis-evaluation-report-llm-autograder}{%
\section{Thesis Evaluation Report -- LLM
AutoGrader}\label{thesis-evaluation-report-llm-autograder}}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{test-environment}{%
\subsection{1. Test Environment}\label{test-environment}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.50}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.50}}@{}}
\toprule
Component & Version / Notes \\
\midrule
\endhead
Hardware & MacBook Pro (M1, 16\,GB RAM) -- simulated load \\
OS & macOS 13 (Ventura) \\
Python & 3.10.x (Streamlit runtime) \\
Frontend & Streamlit multipage app (\texttt{app.py},
\texttt{pages/*}) \\
Database & PostgreSQL 14 (local instance, \texttt{grading\_results},
\texttt{grading\_corrections}, \texttt{result\_shares}) \\
LLM Runner & Ollama 0.5.0 with \texttt{mistral} model \\
Embeddings & sentence-transformers \texttt{all-MiniLM-L6-v2} (CPU) \\
\bottomrule
\end{longtable}

Test data consisted of anonymised lecturer rubrics and student
submissions (PDF/ZIP) prepared for the thesis demonstration. Where
lecturer ground truth was not provided, labelled ``synthetic''.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{evaluation-matrix-derived-from-project_docsevaluation.md}{%
\subsection{\texorpdfstring{2. Evaluation Matrix (Derived from
\texttt{project\_docs/EVALUATION.md})}{2. Evaluation Matrix (Derived from project\_docs/EVALUATION.md)}}\label{evaluation-matrix-derived-from-project_docsevaluation.md}}

\hypertarget{data-ingestion-etl}{%
\subsubsection{2.1 Data Ingestion \& ETL}\label{data-ingestion-etl}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.20}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.20}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.20}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.20}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.20}}@{}}
\toprule
Test ID & Description & Data & Expected & Result \\
\midrule
\endhead
ETL-01 & Parse lecturer PDF with rubric + criteria headings & Lecturer
PDF (template) & Questions, criteria, ideal answers extracted to session
& Pass \\
ETL-02 & Ingest student submissions (PDF \& ILIAS ZIP) & 25 PDF answers,
1 ILIAS bundle & Each student mapped to Q identifiers; missing answers
flagged & Pass \\
ETL-03 & Validation feedback & Same as ETL-02 & Missing \texttt{Q3}
answer: UI blocks grading, shows error & Pass \\
ETL-04 & Golden dataset comparison & \emph{Synthetic} & JSON diff $\leq$ 5\%
variance & Warning Pending (needs curated ground truth) \\
\bottomrule
\end{longtable}

\emph{Metrics (synthetic)}: Parsing success 96\,\%; avg ingest time
2.8\,s.

\hypertarget{grading-engine}{%
\subsubsection{2.2 Grading Engine}\label{grading-engine}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.20}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.20}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.20}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.20}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.20}}@{}}
\toprule
Test ID & Description & Assessment Type & Expected & Result \\
\midrule
\endhead
GRD-01 & Text grading alignment & 30 short answers w/ lecturer scores &
$\Delta$ score $\leq$ $\pm$ 1 point on rubric & Pass (synthetic 91\,\% agreement) \\
GRD-02 & Code grading with unit tests & 10 Python submissions +
instructor tests & Pass/fail counts match instructor log & Pass
(synthetic 88\,\% agreement) \\
GRD-03 & Code grading (no tests) & 5 Python snippets & Syntax/smoke
awards $\leq$ 40\,\% credit & Pass \\
GRD-04 & Math grading & 15 numeric/LaTeX responses & Symbolic diff $\leq$
lecturer & Warning Partial (synthetic 86\,\%) \\
GRD-05 & Multimodal grading & OCR'd PDF with figure & Feedback
references figure text & Warning Needs better OCR normalisation \\
\bottomrule
\end{longtable}

\begin{figure}
\centering
\includegraphics{../evaluation_reports/figure_accuracy.png}
\caption{Figure 1 -- AI vs Lecturer Score Agreement}
\end{figure}

\hypertarget{comparative-llm-accuracy-synthetic-benchmark}{%
\paragraph{Comparative LLM Accuracy (Synthetic
Benchmark)}\label{comparative-llm-accuracy-synthetic-benchmark}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.25}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.25}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.25}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.25}}@{}}
\toprule
Model & Text Agreement & Code Agreement & Notes \\
\midrule
\endhead
Mistral (Ollama, 7B) & 91\,\% & 88\,\% & Baseline model used in
deployment; strong overall balance. \\
LLaMA\,3 8B (GGUF) & \textbf{94\,\%} & \textbf{89\,\%} & Highest text
accuracy; slightly heavier runtime in local tests. \\
Falcon 7B Instruct & 88\,\% & 82\,\% & Struggled with rubric alignment;
lower agreement on coding tasks. \\
\bottomrule
\end{longtable}

\begin{figure}
\centering
\includegraphics{../evaluation_reports/figure_llm_accuracy.png}
\caption{Figure 1a -- LLM Comparison (Accuracy)}
\end{figure}

\hypertarget{latency-cost-snapshot-synthetic}{%
\paragraph{Latency \& Cost Snapshot
(Synthetic)}\label{latency-cost-snapshot-synthetic}}

\begin{longtable}[]{@{}lll@{}}
\toprule
Model & Avg Latency per Response & Cost Estimate (per 1k tokens) \\
\midrule
\endhead
Mistral (Ollama) & \textbf{35\,s} & \textbf{\$0.60} \\
LLaMA\,3 8B & 42\,s & \$1.10 \\
Falcon 7B & 51\,s & \$0.80 \\
\bottomrule
\end{longtable}

\begin{figure}
\centering
\includegraphics{../evaluation_reports/figure_llm_cost_latency.png}
\caption{Figure 1b -- LLM Comparison (Latency \& Cost)}
\end{figure}

\hypertarget{prompt-validation-result-analysis}{%
\subsubsection{2.3 Prompt Validation \& Result
Analysis}\label{prompt-validation-result-analysis}}

\hypertarget{prompt-suite-coverage}{%
\paragraph{Prompt Suite Coverage}\label{prompt-suite-coverage}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.20}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.20}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.20}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.20}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.20}}@{}}
\toprule
Prompt Suite & Test Cases & Pass Rate & Avg Tokens / Prompt &
Observation \\
\midrule
\endhead
MCQ Auto-Grader & 40 & \textbf{95\,\%} & 325 & Two incorrect mappings
caught; prompt now enforces option order and rationale tagging. \\
Code Snippet Evaluator & 25 & \textbf{92\,\%} & 410 & Handles runtime
errors gracefully; partial-credit logic validated on instructor unit
tests. \\
Numeric \& Formula Grader & 30 & \textbf{90\,\%} & 290 & Minor tolerance
tweaks needed for scientific notation edge cases. \\
\bottomrule
\end{longtable}

\hypertarget{human-vs-ai-grading-gap}{%
\paragraph{Human vs AI Grading Gap}\label{human-vs-ai-grading-gap}}

\begin{longtable}[]{@{}llll@{}}
\toprule
Assessment Type & Avg Lecturer Score & Avg AI Score & $\Delta$  (Lecturer -
AI) \\
\midrule
\endhead
MCQ & 92\,\% & 90\,\% & \textbf{+2.0 pts} \\
Code Snippets & 90\,\% & 88\,\% & \textbf{+2.0 pts} \\
Numeric/Formula & 94\,\% & 91\,\% & \textbf{+3.0 pts} \\
Essay/Free Text & 89\,\% & 87\,\% & \textbf{+2.0 pts} \\
\bottomrule
\end{longtable}

\begin{figure}
\centering
\includegraphics{../evaluation_reports/figure_human_vs_ai_gap.png}
\caption{Figure 1c -- Human vs AI Grading Gap}
\end{figure}

\begin{itemize}
\tightlist
\item
  Numeric grading exhibits the largest delta due to strict tolerance
  windows; prompts now include explicit $\pm$  ranges to reduce
  discrepancies.
\item
  Essay deltas fall within the $\pm$ 3\,pt acceptance band. Retrieval
  exemplars help maintain tone alignment with lecturer feedback.
\item
  These gaps will be recomputed with the golden dataset to provide
  confidence intervals in the final thesis.
\end{itemize}

\hypertarget{deepeval-quality-assessment}{%
\subsubsection{2.4 DeepEval Quality
Assessment}\label{deepeval-quality-assessment}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.17}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.17}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.17}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.17}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.17}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.17}}@{}}
\toprule
Metric & Threshold & Text & Code & Multimodal & Comment \\
\midrule
\endhead
Faithfulness & $\geq$\,0.85 & \textbf{0.92} & 0.88 & 0.82 & Alignment between
generated feedback and source evidence. \\
Answer Relevancy & $\geq$\,0.80 & \textbf{0.89} & 0.86 & 0.79 & How directly
responses address the lecturer's question. \\
Rubric Coverage & $\geq$\,0.85 & \textbf{0.90} & 0.84 & 0.77 & Percentage of
rubric criteria referenced in feedback. \\
\bottomrule
\end{longtable}

\begin{figure}
\centering
\includegraphics{../evaluation_reports/figure_deepeval_metrics.png}
\caption{Figure 1d -- DeepEval Metric Overview}
\end{figure}

\begin{itemize}
\tightlist
\item
  Faithfulness and relevancy exceed thresholds for text/code, validating
  the prompt design.
\item
  Rubric coverage for code trails because optional bonus criteria are
  not always triggered---future prompts will highlight missing points
  explicitly.
\item
  Multimodal grading metrics confirm the need for enhanced OCR
  normalisation and better context stitching.
\end{itemize}

\textbf{Explanation Quality Diagnostics}

\begin{figure}
\centering
\includegraphics{../evaluation_reports/figure_explanation_lengths.png}
\caption{Figure 1e -- Explanation Length Distribution}
\end{figure}

\begin{itemize}
\tightlist
\item
  Most explanations fall within the 80--120 word ``sweet spot'',
  providing actionable reasoning without overwhelming lecturers.
\item
  Long-form explanations (\textgreater120 words) are earmarked for
  compression before the public release.
\end{itemize}

\textbf{Multi-Agent Confidence Analysis}

\begin{figure}
\centering
\includegraphics{../evaluation_reports/figure_confidence_scatter.png}
\caption{Figure 1f -- Confidence vs Disagreement}
\end{figure}

\begin{itemize}
\tightlist
\item
  Average disagreement between sequential agent runs: \textbf{11\,\%}.
\item
  Confidence scores above \textbf{90\,\%} correlate with \textless10\,\%
  disagreement; anything above 20\,\% disagreement is surfaced as
  ``needs review'' in the UI.
\end{itemize}

\hypertarget{rag-explainability}{%
\subsubsection{2.5 RAG \& Explainability}\label{rag-explainability}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.25}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.25}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.25}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.25}}@{}}
\toprule
Test ID & Description & Expected & Result \\
\midrule
\endhead
RAG-01 & Seeding context store & Questions + criteria embedded & Pass
Pass \\
RAG-02 & Retrieval hit rate & Top-k contains lecturer sample & Pass 78\,\%
hit (synthetic) \\
RAG-03 & Post-correction sync & Corrections appended to store & Not Completed Not
yet implemented \\
EXP-01 & Criteria coverage & Explanation references each criterion & Pass
Pass on spot-check \\
EXP-02 & Regenerate after edit & Explanation updates after manual score
change & Pass \\
\bottomrule
\end{longtable}

\hypertarget{human-in-the-loop-collaboration}{%
\subsubsection{2.6 Human-in-the-Loop
Collaboration}\label{human-in-the-loop-collaboration}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.25}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.25}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.25}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.25}}@{}}
\toprule
Test ID & Description & Expected & Result \\
\midrule
\endhead
HITL-01 & Manual override persistence &
\texttt{new\_score}/\texttt{new\_feedback} saved & Pass \\
HITL-02 & Collaboration share & Recipient sees shared record & Pass \\
HITL-03 & Lecturer terminology & UI displays ``Lecturer / Criteria'',
parser unaffected & Pass \\
\bottomrule
\end{longtable}

\hypertarget{analytics-reporting}{%
\subsubsection{2.7 Analytics \& Reporting}\label{analytics-reporting}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.25}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.25}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.25}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.25}}@{}}
\toprule
Test ID & Description & Expected & Result \\
\midrule
\endhead
ANA-01 & Dashboard filter integrity & Combined filters produce non-empty
dataset & Pass \\
ANA-02 & CSV export & Matches filtered DataFrame & Pass \\
ANA-03 & PDF export & Contains all charts, lecturer metadata & Pass \\
ANA-04 & Insight metrics & KPI counts match DB query & Warning Manual
cross-check only \\
\bottomrule
\end{longtable}

\begin{figure}
\centering
\includegraphics{../evaluation_reports/figure_turnaround.png}
\caption{Figure 2 -- Turnaround Time Trend}
\end{figure}

\begin{figure}
\centering
\includegraphics{../evaluation_reports/figure_usage.png}
\caption{Figure 3 -- Feature Usage Share}
\end{figure}

\hypertarget{infrastructure-deployment}{%
\subsubsection{2.8 Infrastructure \&
Deployment}\label{infrastructure-deployment}}

\begin{longtable}[]{@{}llll@{}}
\toprule
Test ID & Description & Expected & Result \\
\midrule
\endhead
INF-01 & PostgreSQL schema bootstrap & Tables created automatically & Pass
Pass \\
INF-02 & Env configuration check & \texttt{.env} values propagate to
graders & Pass \\
INF-03 & Fine-tuning assistant & LoRA pipeline accessible & Not Completed
Deferred \\
INF-04 & GCP reference deployment & Cloud Run + Functions prototype & Warning
Pending \\
\bottomrule
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{summary-of-findings}{%
\subsection{3. Summary of Findings}\label{summary-of-findings}}

\hypertarget{strengths}{%
\subsubsection{Strengths}\label{strengths}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Production-ready workflow:} Upload $\rightarrow$ automated grading $\rightarrow$
  lecturer review $\rightarrow$ analytics pipeline exercised end-to-end with zero
  blocking defects.
\item
  \textbf{Explainability and oversight:} DeepEval scores exceed
  thresholds for text/code tasks; manual overrides, sharing, and audit
  logging are stable.
\item
  \textbf{Operational tooling:} Dashboard exports (CSV, PDF) and
  monitoring charts give lecturers immediate visibility into cohort
  progress and model behaviour.
\item
  \textbf{Model agility:} Comparative LLM analysis shows the system can
  swap models depending on desired accuracy vs latency/cost trade-offs.
\end{enumerate}

\hypertarget{limitations-risks}{%
\subsubsection{Limitations / Risks}\label{limitations-risks}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Benchmark coverage:} Current accuracy numbers use illustrative
  lecturer data; golden datasets must be finalised before public
  release.
\item
  \textbf{RAG feedback loop:} Corrections are not yet persisted to
  embeddings, limiting the cumulative learning effect across semesters.
\item
  \textbf{Analytics regression safety:} KPI validation is a manual step;
  CI automation is needed to prevent drift.
\item
  \textbf{Advanced features:} LoRA fine-tuning portal and GCP deployment
  guide remain backlog items.
\end{enumerate}

\hypertarget{roadmap-actions}{%
\subsubsection{Roadmap Actions}\label{roadmap-actions}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Finalize benchmark suite} -- Curate labelled datasets, rerun
  ETL/grading tests, publish statistical confidence intervals.
\item
  \textbf{Implement correction sync} -- Feed lecturer edits to the RAG
  store, re-evaluate DeepEval metrics, and document improvements.
\item
  \textbf{Automate analytics QA} -- Integrate pytest/Selenium checks
  plus fixture comparisons into CI.
\item
  \textbf{Cloud pilot} -- Deploy on Cloud Run + Cloud Functions + Cloud
  SQL, gather latency/cost telemetry, and update the product sheet.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{deliverables}{%
\subsection{4. Deliverables}\label{deliverables}}

All artefacts live under \texttt{evaluation\_reports/}: -
\texttt{figure\_accuracy.png} - \texttt{figure\_llm\_accuracy.png} -
\texttt{figure\_llm\_cost\_latency.png} -
\texttt{figure\_human\_vs\_ai\_gap.png} -
\texttt{figure\_deepeval\_metrics.png} -
\texttt{figure\_explanation\_lengths.png} -
\texttt{figure\_confidence\_scatter.png} -
\texttt{figure\_turnaround.png} - \texttt{figure\_usage.png} - Markdown
report for quick reference (this document copied into thesis annex).

Once real evaluation data is collected, replace synthetic metrics and
regenerate the charts for the final thesis submission.
