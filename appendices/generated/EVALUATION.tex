\hypertarget{comprehensive-evaluation-strategy}{%
\section{Comprehensive Evaluation
Strategy}\label{comprehensive-evaluation-strategy}}

This document outlines the detailed, module-by-module evaluation plan
for the Automated Grading Framework. For each component, it analyzes the
available evaluation options and provides a clear justification for the
chosen, open-source method.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{data-ingestion-etl-module-1_upload_data.py}{%
\subsubsection{\texorpdfstring{1. Data Ingestion \& ETL Module
(\texttt{1\_upload\_data.py})}{1. Data Ingestion \& ETL Module (1\_upload\_data.py)}}\label{data-ingestion-etl-module-1_upload_data.py}}

\begin{itemize}
\item
  \textbf{What We Need to Evaluate:} The accuracy of the
  \texttt{PyMuPDF} and regular expression-based parser in correctly
  extracting and structuring the \textbf{questions, rubric, and student
  answers} from various PDF files.
\item
  \textbf{Available Options:}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    \textbf{Manual Spot-Checking:}

    \begin{itemize}
    \tightlist
    \item
      \emph{Advantages:} Simple to perform, requires no setup, fast for
      a very small number of documents.
    \item
      \emph{Disadvantages:} Not scalable, highly subjective, not
      statistically valid, and likely to miss subtle but critical
      edge-case errors (e.g., incorrect parsing of a specific rubric
      format).
    \end{itemize}
  \item
    \textbf{Error Rate Monitoring:}

    \begin{itemize}
    \tightlist
    \item
      \emph{Advantages:} Easy to implement by wrapping the parsing logic
      in a try-except block; effectively catches 100\% of catastrophic
      failures where the program would otherwise crash.
    \item
      \emph{Disadvantages:} Provides a false sense of security. It
      cannot detect logical errors, such as misattributing an answer to
      the wrong question or failing to extract the last item in a
      rubric. The parser can ``succeed'' but still produce garbage data.
    \end{itemize}
  \item
    \textbf{Golden Dataset Testing:}

    \begin{itemize}
    \tightlist
    \item
      \emph{Advantages:} Provides objective, quantifiable, and
      reproducible metrics (e.g., F1-score, Precision). An automated
      script compares the parser's output against a ``perfect'' ground
      truth, catching both catastrophic and subtle logical errors. It
      enables regression testing to ensure future changes don't break
      existing functionality.
    \item
      \emph{Disadvantages:} Requires a significant upfront investment of
      time to create a diverse and representative ``golden dataset'' of
      PDFs and their corresponding ideal JSON outputs.
    \end{itemize}
  \end{enumerate}
\item
  \textbf{Chosen Method: Golden Dataset Testing}

  \begin{itemize}
  \tightlist
  \item
    \textbf{Open-Source Confirmation:} This is a methodology, and its
    implementation will exclusively use open-source tools, primarily
    \textbf{\texttt{pytest}} for the testing framework and Python's
    standard libraries for file handling and comparison.
  \item
    \textbf{Justification:} The quality of the data entering the system
    is non-negotiable. While it requires upfront effort, the Golden
    Dataset method is the only one that provides true, quantifiable
    confidence in the parser's accuracy. It moves from ``it seems to
    work'' to ``it is 99.2\% accurate on these 15 document types.'' This
    rigor is essential for a reliable application and justifies the
    initial time investment.
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{core-grader-evaluations}{%
\subsection{Core Grader Evaluations}\label{core-grader-evaluations}}

\hypertarget{evaluation-of-the-multi-agent-text-grader}{%
\subsubsection{2.1. Evaluation of the Multi-Agent Text
Grader}\label{evaluation-of-the-multi-agent-text-grader}}

\begin{itemize}
\tightlist
\item
  \textbf{What We Need to Evaluate:}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    \textbf{Numeric Score Accuracy:} Comparison of the AI's score to a
    human expert's score.
  \item
    \textbf{Feedback Quality:} The relevance, helpfulness, and factual
    correctness of the generated text.
  \item
    \textbf{Consensus Reliability (RQ1):} The statistical consistency
    among the different AI agents.
  \end{enumerate}
\item
  \textbf{Available Options for Feedback Quality:}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    \textbf{Lexical Similarity Metrics (e.g., BLEU, ROUGE):}

    \begin{itemize}
    \tightlist
    \item
      \emph{Advantages:} Simple to calculate, fast, and completely
      objective.
    \item
      \emph{Disadvantages:} Fundamentally unsuited for this task. They
      only measure n-gram overlap with a reference answer, not semantic
      meaning, factual accuracy, or logical reasoning. They cannot
      determine if feedback is helpful or even correct.
    \end{itemize}
  \item
    \textbf{LLM-as-a-Judge (Proprietary):}

    \begin{itemize}
    \tightlist
    \item
      \emph{Advantages:} Highly scalable and capable of evaluating
      complex, abstract qualities of the text.
    \item
      \emph{Disadvantages:} Often relies on closed-source, proprietary
      models (e.g., GPT-4), which violates our open-source constraint.
      It can be expensive, and the ``judge'' model may have its own
      biases.
    \end{itemize}
  \item
    \textbf{DeepEval (Open-Source Framework):}

    \begin{itemize}
    \tightlist
    \item
      \emph{Advantages:} It is \textbf{open-source}, specifically
      designed for evaluating LLM outputs, and provides metrics for the
      exact problems we face, such as \texttt{Faithfulness}
      (fact-checking against a context) and \texttt{AnswerRelevancy}. It
      integrates cleanly with \texttt{pytest} for automated testing.
    \item
      \emph{Disadvantages:} Requires some configuration and a clear
      understanding of what each metric is measuring to be used
      effectively.
    \end{itemize}
  \end{enumerate}
\item
  \textbf{Chosen Method: A Hybrid of DeepEval and Open-Source
  Statistical Libraries}

  \begin{itemize}
  \tightlist
  \item
    \textbf{Open-Source Confirmation:} This approach exclusively uses
    open-source libraries: \textbf{\texttt{DeepEval}} for feedback
    quality, \textbf{\texttt{SciPy}} and \textbf{\texttt{NumPy}} for
    numeric score analysis (MAE, Pearson's r), and
    \textbf{\texttt{simpledorff}} for Krippendorff's Alpha.
  \item
    \textbf{Justification:} No single tool can evaluate this complex
    system. A hybrid approach is necessary.

    \begin{itemize}
    \tightlist
    \item
      \textbf{For Feedback Quality, \texttt{DeepEval} is chosen} because
      it is the only open-source option that can look beyond
      word-matching to assess the \emph{semantic quality} of the
      generated text, which is paramount.
    \item
      \textbf{For Numeric Accuracy, standard statistical methods are
      chosen} because they are the universally accepted standard for
      comparing a model's score to a human baseline.
    \item
      \textbf{For Consensus Reliability, Krippendorff's Alpha is chosen}
      because it is the most robust academic standard for measuring
      inter-rater reliability, accounting for chance agreement in a way
      that simpler metrics (like variance) do not.
    \end{itemize}
  \end{itemize}
\end{itemize}

\hypertarget{evaluation-of-the-code-grader}{%
\subsubsection{2.2. Evaluation of the Code
Grader}\label{evaluation-of-the-code-grader}}

\begin{itemize}
\tightlist
\item
  \textbf{What We Need to Evaluate:}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    \textbf{Execution Accuracy:} The accuracy of the unit test execution
    within the Docker sandbox.
  \item
    \textbf{Feedback Quality:} The pedagogical value of the
    LLM-generated feedback on code style and correctness.
  \end{enumerate}
\item
  \textbf{Available Options for Feedback Quality:}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    \textbf{LLM-as-a-Judge:}

    \begin{itemize}
    \tightlist
    \item
      \emph{Advantages:} Scalable.
    \item
      \emph{Disadvantages:} Not open-source, and a generic LLM lacks the
      specific pedagogical context to know what constitutes ``good''
      feedback for a student learning to code.
    \end{itemize}
  \item
    \textbf{Qualitative Human Review by Experts:}

    \begin{itemize}
    \tightlist
    \item
      \emph{Advantages:} The undisputed ``gold standard'' for assessing
      pedagogical value. Human instructors can spot nuances, assess
      tone, and determine if the feedback would actually help a student
      learn---qualities that an AI judge cannot.
    \item
      \emph{Disadvantages:} It is subjective, time-consuming, and does
      not scale.
    \end{itemize}
  \end{enumerate}
\item
  \textbf{Chosen Method: Execution Accuracy \& Qualitative Human Review}

  \begin{itemize}
  \tightlist
  \item
    \textbf{Open-Source Confirmation:} The objective part uses
    open-source tools (\texttt{Docker}, \texttt{pytest}). The subjective
    part is a methodology, not a tool, and is by nature open and
    transparent.
  \item
    \textbf{Justification:} This module's dual nature requires a dual
    evaluation.

    \begin{itemize}
    \tightlist
    \item
      \textbf{For the Objective Score:} A simple \textbf{Execution
      Accuracy Percentage} is perfect. It's a binary, clear, and
      unimpeachable metric derived from comparing the sandbox's
      \texttt{unittest} results to the ground truth.
    \item
      \textbf{For the Subjective Feedback:} \textbf{Qualitative Human
      Review is chosen} over an LLM-as-a-Judge because scalability is
      less important than authenticity. To gain user trust, the feedback
      must be genuinely helpful, and only human experts can be the judge
      of that. For this specific task, human insight is more valuable
      than automated metrics.
    \end{itemize}
  \end{itemize}
\end{itemize}

\hypertarget{evaluation-of-the-math-grader}{%
\subsubsection{2.3. Evaluation of the Math
Grader}\label{evaluation-of-the-math-grader}}

\begin{itemize}
\tightlist
\item
  \textbf{What We Need to Evaluate:}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    \textbf{Symbolic Correctness:} The accuracy of the symbolic math
    engine (e.g., SymPy) in correctly evaluating the student's
    mathematical expressions.
  \item
    \textbf{Feedback Quality:} The clarity and pedagogical value of the
    LLM-generated feedback explaining \emph{why} a mathematical answer
    is correct or incorrect.
  \end{enumerate}
\item
  \textbf{Available Options:}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    \textbf{Manual Calculation:} Manually solve the problems and compare
    the results. This is slow and prone to human error.
  \item
    \textbf{Automated Symbolic Comparison:} Use a symbolic math library
    to programmatically compare the student's final expression against a
    known ``golden'' solution. This is fast, deterministic, and highly
    accurate.
  \item
    \textbf{LLM-as-a-Judge for Feedback:} Use a powerful LLM to evaluate
    the quality of the generated feedback.
  \end{enumerate}
\item
  \textbf{Chosen Method: Automated Symbolic Comparison \& DeepEval}

  \begin{itemize}
  \tightlist
  \item
    \textbf{Open-Source Confirmation:} We will use
    \textbf{\texttt{SymPy}} for the symbolic comparison and
    \textbf{\texttt{DeepEval}} for feedback analysis, both of which are
    open-source.
  \item
    \textbf{Justification:} This hybrid approach addresses both facets
    of the math grader.

    \begin{itemize}
    \tightlist
    \item
      \textbf{For Symbolic Correctness:} \textbf{\texttt{SymPy} is
      chosen} because it allows for a definitive, automated, and
      mathematically sound way to check for the equivalence of symbolic
      expressions. This provides an objective, unimpeachable score for
      correctness.
    \item
      \textbf{For Feedback Quality:} \textbf{\texttt{DeepEval} is
      chosen} to ensure the textual explanation is factually grounded in
      the mathematical error and is relevant to the student's mistake,
      preventing generic or unhelpful feedback.
    \end{itemize}
  \end{itemize}
\end{itemize}

\hypertarget{evaluation-of-the-multimodal-grader}{%
\subsubsection{2.4. Evaluation of the Multimodal
Grader}\label{evaluation-of-the-multimodal-grader}}

\begin{itemize}
\tightlist
\item
  \textbf{What We Need to Evaluate:}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    \textbf{Visual Interpretation Accuracy:} The model's ability to
    correctly interpret the content of images, charts, and diagrams.
  \item
    \textbf{Integrated Reasoning:} The model's capacity to combine
    information from both text and images to accurately assess the
    student's answer against the rubric.
  \item
    \textbf{Feedback Quality:} The quality of the feedback, ensuring it
    references both textual and visual elements where appropriate.
  \end{enumerate}
\item
  \textbf{Available Options:}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    \textbf{Ad-hoc Manual Review:} Simply looking at the output and
    subjectively deciding if it's correct. This is not rigorous or
    reproducible.
  \item
    \textbf{Proprietary Multimodal Evaluation Models:} Use
    closed-source, powerful multimodal models as judges. This violates
    the open-source constraint and can be expensive.
  \item
    \textbf{Human Evaluation with a Structured Rubric:} Have human
    experts evaluate the multimodal output against a specific,
    structured rubric that asks questions like, ``Did the model
    correctly identify the key elements in the diagram?'' and ``Does the
    feedback correctly reference the visual information?''
  \end{enumerate}
\item
  \textbf{Chosen Method: Human Evaluation with a Structured Rubric}

  \begin{itemize}
  \tightlist
  \item
    \textbf{Open-Source Confirmation:} This is a methodology, not a
    specific tool, and is by nature open and transparent.
  \item
    \textbf{Justification:} The automated evaluation of multimodal
    reasoning is a complex, cutting-edge research problem. For the
    practical purposes of this application, \textbf{a structured human
    review is the only truly reliable method}. It is the ``gold
    standard'' for assessing nuanced understanding that combines
    different data types. While not scalable, it provides the most
    accurate and actionable feedback on the performance of the
    multimodal grader.
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{supporting-ai-system-evaluations}{%
\subsection{Supporting AI System
Evaluations}\label{supporting-ai-system-evaluations}}

\hypertarget{evaluation-of-the-rag-integration-module}{%
\subsubsection{3.1. Evaluation of the RAG Integration
Module}\label{evaluation-of-the-rag-integration-module}}

\begin{itemize}
\item
  \textbf{What We Need to Evaluate:} The relevance and completeness of
  the documents retrieved from the FAISS vector store.
\item
  \textbf{Available Options:}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    \textbf{End-to-End Evaluation:}

    \begin{itemize}
    \tightlist
    \item
      \emph{Advantages:} Requires no extra tools or setup.
    \item
      \emph{Disadvantages:} It's an indirect and unreliable signal. A
      good final grade could have occurred \emph{despite} bad retrieval,
      and vice-versa. It doesn't provide actionable insight into the RAG
      pipeline itself.
    \end{itemize}
  \item
    \textbf{Ragas (Open-Source Framework):}

    \begin{itemize}
    \tightlist
    \item
      \emph{Advantages:} It is \textbf{open-source} and the
      purpose-built tool for this exact problem. It isolates the
      retrieval step and provides clear, actionable metrics like
      \texttt{context\_precision} and \texttt{context\_recall}.
    \item
      \emph{Disadvantages:} Requires the creation of a curated
      evaluation dataset containing queries and their expected retrieved
      documents.
    \end{itemize}
  \end{enumerate}
\item
  \textbf{Chosen Method: Ragas Framework}

  \begin{itemize}
  \tightlist
  \item
    \textbf{Open-Source Confirmation:} \textbf{\texttt{Ragas}} is a
    well-known, Apache 2.0 licensed open-source project.
  \item
    \textbf{Justification:} Using a specialized tool is vastly superior
    to indirect measurement. \textbf{Ragas is chosen} because it allows
    us to diagnose the health of our RAG pipeline directly. Knowing our
    \texttt{context\_precision} is low, for example, tells us we need to
    improve our chunking or embedding strategy. This level of targeted
    insight is impossible with an end-to-end approach.
  \end{itemize}
\end{itemize}

\hypertarget{evaluation-of-the-explainability-module-explainer.py}{%
\subsubsection{\texorpdfstring{3.2. Evaluation of the Explainability
Module
(\texttt{explainer.py})}{3.2. Evaluation of the Explainability Module (explainer.py)}}\label{evaluation-of-the-explainability-module-explainer.py}}

\begin{itemize}
\item
  \textbf{What We Need to Evaluate:} The clarity, accuracy, and
  pedagogical value of the generated explanation, ensuring it correctly
  justifies the score by explicitly referencing the rubric and the
  student's answer.
\item
  \textbf{Available Options:}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    \textbf{Human Review (Likert Scale):}

    \begin{itemize}
    \tightlist
    \item
      \emph{Advantages:} Provides a direct measure of user satisfaction
      and perceived clarity.
    \item
      \emph{Disadvantages:} Subjective, slow, and doesn't scale well.
    \end{itemize}
  \item
    \textbf{DeepEval Framework:}

    \begin{itemize}
    \tightlist
    \item
      \emph{Advantages:} \textbf{Open-source} and allows for the
      creation of precise, automated, and objective metrics. We can move
      beyond a simple rating to get a specific score for specific
      qualities.
    \item
      \emph{Disadvantages:} Requires some initial setup for custom
      metrics.
    \end{itemize}
  \end{enumerate}
\item
  \textbf{Chosen Method: DeepEval Framework with Custom Metrics}

  \begin{itemize}
  \tightlist
  \item
    \textbf{Open-Source Confirmation:} \textbf{\texttt{DeepEval}} is an
    open-source framework.
  \item
    \textbf{Justification:} \textbf{DeepEval is chosen} because it
    allows us to create a highly specific and automated metric that
    perfectly matches our goal. We will create a custom \textbf{``Rubric
    Coverage''} metric. This metric programmatically parses the rubric
    and checks if the generated explanation explicitly addresses every
    single criterion. This provides a direct, objective score for the
    explanation's completeness, which is far more valuable and reliable
    than a subjective human rating.
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{system-wide-process-evaluation}{%
\subsection{System-Wide Process
Evaluation}\label{system-wide-process-evaluation}}

\hypertarget{evaluation-of-the-fine-tuning-pipeline}{%
\subsubsection{4.1. Evaluation of the Fine-Tuning
Pipeline}\label{evaluation-of-the-fine-tuning-pipeline}}

\begin{itemize}
\item
  \textbf{What We Need to Evaluate:} The delta in performance between
  the base model and the fine-tuned model on a consistent, held-out test
  set. We need to measure if fine-tuning actually improves grading
  accuracy and feedback quality.
\item
  \textbf{Available Options:}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    \textbf{Training Loss Monitoring:}

    \begin{itemize}
    \tightlist
    \item
      \emph{Advantages:} Very easy, as this data is output by the
      training script by default.
    \item
      \emph{Disadvantages:} Can be highly misleading. A decreasing loss
      only proves the model is learning the \emph{training data}. It
      does not prove the model can \emph{generalize} to new, unseen
      data, which is the entire point of fine-tuning.
    \end{itemize}
  \item
    \textbf{Comparative A/B Testing on a Hold-out Set:}

    \begin{itemize}
    \tightlist
    \item
      \emph{Advantages:} The scientific standard for measuring impact.
      It provides clear, quantitative proof of improvement (or lack
      thereof) by comparing the model's performance before and after
      fine-tuning on the same unseen data.
    \item
      \emph{Disadvantages:} Requires the discipline to maintain a strict
      separation between training data and the held-out test set.
    \end{itemize}
  \end{enumerate}
\item
  \textbf{Chosen Method: Comparative A/B Testing on a Hold-out Set}

  \begin{itemize}
  \tightlist
  \item
    \textbf{Open-Source Confirmation:} This is a methodology that
    leverages the other open-source tools in our evaluation suite
    (\texttt{DeepEval}, \texttt{Ragas}, \texttt{pytest}).
  \item
    \textbf{Justification:} This is the only option that can
    definitively prove the value of the fine-tuning pipeline. \textbf{It
    is chosen} because it directly answers the business question: ``Does
    this feature make our product better?'' By running the entire
    evaluation suite on the base model and then again on the fine-tuned
    model, we can generate a clear, evidence-based report (e.g.,
    ``Fine-tuning improved feedback faithfulness by 15\% and reduced
    scoring error by 10\%''), justifying the feature's existence.
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{backend-infrastructure-evaluation}{%
\subsection{5. Backend Infrastructure
Evaluation}\label{backend-infrastructure-evaluation}}

\hypertarget{evaluation-of-database-technology-choice-postgresql}{%
\subsubsection{5.1. Evaluation of Database Technology Choice
(PostgreSQL)}\label{evaluation-of-database-technology-choice-postgresql}}

\begin{itemize}
\item
  \textbf{What We Need to Evaluate:} The suitability of the chosen
  database technology (PostgreSQL) against other options, based on the
  application's specific requirements for data integrity, query
  complexity, scalability, and ecosystem support.
\item
  \textbf{Available Options:}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    \textbf{NoSQL Databases (e.g., MongoDB, Firestore):}

    \begin{itemize}
    \tightlist
    \item
      \emph{Advantages:} Flexible schema is good for rapidly changing or
      unstructured data. Generally easier to scale horizontally.
    \item
      \emph{Disadvantages:} Weaker transactional guarantees (compared to
      ACID). Performing complex queries with joins (e.g., getting all
      student submissions for a specific assignment with rubric details)
      is difficult and inefficient. The application's data is highly
      structured and relational, making this a poor fit.
    \end{itemize}
  \item
    \textbf{SQLite:}

    \begin{itemize}
    \tightlist
    \item
      \emph{Advantages:} Extremely simple, serverless,
      zero-configuration, and stores the entire database in a single
      file. Excellent for development, testing, or very simple,
      single-user applications.
    \item
      \emph{Disadvantages:} Not designed for concurrency. It struggles
      with multiple users (e.g., several instructors) writing to the
      database at the same time, which is a key requirement for this web
      application. It lacks the advanced features and robustness of a
      full database server.
    \end{itemize}
  \item
    \textbf{Relational Databases (e.g., PostgreSQL, MySQL):}

    \begin{itemize}
    \tightlist
    \item
      \emph{Advantages:} Strong ACID compliance guarantees data
      integrity and transactional safety, which is critical for academic
      records. The standardized SQL language is perfect for the complex,
      relational queries this application needs. Mature, stable, and
      well-supported technology.
    \item
      \emph{Disadvantages:} Requires a more rigid schema upfront. Can be
      more complex to set up and manage than SQLite.
    \end{itemize}
  \end{enumerate}
\item
  \textbf{Chosen Method: PostgreSQL (A Relational Database)}

  \begin{itemize}
  \tightlist
  \item
    \textbf{Open-Source Confirmation:} PostgreSQL is a powerful,
    well-regarded, and fully open-source object-relational database
    system with a liberal license.
  \item
    \textbf{Justification:} \textbf{PostgreSQL was chosen because the
    application's data is fundamentally relational and requires high
    integrity.}

    \begin{enumerate}
    \def\labelenumi{\arabic{enumi}.}
    \tightlist
    \item
      \textbf{Data Integrity is Non-Negotiable:} The system manages
      grades, student submissions, and feedback. The strong ACID
      guarantees of PostgreSQL ensure that this critical data is never
      left in an inconsistent state. A NoSQL database would risk data
      corruption.
    \item
      \textbf{Naturally Relational Data:} The data model consists of
      clear relationships: a \texttt{student} has many
      \texttt{submissions}, an \texttt{assignment} has many
      \texttt{grades}. A relational database is the most efficient and
      logical way to model and query these relationships.
    \item
      \textbf{Requirement for Complex Queries:} The application needs to
      perform complex joins, aggregations, and reports (e.g., ``Find all
      feedback for this student across all assignments,'' ``Calculate
      the average score for question 3 on this exam''). SQL is the most
      powerful and appropriate tool for these tasks.
    \item
      \textbf{Concurrency and Scalability:} Unlike SQLite, PostgreSQL is
      designed from the ground up to handle concurrent connections from
      multiple users, which is essential for a web application used by
      multiple instructors. It provides a robust foundation that can
      scale to a large number of users and a large volume of data.
    \end{enumerate}
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{llm-comparative-evaluation}{%
\subsection{6. LLM Comparative
Evaluation}\label{llm-comparative-evaluation}}

\hypertarget{evaluation-of-different-language-models}{%
\subsubsection{6.1. Evaluation of Different Language
Models}\label{evaluation-of-different-language-models}}

\begin{itemize}
\item
  \textbf{What We Need to Evaluate:} The relative performance of
  different LLMs (e.g., a base model like Llama 3, a fine-tuned version,
  an API-based model like Gemini) for the specific tasks of grading and
  feedback generation. The goal is to determine which model provides the
  best balance of accuracy, quality, cost, and latency for our
  application.
\item
  \textbf{Available Options:}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    \textbf{Public Benchmarks (e.g., MMLU, HumanEval):}

    \begin{itemize}
    \tightlist
    \item
      \emph{Advantages:} Standardized scores are readily available for
      many models, providing a general sense of their capabilities.
    \item
      \emph{Disadvantages:} These benchmarks are generic and do not
      measure performance on our highly specific task of rubric-based
      grading. A model's ability to answer trivia questions (MMLU) is
      not a good proxy for its ability to provide nuanced, pedagogical
      feedback based on a rubric.
    \end{itemize}
  \item
    \textbf{Ad-hoc Manual Testing:}

    \begin{itemize}
    \tightlist
    \item
      \emph{Advantages:} Quick and easy to get a ``feel'' for a model's
      output on a few examples.
    \item
      \emph{Disadvantages:} Not reproducible, not scalable, and highly
      prone to subjective bias. It cannot provide the quantitative data
      needed for a formal comparison or to justify a decision.
    \end{itemize}
  \item
    \textbf{Systematic A/B/n Testing on a Hold-out Set:}

    \begin{itemize}
    \tightlist
    \item
      \emph{Advantages:} The gold standard for comparative analysis. It
      provides direct, head-to-head quantitative comparisons of models
      on the \emph{exact same data} for the \emph{exact same task}. It
      reuses our entire existing evaluation suite (DeepEval, statistical
      tests, etc.) to produce a rich, multi-faceted scorecard for each
      model.
    \item
      \emph{Disadvantages:} Requires more rigorous setup and execution
      time than other methods.
    \end{itemize}
  \end{enumerate}
\item
  \textbf{Chosen Method: Systematic A/B/n Testing on a Hold-out Set}

  \begin{itemize}
  \tightlist
  \item
    \textbf{Open-Source Confirmation:} This is a methodology that
    leverages our entire existing suite of open-source evaluation tools
    (\texttt{DeepEval}, \texttt{Ragas}, \texttt{SciPy},
    \texttt{pytest}). No new tools are needed.
  \item
    \textbf{Justification:} \textbf{This is the only method that
    provides actionable, evidence-based results for our specific use
    case.}

    \begin{enumerate}
    \def\labelenumi{\arabic{enumi}.}
    \tightlist
    \item
      \textbf{Task-Specific Results:} Instead of relying on irrelevant
      public benchmarks, this method tests the models on the
      \emph{actual grading tasks} the application performs.
    \item
      \textbf{Direct, Quantitative Comparison:} The process involves
      running the entire evaluation suite (from Section 2.1) on a fixed
      hold-out set, with the only variable being the LLM being called.
      This produces a clear scorecard comparing each model on metrics
      like \textbf{Mean Absolute Error}, \textbf{Feedback Faithfulness},
      and \textbf{Rubric Coverage}.
    \item
      \textbf{Evidence-Based Decisions:} This approach allows us to make
      definitive statements like, ``Model X reduces scoring errors by
      15\% but increases latency by 40\% compared to Model Y.'' This is
      the critical information needed to make informed decisions about
      which model to deploy, balancing performance, cost, and user
      experience. It turns a subjective choice into a data-driven one.
    \end{enumerate}
  \end{itemize}
\end{itemize}
