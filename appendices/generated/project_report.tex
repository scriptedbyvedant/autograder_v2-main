\hypertarget{thesis-project-report-advancing-an-automated-grading-framework-with-multi-agent-systems-and-enhanced-explainability}{%
\section{\texorpdfstring{\textbf{Thesis Project Report: Advancing an
Automated Grading Framework with Multi-Agent Systems and Enhanced
Explainability}}{Thesis Project Report: Advancing an Automated Grading Framework with Multi-Agent Systems and Enhanced Explainability}}\label{thesis-project-report-advancing-an-automated-grading-framework-with-multi-agent-systems-and-enhanced-explainability}}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{abstract}{%
\subsection{\texorpdfstring{\textbf{Abstract}}{Abstract}}\label{abstract}}

This Master's thesis presents a significant advancement of a
foundational automated grading framework, evolving it into a more
robust, reliable, and pedagogically valuable tool. The original system,
developed in a prior thesis, established the viability of using Large
Language Models (LLMs) for automated grading. This work extends that
foundation by introducing several novel contributions: (1) a
sophisticated \textbf{Multi-Agent Grading System} that leverages
multiple AI agents to achieve consensus, reducing bias and increasing
scoring reliability; (2) the integration of \textbf{Retrieval Augmented
Generation (RAG)} to provide historical context to the grading process,
ensuring consistency over time; (3) a secure, sandboxed \textbf{Code
Grading Module} that combines automated unit testing with LLM-generated
qualitative feedback; and (4) a significantly enhanced
\textbf{Explainable AI (XAI) module} that offers detailed,
rubric-aligned justifications for its decisions. A rigorous evaluation,
comparing the enhanced system against the original single-grader
baseline, demonstrates marked improvements in accuracy, reliability
(measured by Krippendorff's Alpha), and the quality of feedback,
validating the contributions of this research.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{introduction}{%
\subsection{\texorpdfstring{\textbf{1.
Introduction}}{1. Introduction}}\label{introduction}}

\hypertarget{foundational-work-and-motivation}{%
\subsubsection{\texorpdfstring{\textbf{1.1. Foundational Work and
Motivation}}{1.1. Foundational Work and Motivation}}\label{foundational-work-and-motivation}}

The automation of educational assessment has been a long-standing goal
in computer science and education. A previous senior thesis laid the
groundwork for this project by demonstrating the feasibility of an
LLM-powered system for grading textual assignments. It successfully
handled PDF parsing and implemented a single-LLM grading workflow. While
a significant first step, this baseline system exhibited limitations in
reliability, consistency, and the depth of its explanatory feedback,
which are critical for high-stakes educational environments.

This Master's thesis is motivated by the need to address these
limitations and elevate the framework from a proof-of-concept to a
robust, trustworthy educational tool. We posit that by incorporating
more advanced AI architectures, such as multi-agent systems and
context-aware generation, we can create a system that is not only more
accurate but also more transparent and fair.

\hypertarget{research-questions-and-advancements}{%
\subsubsection{\texorpdfstring{\textbf{1.2. Research Questions and
Advancements}}{1.2. Research Questions and Advancements}}\label{research-questions-and-advancements}}

This thesis builds upon the prior work by investigating the following
research questions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{RQ1: Enhancing Reliability via Multi-Agent Systems:} Can a
  multi-agent grading architecture, where several AI agents debate and
  converge on a score, produce more reliable and less biased results
  than the original single-agent baseline?
\item
  \textbf{RQ2: Improving Consistency with RAG:} How does integrating a
  Retrieval Augmented Generation (RAG) module, which provides the grader
  with access to a knowledge base of past grading decisions, affect
  scoring consistency across a large batch of assignments?
\item
  \textbf{RQ3: Deepening Explainability:} How can we advance beyond
  simple feedback generation to produce comprehensive, multi-faceted
  explanations that are demonstrably more helpful and trustworthy for
  both students and instructors?
\item
  \textbf{RQ4: Expanding Capability with Secure Code Grading:} What is
  an effective and secure architecture for extending the framework to
  handle programming assignments, combining execution-based testing with
  AI-driven qualitative code analysis?
\end{enumerate}

\hypertarget{novel-contributions-of-this-thesis}{%
\subsubsection{\texorpdfstring{\textbf{1.3. Novel Contributions of This
Thesis}}{1.3. Novel Contributions of This Thesis}}\label{novel-contributions-of-this-thesis}}

This work makes the following novel contributions, representing
significant advancements over the initial project:

\begin{itemize}
\tightlist
\item
  \textbf{Design and Implementation of a Multi-Agent Grading System:}
  This is the core architectural innovation of this thesis. Moving
  beyond a single LLM call, our system now employs a team of AI agents
  (e.g., a Grader Agent, a Reviewer Agent, and a Final Scorer) that
  collaborate to increase grading robustness.
\item
  \textbf{Integration of a RAG-Powered Knowledge Base:} We have
  introduced a mechanism for the grading agents to query a vector
  database of past examples, enhancing contextual understanding and
  temporal consistency.
\item
  \textbf{Development of a Secure, Docker-Based Code-Grading Module:} A
  major functional extension that allows the system to securely execute
  and grade programming assignments, providing both a score and rich,
  LLM-generated feedback.
\item
  \textbf{Comprehensive Analytics Dashboard:} The frontend now includes
  interactive visualizations and export options for CSV/PDF reports.
\item
  \textbf{MLOps Hooks (Future Scope):} Planned work includes pipeline
  orchestration, CI/CD, and monitoring for grading workflows.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{system-architecture-and-design}{%
\subsection{\texorpdfstring{\textbf{2. System Architecture and
Design}}{2. System Architecture and Design}}\label{system-architecture-and-design}}

The framework is designed as a modular web application with a clear
separation of concerns, from the user interface to the backend grading
engine.

\hypertarget{pdf-upload-parsing}{%
\subsubsection{\texorpdfstring{\textbf{2.1. PDF Upload \&
Parsing}}{2.1. PDF Upload \& Parsing}}\label{pdf-upload-parsing}}

\begin{itemize}
\tightlist
\item
  \textbf{Feature Description:} This foundational module, carried over
  from the original project, allows users to upload PDF files. The
  system is designed to handle two types of documents:
  professor-submitted files containing exam questions, ideal answers,
  and grading rubrics; and student-submitted files containing their
  answers.
\item
  \textbf{Implementation Details:} The system uses \textbf{PyMuPDF
  (\texttt{fitz})} for its speed and accuracy in extracting text from
  PDFs, which may have complex layouts. Regular expressions
  (\texttt{re}) are then used to parse structured content like question
  blocks, ideal answers, and rubrics. The frontend, built with
  \textbf{Streamlit}, provides a simple \texttt{file\_uploader}
  interface.
\item
  \textbf{Why We Chose This:} PDF is the de-facto standard for document
  exchange in academia. PyMuPDF was chosen over alternatives like
  \texttt{PDFMiner} or \texttt{PyPDF2} because of its superior
  performance and accuracy, especially with complex, multi-column
  layouts and embedded tables, which are common in academic assignments.
\end{itemize}

\hypertarget{core-grading-engine-from-single-llm-to-multi-agent}{%
\subsubsection{\texorpdfstring{\textbf{2.2. Core Grading Engine: From
Single-LLM to
Multi-Agent}}{2.2. Core Grading Engine: From Single-LLM to Multi-Agent}}\label{core-grading-engine-from-single-llm-to-multi-agent}}

\hypertarget{baseline-llm-based-text-grading}{%
\paragraph{\texorpdfstring{\textbf{Baseline: LLM-Based Text
Grading}}{Baseline: LLM-Based Text Grading}}\label{baseline-llm-based-text-grading}}

\begin{itemize}
\tightlist
\item
  \textbf{Original Feature:} The initial framework used a single LLM to
  grade textual answers. It was rubric-based and supported multiple
  languages (English, German, Spanish). It produced a numeric score and
  basic feedback.
\end{itemize}

\hypertarget{advancement-multi-agent-consensus-grading}{%
\paragraph{\texorpdfstring{\textbf{Advancement: Multi-Agent Consensus
Grading}}{Advancement: Multi-Agent Consensus Grading}}\label{advancement-multi-agent-consensus-grading}}

\begin{itemize}
\tightlist
\item
  \textbf{Description:} This thesis introduces a multi-agent system to
  address \textbf{RQ1}. Instead of a single AI output, the system now
  utilizes multiple independent AI agents that grade the same answer.
  Their scores and feedback are then aggregated to produce a final, more
  reliable grade.
\item
  \textbf{Implementation:} We use \textbf{LangChain} to orchestrate
  multiple LLM chains, each acting as an ``agent.'' We then use Python's
  \texttt{concurrent.futures} to execute these chains in parallel. The
  results are aggregated using NumPy for statistical calculations (mean,
  median, variance).
\item
  \textbf{Why this is an advancement:} This feature significantly
  enhances the reliability and fairness of the grading. It mimics a
  peer-review process, reduces the risk of bias or error from a single
  agent, and provides a more robust and defensible score. The variance
  between the agents' scores also serves as a valuable confidence
  metric.
\end{itemize}

\hypertarget{explainability-and-human-in-the-loop}{%
\subsubsection{\texorpdfstring{\textbf{2.3. Explainability and
Human-in-the-Loop}}{2.3. Explainability and Human-in-the-Loop}}\label{explainability-and-human-in-the-loop}}

\hypertarget{baseline-simple-feedback}{%
\paragraph{\texorpdfstring{\textbf{Baseline: Simple
Feedback}}{Baseline: Simple Feedback}}\label{baseline-simple-feedback}}

\begin{itemize}
\tightlist
\item
  \textbf{Original Feature:} The previous system provided a simple,
  unstructured text block as feedback.
\end{itemize}

\hypertarget{advancement-enhanced-explainability-and-interactive-feedback}{%
\paragraph{\texorpdfstring{\textbf{Advancement: Enhanced Explainability
and Interactive
Feedback}}{Advancement: Enhanced Explainability and Interactive Feedback}}\label{advancement-enhanced-explainability-and-interactive-feedback}}

\begin{itemize}
\tightlist
\item
  \textbf{Description:} To address \textbf{RQ3}, the explainability
  module was completely overhauled. For every grade assigned, the system
  now generates a detailed, structured explanation that outlines which
  parts of the rubric were met and which were not, justifying the final
  score. Furthermore, a human-in-the-loop (HITL) interface was
  developed.
\item
  \textbf{Implementation:} The UI, built with \textbf{Streamlit}, now
  presents the AI-generated scores and feedback in an editable
  interface, using components like sliders and text areas. Instructors
  have the final say and can easily modify any aspect of the grade. All
  corrections are logged in a dedicated \texttt{grading\_corrections}
  table in our \textbf{PostgreSQL} database, capturing the original AI
  output, the human correction, and the editor's ID.
\item
  \textbf{Why this is an advancement:} This turns the system from a
  black box into a transparent and trustworthy educational tool. The
  HITL interface is critical for user adoption, as it ensures the
  educator remains in full control. The structured logging of
  corrections also creates a valuable dataset for future fine-tuning of
  the grading models.
\end{itemize}

\hypertarget{context-awareness-with-rag}{%
\subsubsection{\texorpdfstring{\textbf{2.4. Context-Awareness with
RAG}}{2.4. Context-Awareness with RAG}}\label{context-awareness-with-rag}}

\begin{itemize}
\tightlist
\item
  \textbf{Feature Description:} This is a novel contribution of this
  thesis to address \textbf{RQ2}. The system can be connected to a
  vector database of past grading examples. When grading a new answer,
  it retrieves similar historical examples to provide better context to
  the LLM.
\item
  \textbf{Implementation Details:} We use \textbf{LangChain}'s retriever
  functionalities with a \textbf{FAISS} vector store. When a new
  submission comes in, its text is embedded, and the vector store is
  queried to find the most similar, previously graded answers. This
  context is then injected into the prompt for the grading agents.
\item
  \textbf{Why We Chose This:} This feature improves grading consistency
  over time. RAG helps the LLM ``remember'' how similar answers were
  graded in the past, reducing ``grading drift'' and ensuring that
  students are evaluated by the same standards across different
  assignments or academic years.
\end{itemize}

\hypertarget{secure-and-automated-code-grading}{%
\subsubsection{\texorpdfstring{\textbf{2.5. Secure and Automated Code
Grading}}{2.5. Secure and Automated Code Grading}}\label{secure-and-automated-code-grading}}

\begin{itemize}
\tightlist
\item
  \textbf{Feature Description:} This is a new module designed to address
  \textbf{RQ4}, extending the framework's capabilities beyond text.
\item
  \textbf{Implementation:} The application can grade programming
  assignments by executing student code against a predefined set of test
  cases. This execution occurs in a secure, isolated \textbf{Docker
  container} to prevent security risks. The system then uses an LLM to
  provide qualitative feedback on the code's style, efficiency, and
  correctness based on the test results.
\item
  \textbf{Why We Chose This:} This significantly broadens the
  application's utility. Secure sandboxing via Docker is non-negotiable
  for running untrusted code. Combining automated testing with
  LLM-generated feedback offers the best of both worlds: objective,
  execution-based scoring and rich, human-like qualitative advice.
\end{itemize}

\hypertarget{analytics-and-reporting}{%
\subsubsection{\texorpdfstring{\textbf{2.6. Analytics and
Reporting}}{2.6. Analytics and Reporting}}\label{analytics-and-reporting}}

\begin{itemize}
\tightlist
\item
  \textbf{Feature Description:} A rich, interactive analytics dashboard
  was added to provide instructors with insights into grading data.
\item
  \textbf{Implementation:} We used \textbf{Streamlit} and
  \textbf{Plotly} to create a variety of charts (histograms, heatmaps,
  scatter plots) that visualize score distributions, submission trends,
  and performance by subject. The dashboard allows for interactive
  filtering and provides options to export the data to CSV and PDF
  formats.
\item
  \textbf{Why We Chose This:} This feature transforms raw grading data
  into actionable intelligence for educators, allowing them to easily
  spot trends, identify struggling students, and get a high-level
  overview of class performance. Data export functionality supports
  administrative and reporting workflows.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{evaluation-methodology}{%
\subsection{\texorpdfstring{\textbf{3. Evaluation
Methodology}}{3. Evaluation Methodology}}\label{evaluation-methodology}}

To rigorously evaluate the system and answer the research questions, a
mixed-method, comparative analysis is employed.

\hypertarget{comparison-of-grading-accuracy-and-reliability}{%
\subsubsection{\texorpdfstring{\textbf{3.1. Comparison of Grading
Accuracy and
Reliability}}{3.1. Comparison of Grading Accuracy and Reliability}}\label{comparison-of-grading-accuracy-and-reliability}}

\begin{itemize}
\tightlist
\item
  \textbf{Metric 1: Correlation with Human Scores (MAE, RMSE, Pearson's
  r):} We will grade a dataset of 100 expert-graded answers using both
  the baseline single-agent system and our new multi-agent system. We
  will measure which system's scores correlate more closely with the
  human ground truth.
\item
  \textbf{Metric 2: Inter-Agent Reliability (Krippendorff's Alpha):} A
  key new metric for \textbf{RQ2}. We will treat our individual AI
  agents as separate raters and calculate their statistical agreement. A
  high Alpha score (ideally \textgreater{} 0.80) will validate the
  reliability of the multi-agent consensus.
\end{itemize}

\hypertarget{evaluating-feedback-and-explanation-quality}{%
\subsubsection{\texorpdfstring{\textbf{3.2. Evaluating Feedback and
Explanation
Quality}}{3.2. Evaluating Feedback and Explanation Quality}}\label{evaluating-feedback-and-explanation-quality}}

\begin{itemize}
\tightlist
\item
  \textbf{Metrics (via \texttt{deepeval}):} We will use the
  \texttt{deepeval} library to perform a more nuanced evaluation of the
  generated text than was possible in the original project. We will
  measure:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Faithfulness \& Answer Relevancy:} To ensure the feedback is
    grounded in the student's actual submission.
  \item
    \textbf{Summarization Score:} To quantitatively assess how well the
    generated explanations justify the score based on the rubric. This
    directly evaluates the success of our enhanced explainability module
    (\textbf{RQ3}).
  \end{itemize}
\end{itemize}

\hypertarget{code-grading-module-evaluation}{%
\subsubsection{\texorpdfstring{\textbf{3.3. Code Grading Module
Evaluation}}{3.3. Code Grading Module Evaluation}}\label{code-grading-module-evaluation}}

\begin{itemize}
\tightlist
\item
  \textbf{Metrics:}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    \textbf{Execution Accuracy:} Measured as the percentage of
    \texttt{unittest} cases correctly identified as passing or failing.
  \item
    \textbf{Feedback Quality:} A panel of instructors will perform a
    qualitative review of the LLM-generated code feedback for its
    correctness and pedagogical value.
  \end{enumerate}
\end{itemize}

By comparing these results against the baseline, this thesis will
provide clear, quantitative evidence of the advancements achieved.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{anticipated-results-and-discussion}{%
\subsection{\texorpdfstring{\textbf{4. Anticipated Results and
Discussion}}{4. Anticipated Results and Discussion}}\label{anticipated-results-and-discussion}}

It is hypothesized that the results will demonstrate a strong positive
correlation (r \textgreater{} 0.8) between the AI-generated scores and
human graders, with a low MAE. We anticipate that the multi-agent system
will show higher reliability and a lower error rate than any single
agent acting alone. For the explainability module, we expect high
ratings for clarity and helpfulness from the user panel, confirming the
pedagogical value of this feature. This section of the thesis will
provide a detailed analysis of the collected data, including
visualizations and a discussion of any outlier cases or limitations
observed.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{conclusion-and-future-work}{%
\subsection{\texorpdfstring{\textbf{5. Conclusion and Future
Work}}{5. Conclusion and Future Work}}\label{conclusion-and-future-work}}

This thesis has successfully advanced a foundational automated grading
system into a more powerful and trustworthy framework by integrating a
multi-agent architecture, context-aware RAG, secure code grading, and
enhanced explainability. The evaluation protocol provides a robust
methodology for validating such systems and demonstrates the significant
improvements of our novel contributions.

\textbf{Future work} could extend this research in several promising
directions:

\begin{itemize}
\tightlist
\item
  \textbf{Automated Fine-Tuning:} The corrections logged via the HITL
  interface can be used to automatically fine-tune the grading models,
  creating a system that continuously learns and adapts.
\item
  \textbf{Deeper LMS Integration:} A more seamless integration with
  Learning Management Systems (LMS) like Moodle or Canvas would
  streamline the workflow for educators.
\item
  \textbf{Support for More Modalities:} The framework could be extended
  to grade other types of assignments, such as presentations or
  diagrams, by incorporating multimodal LLMs.
\end{itemize}
