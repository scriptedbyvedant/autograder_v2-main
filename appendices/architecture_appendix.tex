
\section{Application Architecture}

This document provides a detailed overview of the technical architecture of the automated grading application. It is intended for developers and system administrators. 

\medskip\hrule\medskip

\subsection{1. High-Level Overview}

The application is a multi-tiered system composed of a web-based frontend, a robust backend with a relational database, and a sophisticated AI-powered grading engine. 

\begin{verbatim}
graph TD
    A[Browser] --> B{Streamlit Frontend};
    B --> C{Backend Server};
    C --> D[PostgreSQL Database];
    C --> E{AI Grading Engine};
    E --> F[LLM APIs / Local Models];
    E --> G[Vector Store];

    subgraph User Interface
        B
    end

    subgraph Core Logic & Data
        C
        D
    end

    subgraph AI Processing
        E
        F
        G
    end
\end{verbatim}

\begin{itemize}
  \item \textbf{Frontend:} A multi-page Streamlit application provides the user interface for professors and teaching assistants.
  \item \textbf{Backend:} A Python backend orchestrates the application logic, handling user requests, database interactions, and calls to the grading engine.
  \item \textbf{Database:} A PostgreSQL database stores all persistent data, including user information, assignment details, student submissions, and grading results.
  \item \textbf{AI Grading Engine:} A modular engine that leverages Large Language Models (LLMs) to perform the grading. It includes specialized modules for different types of assignments.
  \item \textbf{Vector Store:} A FAISS-based vector database stores embeddings of past grading decisions to provide historical context (RAG).
\end{itemize}

\medskip\hrule\medskip

\subsection{2. Frontend (Streamlit Application)}

The frontend is built using Streamlit and is organized into a multi-page application structure. 

\subsubsection{\textbf{2.1. Directory Structure}}

\begin{itemize}
  \item \codeword{app.py}: The main entry point of the Streamlit application. It handles routing and global configuration.
  \item \codeword{pages/}: This directory contains the individual pages of the application, such as:
  \item \codeword{0\_auth.py}: User authentication.
  \item \codeword{1\_upload\_data.py}: Interface for professors to upload assignment PDFs and for students to submit their work.
  \item \codeword{2\_grading\_result.py}: Displays the results of the grading process.
  \item \codeword{3\_fine\_tuning.py}: The user-friendly interface for fine-tuning the model.
\end{itemize}

\subsubsection{\textbf{2.2. Authentication and Session Management}}

Authentication is managed through the \codeword{auth} module. User session data, including login status and user roles, is stored in \codeword{st.session\_state}. This ensures that sensitive pages are protected and that the application context is maintained as the user navigates between pages. 

\medskip\hrule\medskip

\subsection{3. Backend and Database}

The backend logic is tightly integrated with the PostgreSQL database, which serves as the single source of truth. 

\subsubsection{\textbf{3.1. Database Schema}}

The database consists of several key tables: 

\begin{itemize}
  \item \codeword{users}: Stores user credentials and roles (e.g., professor, assistant).
  \item \codeword{prof\_data}: Contains the data uploaded by professors, including the course, assignment number, questions, ideal answers, and grading rubrics.
  \item \codeword{student\_data}: Stores student submissions, linked to a specific assignment.
  \item \codeword{grading\_results}: This is the central table for storing the output of the grading engine. It includes the original student answer, the AI-generated score and feedback (\codeword{old\_feedback}), and any human-in-the-loop corrections (\codeword{new\_feedback}, \codeword{new\_score}).
\end{itemize}

\subsubsection{\textbf{3.2. Data Handler (\codeword{database/postgres\_handler.py})}}

All database interactions are abstracted away by the \codeword{PostgresHandler} class. This class provides a standardized interface for executing queries (SELECT, INSERT, UPDATE) and managing connections. This centralized approach ensures consistency and simplifies database management. 

\medskip\hrule\medskip

\subsection{4. The AI Grading Engine (\codeword{grader\_engine/})}

This is the core of the application where the AI-powered grading takes place. 

\begin{verbatim}
graph TD
    A[Request from Backend] --> B{Router};
    B --> |Text| C[Text Grader];
    B --> |Code| D[Code Grader];
    B --> |Multi-Agent| E[Multi-Agent Grader];
    B --> |Multimodal| F[Multimodal Grader];

    C --> G{RAG Integration};
    E --> G;
    
    G --> H[Vector Store];
    C --> I{Explainability Module};
    E --> I;

    I --> J[Final Grade & Feedback];
\end{verbatim}

\subsubsection{\textbf{4.1. Router (\codeword{router.py})}}

The \codeword{Router} is the main entry point for the grading engine. It inspects the assignment type (e.g., text, code, multimodal) and directs the request to the appropriate specialized grader. 

\subsubsection{\textbf{4.2. Multi-Agent Grader (\codeword{multi\_agent.py})}}

To improve reliability and reduce bias, the system employs a multi-agent consensus mechanism: 

\begin{enumerate}
  \item \textbf{Agent Roles:} Multiple LLM agents are instantiated with slightly different personas (e.g., a "strict" grader, a "lenient" grader, a "by-the-book" grader).
  \item \textbf{Concurrent Grading:} These agents grade the same submission in parallel using \codeword{concurrent.futures}.
  \item \textbf{Consensus:} The scores and feedback from each agent are collected. The final score is typically the mean or median of the agents' scores, and the variance is used as a confidence metric.
  \item \textbf{Final Review:} A final "meta-agent" reviews the collected feedback and synthesizes it into a single, high-quality explanation.
\end{enumerate}

\subsubsection{\textbf{4.3. Code Grader (\codeword{code\_grader.py})}}

The code grader provides a secure and comprehensive way to evaluate programming assignments: 

\begin{enumerate}
  \item \textbf{Sandboxed Execution:} Student code is executed within a secure, isolated Docker container to prevent any potential security risks.
  \item \textbf{Unit Testing:} The code is run against a set of predefined \codeword{unittest} cases. The results (pass/fail) form the basis of the objective score.
  \item \textbf{Qualitative Feedback:} An LLM analyzes the student's code, along with the unit test results, to provide qualitative feedback on code style, efficiency, best practices, and potential areas for improvement.
\end{enumerate}

\subsubsection{\textbf{4.4. RAG Integration (\codeword{rag\_integration.py})}}

To ensure consistency over time, the grading engine uses Retrieval Augmented Generation (RAG): 

\begin{enumerate}
  \item \textbf{Vector Store:} When a human-in-the-loop correction is made, the grading context (question, student answer, corrected feedback) is embedded and stored in a FAISS vector store.
  \item \textbf{Contextual Retrieval:} When grading a new submission, the engine queries the vector store to find the most similar previously graded examples.
  \item \textbf{Prompt Injection:} These historical examples are injected into the LLM prompt, providing valuable context that helps the model "remember" how similar cases were graded in the past.
\end{enumerate}

\subsubsection{\textbf{4.5. Explainability Module (\codeword{explainer.py})}}

This module is responsible for generating the detailed, rubric-aligned justifications for the final score. It takes the grading results and structures them into a clear, easy-to-understand format that explains which criteria were met and why. 

\medskip\hrule\medskip

\subsection{5. Model Management \& Finetuning (\codeword{training/})}

The application is designed to allow for continuous improvement of the AI models through a user-friendly fine-tuning process. 

\subsubsection{\textbf{5.1. Finetuning Workflow}}

The new \codeword{pages/3\_fine\_tuning.py} page orchestrates this process: 

\begin{enumerate}
  \item \textbf{Data Generation:} A user clicks a button to generate a \codeword{training\_dataset.jsonl} file. The backend queries the \codeword{grading\_results} table for all human-corrected examples and formats them into the required JSONL structure.
  \item \textbf{Colab Training:} The user uploads this dataset to Google Colab and runs a provided Python script (\codeword{training/colab\_finetune.py}). This script handles the installation of dependencies, data preparation, and the MLX-based fine-tuning process.
  \item \textbf{Adapter Deployment:} The training script produces a \codeword{trained\_adapters.npz} file. The user downloads this file and places it in the \codeword{training/} directory of the application.
  \item \textbf{Automatic Loading:} On startup, the application checks for the existence of \codeword{trained\_adapters.npz}. If found, it automatically loads the fine-tuned LoRA adapters, enhancing the base model with the user's specific corrections.
\end{enumerate}